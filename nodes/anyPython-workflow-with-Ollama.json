{
  "last_node_id": 34,
  "last_link_id": 37,
  "nodes": [
    {
      "id": 9,
      "type": "Any Python 1 input",
      "pos": [
        1980,
        600
      ],
      "size": {
        "0": 740,
        "1": 580
      },
      "flags": {},
      "order": 4,
      "mode": 0,
      "inputs": [
        {
          "name": "variable",
          "type": "STRING",
          "link": 15,
          "widget": {
            "name": "variable"
          }
        }
      ],
      "outputs": [
        {
          "name": "STRING",
          "type": "STRING",
          "links": [
            10
          ],
          "shape": 3,
          "slot_index": 0
        }
      ],
      "properties": {
        "Node name for S&R": "Any Python 1 input"
      },
      "widgets_values": [
        "from bs4 import BeautifulSoup\n\n# Assuming 'variable' contains your HTML content\nsoup = BeautifulSoup(variable, 'html.parser')\n\n# Define how to process each tag type\ndef process_tag(tag):\n    if tag.name == 'h1':\n        return f\"# {tag.get_text()}\"\n    elif tag.name == 'h2':\n        return f\"## {tag.get_text()}\"\n    elif tag.name == 'h3':\n        return f\"### {tag.get_text()}\"\n    elif tag.name == 'li':\n        return f\"- {tag.get_text()}\"\n    else:\n        return tag.get_text()\n\n# Initialize an empty string\ntext_content = ''\n\n# Use a for loop to process each tag\nfor tag in soup.find_all(['h1', 'h2', 'h3', 'li', 'p']):\n    text_content += process_tag(tag) + '\\n'\n\nprint(text_content.strip())  # strip() is used to remove the last newline character\n",
        "from bs4 import BeautifulSoup\nimport re\n\ndef convert_html_to_markup(variable):\n    soup = BeautifulSoup(variable, 'html.parser')\n    \n    def replace_tags(tag):\n        if tag.name == 'p': return '\\n\\n' + tag.text + '\\n\\n'\n        elif tag.name == 'h1': return '# ' + tag.text\n        elif tag.name == 'h2': return '## ' + tag_text + '\\n'\n        elif tag.name == 'h3': return '### ' + tag.text\n        elif tag.name == 'ul': return convert_html_to_markup(str(tag))\n        elif tag.name == 'li': return '- ' + tag.text + '\\n'\n        else: return tag.text\n    \n    markup = ''\n    for child in soup.contents:\n        if hasattr(child, 'children'):\n            markup += replace_tags(child)\n        else: \n            # Clean the text of any unwanted characters\n            cleaned_text = re.sub(r'\\s+', ' ', child).strip()\n            markup += cleaned_text + '\\n'\n            \n    return markup"
      ]
    },
    {
      "id": 4,
      "type": "Any Python 1 input",
      "pos": [
        1400,
        580
      ],
      "size": {
        "0": 400,
        "1": 200
      },
      "flags": {},
      "order": 2,
      "mode": 0,
      "inputs": [
        {
          "name": "variable",
          "type": "STRING",
          "link": 7,
          "widget": {
            "name": "variable"
          }
        }
      ],
      "outputs": [
        {
          "name": "STRING",
          "type": "STRING",
          "links": [
            8,
            15
          ],
          "shape": 3,
          "slot_index": 0
        }
      ],
      "properties": {
        "Node name for S&R": "Any Python 1 input"
      },
      "widgets_values": [
        "import requests\n\nresponse = requests.get(variable)\n\n# Check if the request was successful\nif response.status_code == 200:\n    html_content = response.text\n    print(html_content)\nelse:\n    print(\"Failed to retrieve the content\")\n",
        "1"
      ]
    },
    {
      "id": 5,
      "type": "ShowText|pysssss",
      "pos": [
        1040,
        880
      ],
      "size": {
        "0": 820,
        "1": 780
      },
      "flags": {},
      "order": 3,
      "mode": 0,
      "inputs": [
        {
          "name": "text",
          "type": "STRING",
          "link": 8,
          "widget": {
            "name": "text"
          }
        }
      ],
      "outputs": [
        {
          "name": "STRING",
          "type": "STRING",
          "links": null,
          "shape": 6
        }
      ],
      "properties": {
        "Node name for S&R": "ShowText|pysssss"
      },
      "widgets_values": [
        "",
        "<!DOCTYPE html>\n<html class=\"client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-sticky-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-1 vector-feature-appearance-enabled vector-feature-appearance-pinned-clientpref-1 vector-feature-night-mode-disabled skin-theme-clientpref-day vector-toc-available\" lang=\"en\" dir=\"ltr\">\n<head>\n<meta charset=\"UTF-8\">\n<title>Stable Diffusion - Wikipedia</title>\n<script>(function(){var className=\"client-js vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-sticky-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-1 vector-feature-appearance-enabled vector-feature-appearance-pinned-clientpref-1 vector-feature-night-mode-disabled skin-theme-clientpref-day vector-toc-available\";var cookie=document.cookie.match(/(?:^|; )enwikimwclientpreferences=([^;]+)/);if(cookie){cookie[1].split('%2C').forEach(function(pref){className=className.replace(new RegExp('(^| )'+pref.replace(/-clientpref-\\w+$|[^\\w-]+/g,'')+'-clientpref-\\\\w+( |$)'),'$1'+pref+'$2');});}document.documentElement.className=className;}());RLCONF={\"wgBreakFrames\":false,\"wgSeparatorTransformTable\":[\"\",\"\"],\"wgDigitTransformTable\":[\n\"\",\"\"],\"wgDefaultDateFormat\":\"dmy\",\"wgMonthNames\":[\"\",\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"],\"wgRequestId\":\"5adc30f2-9b53-4bba-9ef3-eb04153de4a4\",\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":false,\"wgNamespaceNumber\":0,\"wgPageName\":\"Stable_Diffusion\",\"wgTitle\":\"Stable Diffusion\",\"wgCurRevisionId\":1229364285,\"wgRevisionId\":1229364285,\"wgArticleId\":71642695,\"wgIsArticle\":true,\"wgIsRedirect\":false,\"wgAction\":\"view\",\"wgUserName\":null,\"wgUserGroups\":[\"*\"],\"wgCategories\":[\"CS1 maint: multiple names: authors list\",\"CS1 maint: numeric names: authors list\",\"CS1 Japanese-language sources (ja)\",\"Articles with short description\",\"Short description matches Wikidata\",\"Use mdy dates from October 2023\",\"All articles with unsourced statements\",\"Articles with unsourced statements from October 2023\",\"Pages using multiple image with auto scaled images\",\"Commons category link from Wikidata\",\"Artificial intelligence art\",\n\"Deep learning software applications\",\"Text-to-image generation\",\"Unsupervised learning\",\"Art controversies\",\"Works involved in plagiarism controversies\",\"2022 software\"],\"wgPageViewLanguage\":\"en\",\"wgPageContentLanguage\":\"en\",\"wgPageContentModel\":\"wikitext\",\"wgRelevantPageName\":\"Stable_Diffusion\",\"wgRelevantArticleId\":71642695,\"wgIsProbablyEditable\":true,\"wgRelevantPageIsProbablyEditable\":true,\"wgRestrictionEdit\":[],\"wgRestrictionMove\":[],\"wgNoticeProject\":\"wikipedia\",\"wgCiteReferencePreviewsActive\":false,\"wgFlaggedRevsParams\":{\"tags\":{\"status\":{\"levels\":1}}},\"wgMediaViewerOnClick\":true,\"wgMediaViewerEnabledByDefault\":true,\"wgPopupsFlags\":6,\"wgVisualEditor\":{\"pageLanguageCode\":\"en\",\"pageLanguageDir\":\"ltr\",\"pageVariantFallbacks\":\"en\"},\"wgMFDisplayWikibaseDescriptions\":{\"search\":true,\"watchlist\":true,\"tagline\":false,\"nearby\":true},\"wgWMESchemaEditAttemptStepOversample\":false,\"wgWMEPageLength\":60000,\"wgULSCurrentAutonym\":\"English\",\"wgCentralAuthMobileDomain\":false,\n\"wgEditSubmitButtonLabelPublish\":true,\"wgULSPosition\":\"interlanguage\",\"wgULSisCompactLinksEnabled\":false,\"wgVector2022LanguageInHeader\":true,\"wgULSisLanguageSelectorEmpty\":false,\"wgWikibaseItemId\":\"Q113660857\",\"wgCheckUserClientHintsHeadersJsApi\":[\"architecture\",\"bitness\",\"brands\",\"fullVersionList\",\"mobile\",\"model\",\"platform\",\"platformVersion\"],\"GEHomepageSuggestedEditsEnableTopics\":true,\"wgGETopicsMatchModeEnabled\":false,\"wgGEStructuredTaskRejectionReasonTextInputEnabled\":false,\"wgGELevelingUpEnabledForUser\":false};RLSTATE={\"ext.globalCssJs.user.styles\":\"ready\",\"site.styles\":\"ready\",\"user.styles\":\"ready\",\"ext.globalCssJs.user\":\"ready\",\"user\":\"ready\",\"user.options\":\"loading\",\"ext.cite.styles\":\"ready\",\"skins.vector.search.codex.styles\":\"ready\",\"skins.vector.styles\":\"ready\",\"skins.vector.icons\":\"ready\",\"jquery.makeCollapsible.styles\":\"ready\",\"ext.wikimediamessages.styles\":\"ready\",\"ext.visualEditor.desktopArticleTarget.noscript\":\"ready\",\"ext.uls.interlanguage\":\"ready\",\n\"wikibase.client.init\":\"ready\",\"ext.wikimediaBadges\":\"ready\"};RLPAGEMODULES=[\"ext.cite.ux-enhancements\",\"mediawiki.page.media\",\"site\",\"mediawiki.page.ready\",\"jquery.makeCollapsible\",\"mediawiki.toc\",\"skins.vector.js\",\"ext.centralNotice.geoIP\",\"ext.centralNotice.startUp\",\"ext.gadget.ReferenceTooltips\",\"ext.gadget.switcher\",\"ext.urlShortener.toolbar\",\"ext.centralauth.centralautologin\",\"mmv.head\",\"mmv.bootstrap.autostart\",\"ext.popups\",\"ext.visualEditor.desktopArticleTarget.init\",\"ext.visualEditor.targetLoader\",\"ext.echo.centralauth\",\"ext.eventLogging\",\"ext.wikimediaEvents\",\"ext.navigationTiming\",\"ext.uls.interface\",\"ext.cx.eventlogging.campaigns\",\"ext.cx.uls.quick.actions\",\"wikibase.client.vector-2022\",\"ext.checkUser.clientHints\",\"ext.quicksurveys.init\",\"ext.growthExperiments.SuggestedEditSession\"];</script>\n<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.impl(function(){return[\"user.options@12s5i\",function($,jQuery,require,module){mw.user.tokens.set({\"patrolToken\":\"+\\\\\",\"watchToken\":\"+\\\\\",\"csrfToken\":\"+\\\\\"});\n}];});});</script>\n<link rel=\"stylesheet\" href=\"/w/load.php?lang=en&amp;modules=ext.cite.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cext.wikimediamessages.styles%7Cjquery.makeCollapsible.styles%7Cskins.vector.icons%2Cstyles%7Cskins.vector.search.codex.styles%7Cwikibase.client.init&amp;only=styles&amp;skin=vector-2022\">\n<script async=\"\" src=\"/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;raw=1&amp;skin=vector-2022\"></script>\n<meta name=\"ResourceLoaderDynamicStyles\" content=\"\">\n<link rel=\"stylesheet\" href=\"/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector-2022\">\n<meta name=\"generator\" content=\"MediaWiki 1.43.0-wmf.9\">\n<meta name=\"referrer\" content=\"origin\">\n<meta name=\"referrer\" content=\"origin-when-cross-origin\">\n<meta name=\"robots\" content=\"max-image-preview:standard\">\n<meta name=\"format-detection\" content=\"telephone=no\">\n<meta property=\"og:image\" content=\"https://upload.wikimedia.org/wikipedia/commons/d/d3/Astronaut_Riding_a_Horse_%28SDXL%29.jpg\">\n<meta property=\"og:image:width\" content=\"1200\">\n<meta property=\"og:image:height\" content=\"1200\">\n<meta property=\"og:image\" content=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d3/Astronaut_Riding_a_Horse_%28SDXL%29.jpg/800px-Astronaut_Riding_a_Horse_%28SDXL%29.jpg\">\n<meta property=\"og:image:width\" content=\"800\">\n<meta property=\"og:image:height\" content=\"800\">\n<meta property=\"og:image\" content=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d3/Astronaut_Riding_a_Horse_%28SDXL%29.jpg/640px-Astronaut_Riding_a_Horse_%28SDXL%29.jpg\">\n<meta property=\"og:image:width\" content=\"640\">\n<meta property=\"og:image:height\" content=\"640\">\n<meta name=\"viewport\" content=\"width=1120\">\n<meta property=\"og:title\" content=\"Stable Diffusion - Wikipedia\">\n<meta property=\"og:type\" content=\"website\">\n<link rel=\"preconnect\" href=\"//upload.wikimedia.org\">\n<link rel=\"alternate\" media=\"only screen and (max-width: 640px)\" href=\"//en.m.wikipedia.org/wiki/Stable_Diffusion\">\n<link rel=\"alternate\" type=\"application/x-wiki\" title=\"Edit this page\" href=\"/w/index.php?title=Stable_Diffusion&amp;action=edit\">\n<link rel=\"apple-touch-icon\" href=\"/static/apple-touch/wikipedia.png\">\n<link rel=\"icon\" href=\"/static/favicon/wikipedia.ico\">\n<link rel=\"search\" type=\"application/opensearchdescription+xml\" href=\"/w/rest.php/v1/search\" title=\"Wikipedia (en)\">\n<link rel=\"EditURI\" type=\"application/rsd+xml\" href=\"//en.wikipedia.org/w/api.php?action=rsd\">\n<link rel=\"canonical\" href=\"https://en.wikipedia.org/wiki/Stable_Diffusion\">\n<link rel=\"license\" href=\"https://creativecommons.org/licenses/by-sa/4.0/deed.en\">\n<link rel=\"alternate\" type=\"application/atom+xml\" title=\"Wikipedia Atom feed\" href=\"/w/index.php?title=Special:RecentChanges&amp;feed=atom\">\n<link rel=\"dns-prefetch\" href=\"//meta.wikimedia.org\" />\n<link rel=\"dns-prefetch\" href=\"//login.wikimedia.org\">\n</head>\n<body class=\"skin--responsive skin-vector skin-vector-search-vue mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-Stable_Diffusion rootpage-Stable_Diffusion skin-vector-2022 action-view\"><a class=\"mw-jump-link\" href=\"#bodyContent\">Jump to content</a>\n<div class=\"vector-header-container\">\n\t<header class=\"vector-header mw-header\">\n\t\t<div class=\"vector-header-start\">\n\t\t\t<nav class=\"vector-main-menu-landmark\" aria-label=\"Site\">\n\t\t\t\t\n<div id=\"vector-main-menu-dropdown\" class=\"vector-dropdown vector-main-menu-dropdown vector-button-flush-left vector-button-flush-right\"  >\n\t<input type=\"checkbox\" id=\"vector-main-menu-dropdown-checkbox\" role=\"button\" aria-haspopup=\"true\" data-event-name=\"ui.dropdown-vector-main-menu-dropdown\" class=\"vector-dropdown-checkbox \"  aria-label=\"Main menu\"  >\n\t<label id=\"vector-main-menu-dropdown-label\" for=\"vector-main-menu-dropdown-checkbox\" class=\"vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only \" aria-hidden=\"true\"  ><span class=\"vector-icon mw-ui-icon-menu mw-ui-icon-wikimedia-menu\"></span>\n\n<span class=\"vector-dropdown-label-text\">Main menu</span>\n\t</label>\n\t<div class=\"vector-dropdown-content\">\n\n\n\t\t\t\t<div id=\"vector-main-menu-unpinned-container\" class=\"vector-unpinned-container\">\n\t\t\n<div id=\"vector-main-menu\" class=\"vector-main-menu vector-pinnable-element\">\n\t<div\n\tclass=\"vector-pinnable-header vector-main-menu-pinnable-header vector-pinnable-header-unpinned\"\n\tdata-feature-name=\"main-menu-pinned\"\n\tdata-pinnable-element-id=\"vector-main-menu\"\n\tdata-pinned-container-id=\"vector-main-menu-pinned-container\"\n\tdata-unpinned-container-id=\"vector-main-menu-unpinned-container\"\n>\n\t<div class=\"vector-pinnable-header-label\">Main menu</div>\n\t<button class=\"vector-pinnable-header-toggle-button vector-pinnable-header-pin-button\" data-event-name=\"pinnable-header.vector-main-menu.pin\">move to sidebar</button>\n\t<button class=\"vector-pinnable-header-toggle-button vector-pinnable-header-unpin-button\" data-event-name=\"pinnable-header.vector-main-menu.unpin\">hide</button>\n</div>\n\n\t\n<div id=\"p-navigation\" class=\"vector-menu mw-portlet mw-portlet-navigation\"  >\n\t<div class=\"vector-menu-heading\">\n\t\tNavigation\n\t</div>\n\t<div class=\"vector-menu-content\">\n\t\t\n\t\t<ul class=\"vector-menu-content-list\">\n\t\t\t\n\t\t\t<li id=\"n-mainpage-description\" class=\"mw-list-item\"><a href=\"/wiki/Main_Page\" title=\"Visit the main page [z]\" accesskey=\"z\"><span>Main page</span></a></li><li id=\"n-contents\" class=\"mw-list-item\"><a href=\"/wiki/Wikipedia:Contents\" title=\"Guides to browsing Wikipedia\"><span>Contents</span></a></li><li id=\"n-currentevents\" class=\"mw-list-item\"><a href=\"/wiki/Portal:Current_events\" title=\"Articles related to current events\"><span>Current events</span></a></li><li id=\"n-randompage\" class=\"mw-list-item\"><a href=\"/wiki/Special:Random\" title=\"Visit a randomly selected article [x]\" accesskey=\"x\"><span>Random article</span></a></li><li id=\"n-aboutsite\" class=\"mw-list-item\"><a href=\"/wiki/Wikipedia:About\" title=\"Learn about Wikipedia and how it works\"><span>About Wikipedia</span></a></li><li id=\"n-contactpage\" class=\"mw-list-item\"><a href=\"//en.wikipedia.org/wiki/Wikipedia:Contact_us\" title=\"How to contact Wikipedia\"><span>Contact us</span></a></li><li id=\"n-sitesupport\" class=\"mw-list-item\"><a href=\"https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en\" title=\"Support us by donating to the Wikimedia Foundation\"><span>Donate</span></a></li>\n\t\t</ul>\n\t\t\n\t</div>\n</div>\n\n\t\n\t\n<div id=\"p-interaction\" class=\"vector-menu mw-portlet mw-portlet-interaction\"  >\n\t<div class=\"vector-menu-heading\">\n\t\tContribute\n\t</div>\n\t<div class=\"vector-menu-content\">\n\t\t\n\t\t<ul class=\"vector-menu-content-list\">\n\t\t\t\n\t\t\t<li id=\"n-help\" class=\"mw-list-item\"><a href=\"/wiki/Help:Contents\" title=\"Guidance on how to use and edit Wikipedia\"><span>Help</span></a></li><li id=\"n-introduction\" class=\"mw-list-item\"><a href=\"/wiki/Help:Introduction\" title=\"Learn how to edit Wikipedia\"><span>Learn to edit</span></a></li><li id=\"n-portal\" class=\"mw-list-item\"><a href=\"/wiki/Wikipedia:Community_portal\" title=\"The hub for editors\"><span>Community portal</span></a></li><li id=\"n-recentchanges\" class=\"mw-list-item\"><a href=\"/wiki/Special:RecentChanges\" title=\"A list of recent changes to Wikipedia [r]\" accesskey=\"r\"><span>Recent changes</span></a></li><li id=\"n-upload\" class=\"mw-list-item\"><a href=\"/wiki/Wikipedia:File_upload_wizard\" title=\"Add images or other media for use on Wikipedia\"><span>Upload file</span></a></li>\n\t\t</ul>\n\t\t\n\t</div>\n</div>\n\n</div>\n\n\t\t\t\t</div>\n\n\t</div>\n</div>\n\n\t\t</nav>\n\t\t\t\n<a href=\"/wiki/Main_Page\" class=\"mw-logo\">\n\t<img class=\"mw-logo-icon\" src=\"/static/images/icons/wikipedia.png\" alt=\"\" aria-hidden=\"true\" height=\"50\" width=\"50\">\n\t<span class=\"mw-logo-container skin-invert\">\n\t\t<img class=\"mw-logo-wordmark\" alt=\"Wikipedia\" src=\"/static/images/mobile/copyright/wikipedia-wordmark-en.svg\" style=\"width: 7.5em; height: 1.125em;\">\n\t\t<img class=\"mw-logo-tagline\" alt=\"The Free Encyclopedia\" src=\"/static/images/mobile/copyright/wikipedia-tagline-en.svg\" width=\"117\" height=\"13\" style=\"width: 7.3125em; height: 0.8125em;\">\n\t</span>\n</a>\n\n\t\t</div>\n\t\t<div class=\"vector-header-end\">\n\t\t\t\n<div id=\"p-search\" role=\"search\" class=\"vector-search-box-vue  vector-search-box-collapses vector-search-box-show-thumbnail vector-search-box-auto-expand-width vector-search-box\">\n\t<a href=\"/wiki/Special:Search\" class=\"cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only search-toggle\" title=\"Search Wikipedia [f]\" accesskey=\"f\"><span class=\"vector-icon mw-ui-icon-search mw-ui-icon-wikimedia-search\"></span>\n\n<span>Search</span>\n\t</a>\n\t<div class=\"vector-typeahead-search-container\">\n\t\t<div class=\"cdx-typeahead-search cdx-typeahead-search--show-thumbnail cdx-typeahead-search--auto-expand-width\">\n\t\t\t<form action=\"/w/index.php\" id=\"searchform\" class=\"cdx-search-input cdx-search-input--has-end-button\">\n\t\t\t\t<div id=\"simpleSearch\" class=\"cdx-search-input__input-wrapper\"  data-search-loc=\"header-moved\">\n\t\t\t\t\t<div class=\"cdx-text-input cdx-text-input--has-start-icon\">\n\t\t\t\t\t\t<input\n\t\t\t\t\t\t\tclass=\"cdx-text-input__input\"\n\t\t\t\t\t\t\t type=\"search\" name=\"search\" placeholder=\"Search Wikipedia\" aria-label=\"Search Wikipedia\" autocapitalize=\"sentences\" title=\"Search Wikipedia [f]\" accesskey=\"f\" id=\"searchInput\"\n\t\t\t\t\t\t\t>\n\t\t\t\t\t\t<span class=\"cdx-text-input__icon cdx-text-input__start-icon\"></span>\n\t\t\t\t\t</div>\n\t\t\t\t\t<input type=\"hidden\" name=\"title\" value=\"Special:Search\">\n\t\t\t\t</div>\n\t\t\t\t<button class=\"cdx-button cdx-search-input__end-button\">Search</button>\n\t\t\t</form>\n\t\t</div>\n\t</div>\n</div>\n\n\t\t\t<nav class=\"vector-user-links vector-user-links-wide\" aria-label=\"Personal tools\">\n\t<div class=\"vector-user-links-main\">\n\t\n<div id=\"p-vector-user-menu-preferences\" class=\"vector-menu mw-portlet emptyPortlet\"  >\n\t<div class=\"vector-menu-content\">\n\t\t\n\t\t<ul class=\"vector-menu-content-list\">\n\t\t\t\n\t\t\t\n\t\t</ul>\n\t\t\n\t</div>\n</div>\n\n\t\n<div id=\"p-vector-user-menu-userpage\" class=\"vector-menu mw-portlet emptyPortlet\"  >\n\t<div class=\"vector-menu-content\">\n\t\t\n\t\t<ul class=\"vector-menu-content-list\">\n\t\t\t\n\t\t\t\n\t\t</ul>\n\t\t\n\t</div>\n</div>\n\n\t<nav class=\"vector-appearance-landmark\" aria-label=\"Appearance\">\n\t\t\n<div id=\"vector-appearance-dropdown\" class=\"vector-dropdown \"  >\n\t<input type=\"checkbox\" id=\"vector-appearance-dropdown-checkbox\" role=\"button\" aria-haspopup=\"true\" data-event-name=\"ui.dropdown-vector-appearance-dropdown\" class=\"vector-dropdown-checkbox \"  aria-label=\"Appearance\"  >\n\t<label id=\"vector-appearance-dropdown-label\" for=\"vector-appearance-dropdown-checkbox\" class=\"vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only \" aria-hidden=\"true\"  ><span class=\"vector-icon mw-ui-icon-appearance mw-ui-icon-wikimedia-appearance\"></span>\n\n<span class=\"vector-dropdown-label-text\">Appearance</span>\n\t</label>\n\t<div class=\"vector-dropdown-content\">\n\n\n\t\t\t<div id=\"vector-appearance-unpinned-container\" class=\"vector-unpinned-container\">\n\t\t\t\t\n\t\t\t</div>\n\t\t\n\t</div>\n</div>\n\n\t</nav>\n\t\n<div id=\"p-vector-user-menu-notifications\" class=\"vector-menu mw-portlet emptyPortlet\"  >\n\t<div class=\"vector-menu-content\">\n\t\t\n\t\t<ul class=\"vector-menu-content-list\">\n\t\t\t\n\t\t\t\n\t\t</ul>\n\t\t\n\t</div>\n</div>\n\n\t\n<div id=\"p-vector-user-menu-overflow\" class=\"vector-menu mw-portlet\"  >\n\t<div class=\"vector-menu-content\">\n\t\t\n\t\t<ul class=\"vector-menu-content-list\">\n\t\t\t<li id=\"pt-createaccount-2\" class=\"user-links-collapsible-item mw-list-item user-links-collapsible-item\"><a data-mw=\"interface\" href=\"/w/index.php?title=Special:CreateAccount&amp;returnto=Stable+Diffusion\" title=\"You are encouraged to create an account and log in; however, it is not mandatory\" class=\"\"><span>Create account</span></a>\n</li>\n<li id=\"pt-login-2\" class=\"user-links-collapsible-item mw-list-item user-links-collapsible-item\"><a data-mw=\"interface\" href=\"/w/index.php?title=Special:UserLogin&amp;returnto=Stable+Diffusion\" title=\"You&#039;re encouraged to log in; however, it&#039;s not mandatory. [o]\" accesskey=\"o\" class=\"\"><span>Log in</span></a>\n</li>\n\n\t\t\t\n\t\t</ul>\n\t\t\n\t</div>\n</div>\n\n\t</div>\n\t\n<div id=\"vector-user-links-dropdown\" class=\"vector-dropdown vector-user-menu vector-button-flush-right vector-user-menu-logged-out\"  title=\"Log in and more options\" >\n\t<input type=\"checkbox\" id=\"vector-user-links-dropdown-checkbox\" role=\"button\" aria-haspopup=\"true\" data-event-name=\"ui.dropdown-vector-user-links-dropdown\" class=\"vector-dropdown-checkbox \"  aria-label=\"Personal tools\"  >\n\t<label id=\"vector-user-links-dropdown-label\" for=\"vector-user-links-dropdown-checkbox\" class=\"vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only \" aria-hidden=\"true\"  ><span class=\"vector-icon mw-ui-icon-ellipsis mw-ui-icon-wikimedia-ellipsis\"></span>\n\n<span class=\"vector-dropdown-label-text\">Personal tools</span>\n\t</label>\n\t<div class=\"vector-dropdown-content\">\n\n\n\t\t\n<div id=\"p-personal\" class=\"vector-menu mw-portlet mw-portlet-personal user-links-collapsible-item\"  title=\"User menu\" >\n\t<div class=\"vector-menu-content\">\n\t\t\n\t\t<ul class=\"vector-menu-content-list\">\n\t\t\t\n\t\t\t<li id=\"pt-createaccount\" class=\"user-links-collapsible-item mw-list-item\"><a href=\"/w/index.php?title=Special:CreateAccount&amp;returnto=Stable+Diffusion\" title=\"You are encouraged to create an account and log in; however, it is not mandatory\"><span class=\"vector-icon mw-ui-icon-userAdd mw-ui-icon-wikimedia-userAdd\"></span> <span>Create account</span></a></li><li id=\"pt-login\" class=\"user-links-collapsible-item mw-list-item\"><a href=\"/w/index.php?title=Special:UserLogin&amp;returnto=Stable+Diffusion\" title=\"You&#039;re encouraged to log in; however, it&#039;s not mandatory. [o]\" accesskey=\"o\"><span class=\"vector-icon mw-ui-icon-logIn mw-ui-icon-wikimedia-logIn\"></span> <span>Log in</span></a></li>\n\t\t</ul>\n\t\t\n\t</div>\n</div>\n\n<div id=\"p-user-menu-anon-editor\" class=\"vector-menu mw-portlet mw-portlet-user-menu-anon-editor\"  >\n\t<div class=\"vector-menu-heading\">\n\t\tPages for logged out editors <a href=\"/wiki/Help:Introduction\" aria-label=\"Learn more about editing\"><span>learn more</span></a>\n\t</div>\n\t<div class=\"vector-menu-content\">\n\t\t\n\t\t<ul class=\"vector-menu-content-list\">\n\t\t\t\n\t\t\t<li id=\"pt-anoncontribs\" class=\"mw-list-item\"><a href=\"/wiki/Special:MyContributions\" title=\"A list of edits made from this IP address [y]\" accesskey=\"y\"><span>Contributions</span></a></li><li id=\"pt-anontalk\" class=\"mw-list-item\"><a href=\"/wiki/Special:MyTalk\" title=\"Discussion about edits from this IP address [n]\" accesskey=\"n\"><span>Talk</span></a></li>\n\t\t</ul>\n\t\t\n\t</div>\n</div>\n\n\t\n\t</div>\n</div>\n\n</nav>\n\n\t\t</div>\n\t</header>\n</div>\n<div class=\"mw-page-container\">\n\t<div class=\"mw-page-container-inner\">\n\t\t<div class=\"vector-sitenotice-container\">\n\t\t\t<div id=\"siteNotice\" class=\"notheme\"><!-- CentralNotice --></div>\n\t\t</div>\n\t\t<div class=\"vector-column-start\">\n\t\t\t<div class=\"vector-main-menu-container\">\n\t\t<div id=\"mw-navigation\">\n\t\t\t<nav id=\"mw-panel\" class=\"vector-main-menu-landmark\" aria-label=\"Site\">\n\t\t\t\t<div id=\"vector-main-menu-pinned-container\" class=\"vector-pinned-container\">\n\t\t\t\t\n\t\t\t\t</div>\n\t\t</nav>\n\t\t</div>\n\t</div>\n\t<div class=\"vector-sticky-pinned-container\">\n\t\t\t\t<nav id=\"mw-panel-toc\" aria-label=\"Contents\" data-event-name=\"ui.sidebar-toc\" class=\"mw-table-of-contents-container vector-toc-landmark\">\n\t\t\t\t\t<div id=\"vector-toc-pinned-container\" class=\"vector-pinned-container\">\n\t\t\t\t\t<div id=\"vector-toc\" class=\"vector-toc vector-pinnable-element\">\n\t<div\n\tclass=\"vector-pinnable-header vector-toc-pinnable-header vector-pinnable-header-pinned\"\n\tdata-feature-name=\"toc-pinned\"\n\tdata-pinnable-element-id=\"vector-toc\"\n\t\n\t\n>\n\t<h2 class=\"vector-pinnable-header-label\">Contents</h2>\n\t<button class=\"vector-pinnable-header-toggle-button vector-pinnable-header-pin-button\" data-event-name=\"pinnable-header.vector-toc.pin\">move to sidebar</button>\n\t<button class=\"vector-pinnable-header-toggle-button vector-pinnable-header-unpin-button\" data-event-name=\"pinnable-header.vector-toc.unpin\">hide</button>\n</div>\n\n\n\t<ul class=\"vector-toc-contents\" id=\"mw-panel-toc-list\">\n\t\t<li id=\"toc-mw-content-text\"\n\t\t\tclass=\"vector-toc-list-item vector-toc-level-1\">\n\t\t\t<a href=\"#\" class=\"vector-toc-link\">\n\t\t\t\t<div class=\"vector-toc-text\">(Top)</div>\n\t\t\t</a>\n\t\t</li>\n\t\t<li id=\"toc-Development\"\n\t\tclass=\"vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded\">\n\t\t<a class=\"vector-toc-link\" href=\"#Development\">\n\t\t\t<div class=\"vector-toc-text\">\n\t\t\t\t<span class=\"vector-toc-numb\">1</span>\n\t\t\t\t<span>Development</span>\n\t\t\t</div>\n\t\t</a>\n\t\t\n\t\t<ul id=\"toc-Development-sublist\" class=\"vector-toc-list\">\n\t\t</ul>\n\t</li>\n\t<li id=\"toc-Technology\"\n\t\tclass=\"vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded\">\n\t\t<a class=\"vector-toc-link\" href=\"#Technology\">\n\t\t\t<div class=\"vector-toc-text\">\n\t\t\t\t<span class=\"vector-toc-numb\">2</span>\n\t\t\t\t<span>Technology</span>\n\t\t\t</div>\n\t\t</a>\n\t\t\n\t\t\t<button aria-controls=\"toc-Technology-sublist\" class=\"cdx-button cdx-button--weight-quiet cdx-button--icon-only vector-toc-toggle\">\n\t\t\t\t<span class=\"vector-icon mw-ui-icon-wikimedia-expand\"></span>\n\t\t\t\t<span>Toggle Technology subsection</span>\n\t\t\t</button>\n\t\t\n\t\t<ul id=\"toc-Technology-sublist\" class=\"vector-toc-list\">\n\t\t\t<li id=\"toc-Architecture\"\n\t\t\tclass=\"vector-toc-list-item vector-toc-level-2\">\n\t\t\t<a class=\"vector-toc-link\" href=\"#Architecture\">\n\t\t\t\t<div class=\"vector-toc-text\">\n\t\t\t\t\t<span class=\"vector-toc-numb\">2.1</span>\n\t\t\t\t\t<span>Architecture</span>\n\t\t\t\t</div>\n\t\t\t</a>\n\t\t\t\n\t\t\t<ul id=\"toc-Architecture-sublist\" class=\"vector-toc-list\">\n\t\t\t\t<li id=\"toc-SD_XL\"\n\t\t\tclass=\"vector-toc-list-item vector-toc-level-3\">\n\t\t\t<a class=\"vector-toc-link\" href=\"#SD_XL\">\n\t\t\t\t<div class=\"vector-toc-text\">\n\t\t\t\t\t<span class=\"vector-toc-numb\">2.1.1</span>\n\t\t\t\t\t<span>SD XL</span>\n\t\t\t\t</div>\n\t\t\t</a>\n\t\t\t\n\t\t\t<ul id=\"toc-SD_XL-sublist\" class=\"vector-toc-list\">\n\t\t\t</ul>\n\t\t</li>\n\t\t<li id=\"toc-SD_3.0\"\n\t\t\tclass=\"vector-toc-list-item vector-toc-level-3\">\n\t\t\t<a class=\"vector-toc-link\" href=\"#SD_3.0\">\n\t\t\t\t<div class=\"vector-toc-text\">\n\t\t\t\t\t<span class=\"vector-toc-numb\">2.1.2</span>\n\t\t\t\t\t<span>SD 3.0</span>\n\t\t\t\t</div>\n\t\t\t</a>\n\t\t\t\n\t\t\t<ul id=\"toc-SD_3.0-sublist\" class=\"vector-toc-list\">\n\t\t\t</ul>\n\t\t</li>\n\t</ul>\n\t\t</li>\n\t\t<li id=\"toc-Training_data\"\n\t\t\tclass=\"vector-toc-list-item vector-toc-level-2\">\n\t\t\t<a class=\"vector-toc-link\" href=\"#Training_data\">\n\t\t\t\t<div class=\"vector-toc-text\">\n\t\t\t\t\t<span class=\"vector-toc-numb\">2.2</span>\n\t\t\t\t\t<span>Training data</span>\n\t\t\t\t</div>\n\t\t\t</a>\n\t\t\t\n\t\t\t<ul id=\"toc-Training_data-sublist\" class=\"vector-toc-list\">\n\t\t\t</ul>\n\t\t</li>\n\t\t<li id=\"toc-Training_procedures\"\n\t\t\tclass=\"vector-toc-list-item vector-toc-level-2\">\n\t\t\t<a class=\"vector-toc-link\" href=\"#Training_procedures\">\n\t\t\t\t<div class=\"vector-toc-text\">\n\t\t\t\t\t<span class=\"vector-toc-numb\">2.3</span>\n\t\t\t\t\t<span>Training procedures</span>\n\t\t\t\t</div>\n\t\t\t</a>\n\t\t\t\n\t\t\t<ul id=\"toc-Training_procedures-sublist\" class=\"vector-toc-list\">\n\t\t\t</ul>\n\t\t</li>\n\t\t<li id=\"toc-Limitations\"\n\t\t\tclass=\"vector-toc-list-item vector-toc-level-2\">\n\t\t\t<a class=\"vector-toc-link\" href=\"#Limitations\">\n\t\t\t\t<div class=\"vector-toc-text\">\n\t\t\t\t\t<span class=\"vector-toc-numb\">2.4</span>\n\t\t\t\t\t<span>Limitations</span>\n\t\t\t\t</div>\n\t\t\t</a>\n\t\t\t\n\t\t\t<ul id=\"toc-Limitations-sublist\" class=\"vector-toc-list\">\n\t\t\t</ul>\n\t\t</li>\n\t\t<li id=\"toc-End-user_fine-tuning\"\n\t\t\tclass=\"vector-toc-list-item vector-toc-level-2\">\n\t\t\t<a class=\"vector-toc-link\" href=\"#End-user_fine-tuning\">\n\t\t\t\t<div class=\"vector-toc-text\">\n\t\t\t\t\t<span class=\"vector-toc-numb\">2.5</span>\n\t\t\t\t\t<span>End-user fine-tuning</span>\n\t\t\t\t</div>\n\t\t\t</a>\n\t\t\t\n\t\t\t<ul id=\"toc-End-user_fine-tuning-sublist\" class=\"vector-toc-list\">\n\t\t\t</ul>\n\t\t</li>\n\t</ul>\n\t</li>\n\t<li id=\"toc-Capabilities\"\n\t\tclass=\"vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded\">\n\t\t<a class=\"vector-toc-link\" href=\"#Capabilities\">\n\t\t\t<div class=\"vector-toc-text\">\n\t\t\t\t<span class=\"vector-toc-numb\">3</span>\n\t\t\t\t<span>Capabilities</span>\n\t\t\t</div>\n\t\t</a>\n\t\t\n\t\t\t<button aria-controls=\"toc-Capabilities-sublist\" class=\"cdx-button cdx-button--weight-quiet cdx-button--icon-only vector-toc-toggle\">\n\t\t\t\t<span class=\"vector-icon mw-ui-icon-wikimedia-expand\"></span>\n\t\t\t\t<span>Toggle Capabilities subsection</span>\n\t\t\t</button>\n\t\t\n\t\t<ul id=\"toc-Capabilities-sublist\" class=\"vector-toc-list\">\n\t\t\t<li id=\"toc-Text_to_image_generation\"\n\t\t\tclass=\"vector-toc-list-item vector-toc-level-2\">\n\t\t\t<a class=\"vector-toc-link\" href=\"#Text_to_image_generation\">\n\t\t\t\t<div class=\"vector-toc-text\">\n\t\t\t\t\t<span class=\"vector-toc-numb\">3.1</span>\n\t\t\t\t\t<span>Text to image generation</span>\n\t\t\t\t</div>\n\t\t\t</a>\n\t\t\t\n\t\t\t<ul id=\"toc-Text_to_image_generation-sublist\" class=\"vector-toc-list\">\n\t\t\t</ul>\n\t\t</li>\n\t\t<li id=\"toc-Image_modification\"\n\t\t\tclass=\"vector-toc-list-item vector-toc-level-2\">\n\t\t\t<a class=\"vector-toc-link\" href=\"#Image_modification\">\n\t\t\t\t<div class=\"vector-toc-text\">\n\t\t\t\t\t<span class=\"vector-toc-numb\">3.2</span>\n\t\t\t\t\t<span>Image modification</span>\n\t\t\t\t</div>\n\t\t\t</a>\n\t\t\t\n\t\t\t<ul id=\"toc-Image_modification-sublist\" class=\"vector-toc-list\">\n\t\t\t</ul>\n\t\t</li>\n\t\t<li id=\"toc-ControlNet\"\n\t\t\tclass=\"vector-toc-list-item vector-toc-level-2\">\n\t\t\t<a class=\"vector-toc-link\" href=\"#ControlNet\">\n\t\t\t\t<div class=\"vector-toc-text\">\n\t\t\t\t\t<span class=\"vector-toc-numb\">3.3</span>\n\t\t\t\t\t<span>ControlNet</span>\n\t\t\t\t</div>\n\t\t\t</a>\n\t\t\t\n\t\t\t<ul id=\"toc-ControlNet-sublist\" class=\"vector-toc-list\">\n\t\t\t</ul>\n\t\t</li>\n\t</ul>\n\t</li>\n\t<li id=\"toc-Releases\"\n\t\tclass=\"vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded\">\n\t\t<a class=\"vector-toc-link\" href=\"#Releases\">\n\t\t\t<div class=\"vector-toc-text\">\n\t\t\t\t<span class=\"vector-toc-numb\">4</span>\n\t\t\t\t<span>Releases</span>\n\t\t\t</div>\n\t\t</a>\n\t\t\n\t\t<ul id=\"toc-Releases-sublist\" class=\"vector-toc-list\">\n\t\t</ul>\n\t</li>\n\t<li id=\"toc-Usage_and_controversy\"\n\t\tclass=\"vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded\">\n\t\t<a class=\"vector-toc-link\" href=\"#Usage_and_controversy\">\n\t\t\t<div class=\"vector-toc-text\">\n\t\t\t\t<span class=\"vector-toc-numb\">5</span>\n\t\t\t\t<span>Usage and controversy</span>\n\t\t\t</div>\n\t\t</a>\n\t\t\n\t\t\t<button aria-controls=\"toc-Usage_and_controversy-sublist\" class=\"cdx-button cdx-button--weight-quiet cdx-button--icon-only vector-toc-toggle\">\n\t\t\t\t<span class=\"vector-icon mw-ui-icon-wikimedia-expand\"></span>\n\t\t\t\t<span>Toggle Usage and controversy subsection</span>\n\t\t\t</button>\n\t\t\n\t\t<ul id=\"toc-Usage_and_controversy-sublist\" class=\"vector-toc-list\">\n\t\t\t<li id=\"toc-ComfyUI_extension_compromise\"\n\t\t\tclass=\"vector-toc-list-item vector-toc-level-2\">\n\t\t\t<a class=\"vector-toc-link\" href=\"#ComfyUI_extension_compromise\">\n\t\t\t\t<div class=\"vector-toc-text\">\n\t\t\t\t\t<span class=\"vector-toc-numb\">5.1</span>\n\t\t\t\t\t<span>ComfyUI extension compromise</span>\n\t\t\t\t</div>\n\t\t\t</a>\n\t\t\t\n\t\t\t<ul id=\"toc-ComfyUI_extension_compromise-sublist\" class=\"vector-toc-list\">\n\t\t\t</ul>\n\t\t</li>\n\t</ul>\n\t</li>\n\t<li id=\"toc-Litigation\"\n\t\tclass=\"vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded\">\n\t\t<a class=\"vector-toc-link\" href=\"#Litigation\">\n\t\t\t<div class=\"vector-toc-text\">\n\t\t\t\t<span class=\"vector-toc-numb\">6</span>\n\t\t\t\t<span>Litigation</span>\n\t\t\t</div>\n\t\t</a>\n\t\t\n\t\t<ul id=\"toc-Litigation-sublist\" class=\"vector-toc-list\">\n\t\t</ul>\n\t</li>\n\t<li id=\"toc-License\"\n\t\tclass=\"vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded\">\n\t\t<a class=\"vector-toc-link\" href=\"#License\">\n\t\t\t<div class=\"vector-toc-text\">\n\t\t\t\t<span class=\"vector-toc-numb\">7</span>\n\t\t\t\t<span>License</span>\n\t\t\t</div>\n\t\t</a>\n\t\t\n\t\t<ul id=\"toc-License-sublist\" class=\"vector-toc-list\">\n\t\t</ul>\n\t</li>\n\t<li id=\"toc-See_also\"\n\t\tclass=\"vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded\">\n\t\t<a class=\"vector-toc-link\" href=\"#See_also\">\n\t\t\t<div class=\"vector-toc-text\">\n\t\t\t\t<span class=\"vector-toc-numb\">8</span>\n\t\t\t\t<span>See also</span>\n\t\t\t</div>\n\t\t</a>\n\t\t\n\t\t<ul id=\"toc-See_also-sublist\" class=\"vector-toc-list\">\n\t\t</ul>\n\t</li>\n\t<li id=\"toc-References\"\n\t\tclass=\"vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded\">\n\t\t<a class=\"vector-toc-link\" href=\"#References\">\n\t\t\t<div class=\"vector-toc-text\">\n\t\t\t\t<span class=\"vector-toc-numb\">9</span>\n\t\t\t\t<span>References</span>\n\t\t\t</div>\n\t\t</a>\n\t\t\n\t\t<ul id=\"toc-References-sublist\" class=\"vector-toc-list\">\n\t\t</ul>\n\t</li>\n\t<li id=\"toc-External_links\"\n\t\tclass=\"vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded\">\n\t\t<a class=\"vector-toc-link\" href=\"#External_links\">\n\t\t\t<div class=\"vector-toc-text\">\n\t\t\t\t<span class=\"vector-toc-numb\">10</span>\n\t\t\t\t<span>External links</span>\n\t\t\t</div>\n\t\t</a>\n\t\t\n\t\t<ul id=\"toc-External_links-sublist\" class=\"vector-toc-list\">\n\t\t</ul>\n\t</li>\n</ul>\n</div>\n\n\t\t\t\t\t</div>\n\t\t</nav>\n\t\t\t</div>\n\t\t</div>\n\t\t<div class=\"mw-content-container\">\n\t\t\t<main id=\"content\" class=\"mw-body\">\n\t\t\t\t<header class=\"mw-body-header vector-page-titlebar\">\n\t\t\t\t\t<nav aria-label=\"Contents\" class=\"vector-toc-landmark\">\n\t\t\t\t\t\t\n<div id=\"vector-page-titlebar-toc\" class=\"vector-dropdown vector-page-titlebar-toc vector-button-flush-left\"  >\n\t<input type=\"checkbox\" id=\"vector-page-titlebar-toc-checkbox\" role=\"button\" aria-haspopup=\"true\" data-event-name=\"ui.dropdown-vector-page-titlebar-toc\" class=\"vector-dropdown-checkbox \"  aria-label=\"Toggle the table of contents\"  >\n\t<label id=\"vector-page-titlebar-toc-label\" for=\"vector-page-titlebar-toc-checkbox\" class=\"vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only \" aria-hidden=\"true\"  ><span class=\"vector-icon mw-ui-icon-listBullet mw-ui-icon-wikimedia-listBullet\"></span>\n\n<span class=\"vector-dropdown-label-text\">Toggle the table of contents</span>\n\t</label>\n\t<div class=\"vector-dropdown-content\">\n\n\n\t\t\t\t\t\t\t<div id=\"vector-page-titlebar-toc-unpinned-container\" class=\"vector-unpinned-container\">\n\t\t\t</div>\n\t\t\n\t</div>\n</div>\n\n\t\t\t\t\t</nav>\n\t\t\t\t\t<h1 id=\"firstHeading\" class=\"firstHeading mw-first-heading\"><span class=\"mw-page-title-main\">Stable Diffusion</span></h1>\n\t\t\t\t\t\t\t\n<div id=\"p-lang-btn\" class=\"vector-dropdown mw-portlet mw-portlet-lang\"  >\n\t<input type=\"checkbox\" id=\"p-lang-btn-checkbox\" role=\"button\" aria-haspopup=\"true\" data-event-name=\"ui.dropdown-p-lang-btn\" class=\"vector-dropdown-checkbox mw-interlanguage-selector\" aria-label=\"Go to an article in another language. Available in 25 languages\"   >\n\t<label id=\"p-lang-btn-label\" for=\"p-lang-btn-checkbox\" class=\"vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--action-progressive mw-portlet-lang-heading-25\" aria-hidden=\"true\"  ><span class=\"vector-icon mw-ui-icon-language-progressive mw-ui-icon-wikimedia-language-progressive\"></span>\n\n<span class=\"vector-dropdown-label-text\">25 languages</span>\n\t</label>\n\t<div class=\"vector-dropdown-content\">\n\n\t\t<div class=\"vector-menu-content\">\n\t\t\t\n\t\t\t<ul class=\"vector-menu-content-list\">\n\t\t\t\t\n\t\t\t\t<li class=\"interlanguage-link interwiki-az mw-list-item\"><a href=\"https://az.wikipedia.org/wiki/Stable_Diffusion\" title=\"Stable Diffusion – Azerbaijani\" lang=\"az\" hreflang=\"az\" class=\"interlanguage-link-target\"><span>Azərbaycanca</span></a></li><li class=\"interlanguage-link interwiki-bg mw-list-item\"><a href=\"https://bg.wikipedia.org/wiki/Stable_Diffusion\" title=\"Stable Diffusion – Bulgarian\" lang=\"bg\" hreflang=\"bg\" class=\"interlanguage-link-target\"><span>Български</span></a></li><li class=\"interlanguage-link interwiki-ca mw-list-item\"><a href=\"https://ca.wikipedia.org/wiki/Stable_Diffusion\" title=\"Stable Diffusion – Catalan\" lang=\"ca\" hreflang=\"ca\" class=\"interlanguage-link-target\"><span>Català</span></a></li><li class=\"interlanguage-link interwiki-cs mw-list-item\"><a href=\"https://cs.wikipedia.org/wiki/Stable_Diffusion\" title=\"Stable Diffusion – Czech\" lang=\"cs\" hreflang=\"cs\" class=\"interlanguage-link-target\"><span>Čeština</span></a></li><li class=\"interlanguage-link interwiki-de mw-list-item\"><a href=\"https://de.wikipedia.org/wiki/Stable_Diffusion\" title=\"Stable Diffusion – German\" lang=\"de\" hreflang=\"de\" class=\"interlanguage-link-target\"><span>Deutsch</span></a></li><li class=\"interlanguage-link interwiki-es mw-list-item\"><a href=\"https://es.wikipedia.org/wiki/Stable_Diffusion\" title=\"Stable Diffusion – Spanish\" lang=\"es\" hreflang=\"es\" class=\"interlanguage-link-target\"><span>Español</span></a></li><li class=\"interlanguage-link interwiki-fa mw-list-item\"><a href=\"https://fa.wikipedia.org/wiki/%D8%A7%D8%B3%D8%AA%DB%8C%D8%A8%D9%84_%D8%AF%DB%8C%D9%81%DB%8C%D9%88%DA%98%D9%86\" title=\"استیبل دیفیوژن – Persian\" lang=\"fa\" hreflang=\"fa\" class=\"interlanguage-link-target\"><span>فارسی</span></a></li><li class=\"interlanguage-link interwiki-fr mw-list-item\"><a href=\"https://fr.wikipedia.org/wiki/Stable_Diffusion\" title=\"Stable Diffusion – French\" lang=\"fr\" hreflang=\"fr\" class=\"interlanguage-link-target\"><span>Français</span></a></li><li class=\"interlanguage-link interwiki-gl mw-list-item\"><a href=\"https://gl.wikipedia.org/wiki/Stable_Diffusion\" title=\"Stable Diffusion – Galician\" lang=\"gl\" hreflang=\"gl\" class=\"interlanguage-link-target\"><span>Galego</span></a></li><li class=\"interlanguage-link interwiki-ko mw-list-item\"><a href=\"https://ko.wikipedia.org/wiki/%EC%8A%A4%ED%85%8C%EC%9D%B4%EB%B8%94_%EB%94%94%ED%93%A8%EC%A0%84\" title=\"스테이블 디퓨전 – Korean\" lang=\"ko\" hreflang=\"ko\" class=\"interlanguage-link-target\"><span>한국어</span></a></li><li class=\"interlanguage-link interwiki-id mw-list-item\"><a href=\"https://id.wikipedia.org/wiki/Stable_Diffusion\" title=\"Stable Diffusion – Indonesian\" lang=\"id\" hreflang=\"id\" class=\"interlanguage-link-target\"><span>Bahasa Indonesia</span></a></li><li class=\"interlanguage-link interwiki-it mw-list-item\"><a href=\"https://it.wikipedia.org/wiki/Stable_Diffusion\" title=\"Stable Diffusion – Italian\" lang=\"it\" hreflang=\"it\" class=\"interlanguage-link-target\"><span>Italiano</span></a></li><li class=\"interlanguage-link interwiki-he mw-list-item\"><a href=\"https://he.wikipedia.org/wiki/Stable_Diffusion\" title=\"Stable Diffusion – Hebrew\" lang=\"he\" hreflang=\"he\" class=\"interlanguage-link-target\"><span>עברית</span></a></li><li class=\"interlanguage-link interwiki-nl mw-list-item\"><a href=\"https://nl.wikipedia.org/wiki/Stable_Diffusion\" title=\"Stable Diffusion – Dutch\" lang=\"nl\" hreflang=\"nl\" class=\"interlanguage-link-target\"><span>Nederlands</span></a></li><li class=\"interlanguage-link interwiki-ja mw-list-item\"><a href=\"https://ja.wikipedia.org/wiki/Stable_Diffusion\" title=\"Stable Diffusion – Japanese\" lang=\"ja\" hreflang=\"ja\" class=\"interlanguage-link-target\"><span>日本語</span></a></li><li class=\"interlanguage-link interwiki-pt mw-list-item\"><a href=\"https://pt.wikipedia.org/wiki/Stable_Diffusion\" title=\"Stable Diffusion – Portuguese\" lang=\"pt\" hreflang=\"pt\" class=\"interlanguage-link-target\"><span>Português</span></a></li><li class=\"interlanguage-link interwiki-qu mw-list-item\"><a href=\"https://qu.wikipedia.org/wiki/Stable_Diffusion\" title=\"Stable Diffusion – Quechua\" lang=\"qu\" hreflang=\"qu\" class=\"interlanguage-link-target\"><span>Runa Simi</span></a></li><li class=\"interlanguage-link interwiki-ru mw-list-item\"><a href=\"https://ru.wikipedia.org/wiki/Stable_Diffusion\" title=\"Stable Diffusion – Russian\" lang=\"ru\" hreflang=\"ru\" class=\"interlanguage-link-target\"><span>Русский</span></a></li><li class=\"interlanguage-link interwiki-si mw-list-item\"><a href=\"https://si.wikipedia.org/wiki/%E0%B7%83%E0%B7%8A%E0%B6%AE%E0%B7%8F%E0%B6%BA%E0%B7%93_%E0%B7%80%E0%B7%92%E0%B7%83%E0%B6%BB%E0%B6%AB%E0%B6%BA\" title=\"ස්ථායී විසරණය – Sinhala\" lang=\"si\" hreflang=\"si\" class=\"interlanguage-link-target\"><span>සිංහල</span></a></li><li class=\"interlanguage-link interwiki-fi mw-list-item\"><a href=\"https://fi.wikipedia.org/wiki/Stable_Diffusion\" title=\"Stable Diffusion – Finnish\" lang=\"fi\" hreflang=\"fi\" class=\"interlanguage-link-target\"><span>Suomi</span></a></li><li class=\"interlanguage-link interwiki-tr mw-list-item\"><a href=\"https://tr.wikipedia.org/wiki/Stable_Diffusion\" title=\"Stable Diffusion – Turkish\" lang=\"tr\" hreflang=\"tr\" class=\"interlanguage-link-target\"><span>Türkçe</span></a></li><li class=\"interlanguage-link interwiki-uk mw-list-item\"><a href=\"https://uk.wikipedia.org/wiki/Stable_Diffusion\" title=\"Stable Diffusion – Ukrainian\" lang=\"uk\" hreflang=\"uk\" class=\"interlanguage-link-target\"><span>Українська</span></a></li><li class=\"interlanguage-link interwiki-vi mw-list-item\"><a href=\"https://vi.wikipedia.org/wiki/Stable_Diffusion\" title=\"Stable Diffusion – Vietnamese\" lang=\"vi\" hreflang=\"vi\" class=\"interlanguage-link-target\"><span>Tiếng Việt</span></a></li><li class=\"interlanguage-link interwiki-zh-yue mw-list-item\"><a href=\"https://zh-yue.wikipedia.org/wiki/Stable_Diffusion\" title=\"Stable Diffusion – Cantonese\" lang=\"yue\" hreflang=\"yue\" class=\"interlanguage-link-target\"><span>粵語</span></a></li><li class=\"interlanguage-link interwiki-zh mw-list-item\"><a href=\"https://zh.wikipedia.org/wiki/Stable_Diffusion\" title=\"Stable Diffusion – Chinese\" lang=\"zh\" hreflang=\"zh\" class=\"interlanguage-link-target\"><span>中文</span></a></li>\n\t\t\t</ul>\n\t\t\t<div class=\"after-portlet after-portlet-lang\"><span class=\"wb-langlinks-edit wb-langlinks-link\"><a href=\"https://www.wikidata.org/wiki/Special:EntityPage/Q113660857#sitelinks-wikipedia\" title=\"Edit interlanguage links\" class=\"wbc-editpage\">Edit links</a></span></div>\n\t\t</div>\n\n\t</div>\n</div>\n</header>\n\t\t\t\t<div class=\"vector-page-toolbar\">\n\t\t\t\t\t<div class=\"vector-page-toolbar-container\">\n\t\t\t\t\t\t<div id=\"left-navigation\">\n\t\t\t\t\t\t\t<nav aria-label=\"Namespaces\">\n\t\t\t\t\t\t\t\t\n<div id=\"p-associated-pages\" class=\"vector-menu vector-menu-tabs mw-portlet mw-portlet-associated-pages\"  >\n\t<div class=\"vector-menu-content\">\n\t\t\n\t\t<ul class=\"vector-menu-content-list\">\n\t\t\t\n\t\t\t<li id=\"ca-nstab-main\" class=\"selected vector-tab-noicon mw-list-item\"><a href=\"/wiki/Stable_Diffusion\" title=\"View the content page [c]\" accesskey=\"c\"><span>Article</span></a></li><li id=\"ca-talk\" class=\"vector-tab-noicon mw-list-item\"><a href=\"/wiki/Talk:Stable_Diffusion\" rel=\"discussion\" title=\"Discuss improvements to the content page [t]\" accesskey=\"t\"><span>Talk</span></a></li>\n\t\t</ul>\n\t\t\n\t</div>\n</div>\n\n\t\t\t\t\t\t\t\t\n<div id=\"vector-variants-dropdown\" class=\"vector-dropdown emptyPortlet\"  >\n\t<input type=\"checkbox\" id=\"vector-variants-dropdown-checkbox\" role=\"button\" aria-haspopup=\"true\" data-event-name=\"ui.dropdown-vector-variants-dropdown\" class=\"vector-dropdown-checkbox \" aria-label=\"Change language variant\"   >\n\t<label id=\"vector-variants-dropdown-label\" for=\"vector-variants-dropdown-checkbox\" class=\"vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet\" aria-hidden=\"true\"  ><span class=\"vector-dropdown-label-text\">English</span>\n\t</label>\n\t<div class=\"vector-dropdown-content\">\n\n\n\t\t\t\t\t\n<div id=\"p-variants\" class=\"vector-menu mw-portlet mw-portlet-variants emptyPortlet\"  >\n\t<div class=\"vector-menu-content\">\n\t\t\n\t\t<ul class=\"vector-menu-content-list\">\n\t\t\t\n\t\t\t\n\t\t</ul>\n\t\t\n\t</div>\n</div>\n\n\t\t\t\t\n\t</div>\n</div>\n\n\t\t\t\t\t\t\t</nav>\n\t\t\t\t\t\t</div>\n\t\t\t\t\t\t<div id=\"right-navigation\" class=\"vector-collapsible\">\n\t\t\t\t\t\t\t<nav aria-label=\"Views\">\n\t\t\t\t\t\t\t\t\n<div id=\"p-views\" class=\"vector-menu vector-menu-tabs mw-portlet mw-portlet-views\"  >\n\t<div class=\"vector-menu-content\">\n\t\t\n\t\t<ul class=\"vector-menu-content-list\">\n\t\t\t\n\t\t\t<li id=\"ca-view\" class=\"selected vector-tab-noicon mw-list-item\"><a href=\"/wiki/Stable_Diffusion\"><span>Read</span></a></li><li id=\"ca-edit\" class=\"vector-tab-noicon mw-list-item\"><a href=\"/w/index.php?title=Stable_Diffusion&amp;action=edit\" title=\"Edit this page [e]\" accesskey=\"e\"><span>Edit</span></a></li><li id=\"ca-history\" class=\"vector-tab-noicon mw-list-item\"><a href=\"/w/index.php?title=Stable_Diffusion&amp;action=history\" title=\"Past revisions of this page [h]\" accesskey=\"h\"><span>View history</span></a></li>\n\t\t</ul>\n\t\t\n\t</div>\n</div>\n\n\t\t\t\t\t\t\t</nav>\n\t\t\t\t\n\t\t\t\t\t\t\t<nav class=\"vector-page-tools-landmark\" aria-label=\"Page tools\">\n\t\t\t\t\t\t\t\t\n<div id=\"vector-page-tools-dropdown\" class=\"vector-dropdown vector-page-tools-dropdown\"  >\n\t<input type=\"checkbox\" id=\"vector-page-tools-dropdown-checkbox\" role=\"button\" aria-haspopup=\"true\" data-event-name=\"ui.dropdown-vector-page-tools-dropdown\" class=\"vector-dropdown-checkbox \"  aria-label=\"Tools\"  >\n\t<label id=\"vector-page-tools-dropdown-label\" for=\"vector-page-tools-dropdown-checkbox\" class=\"vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet\" aria-hidden=\"true\"  ><span class=\"vector-dropdown-label-text\">Tools</span>\n\t</label>\n\t<div class=\"vector-dropdown-content\">\n\n\n\t\t\t\t\t\t\t\t\t<div id=\"vector-page-tools-unpinned-container\" class=\"vector-unpinned-container\">\n\t\t\t\t\t\t\n<div id=\"vector-page-tools\" class=\"vector-page-tools vector-pinnable-element\">\n\t<div\n\tclass=\"vector-pinnable-header vector-page-tools-pinnable-header vector-pinnable-header-unpinned\"\n\tdata-feature-name=\"page-tools-pinned\"\n\tdata-pinnable-element-id=\"vector-page-tools\"\n\tdata-pinned-container-id=\"vector-page-tools-pinned-container\"\n\tdata-unpinned-container-id=\"vector-page-tools-unpinned-container\"\n>\n\t<div class=\"vector-pinnable-header-label\">Tools</div>\n\t<button class=\"vector-pinnable-header-toggle-button vector-pinnable-header-pin-button\" data-event-name=\"pinnable-header.vector-page-tools.pin\">move to sidebar</button>\n\t<button class=\"vector-pinnable-header-toggle-button vector-pinnable-header-unpin-button\" data-event-name=\"pinnable-header.vector-page-tools.unpin\">hide</button>\n</div>\n\n\t\n<div id=\"p-cactions\" class=\"vector-menu mw-portlet mw-portlet-cactions emptyPortlet vector-has-collapsible-items\"  title=\"More options\" >\n\t<div class=\"vector-menu-heading\">\n\t\tActions\n\t</div>\n\t<div class=\"vector-menu-content\">\n\t\t\n\t\t<ul class=\"vector-menu-content-list\">\n\t\t\t\n\t\t\t<li id=\"ca-more-view\" class=\"selected vector-more-collapsible-item mw-list-item\"><a href=\"/wiki/Stable_Diffusion\"><span>Read</span></a></li><li id=\"ca-more-edit\" class=\"vector-more-collapsible-item mw-list-item\"><a href=\"/w/index.php?title=Stable_Diffusion&amp;action=edit\" title=\"Edit this page [e]\" accesskey=\"e\"><span>Edit</span></a></li><li id=\"ca-more-history\" class=\"vector-more-collapsible-item mw-list-item\"><a href=\"/w/index.php?title=Stable_Diffusion&amp;action=history\"><span>View history</span></a></li>\n\t\t</ul>\n\t\t\n\t</div>\n</div>\n\n<div id=\"p-tb\" class=\"vector-menu mw-portlet mw-portlet-tb\"  >\n\t<div class=\"vector-menu-heading\">\n\t\tGeneral\n\t</div>\n\t<div class=\"vector-menu-content\">\n\t\t\n\t\t<ul class=\"vector-menu-content-list\">\n\t\t\t\n\t\t\t<li id=\"t-whatlinkshere\" class=\"mw-list-item\"><a href=\"/wiki/Special:WhatLinksHere/Stable_Diffusion\" title=\"List of all English Wikipedia pages containing links to this page [j]\" accesskey=\"j\"><span>What links here</span></a></li><li id=\"t-recentchangeslinked\" class=\"mw-list-item\"><a href=\"/wiki/Special:RecentChangesLinked/Stable_Diffusion\" rel=\"nofollow\" title=\"Recent changes in pages linked from this page [k]\" accesskey=\"k\"><span>Related changes</span></a></li><li id=\"t-upload\" class=\"mw-list-item\"><a href=\"/wiki/Wikipedia:File_Upload_Wizard\" title=\"Upload files [u]\" accesskey=\"u\"><span>Upload file</span></a></li><li id=\"t-specialpages\" class=\"mw-list-item\"><a href=\"/wiki/Special:SpecialPages\" title=\"A list of all special pages [q]\" accesskey=\"q\"><span>Special pages</span></a></li><li id=\"t-permalink\" class=\"mw-list-item\"><a href=\"/w/index.php?title=Stable_Diffusion&amp;oldid=1229364285\" title=\"Permanent link to this revision of this page\"><span>Permanent link</span></a></li><li id=\"t-info\" class=\"mw-list-item\"><a href=\"/w/index.php?title=Stable_Diffusion&amp;action=info\" title=\"More information about this page\"><span>Page information</span></a></li><li id=\"t-cite\" class=\"mw-list-item\"><a href=\"/w/index.php?title=Special:CiteThisPage&amp;page=Stable_Diffusion&amp;id=1229364285&amp;wpFormIdentifier=titleform\" title=\"Information on how to cite this page\"><span>Cite this page</span></a></li><li id=\"t-urlshortener\" class=\"mw-list-item\"><a href=\"/w/index.php?title=Special:UrlShortener&amp;url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FStable_Diffusion\"><span>Get shortened URL</span></a></li><li id=\"t-urlshortener-qrcode\" class=\"mw-list-item\"><a href=\"/w/index.php?title=Special:QrCode&amp;url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FStable_Diffusion\"><span>Download QR code</span></a></li><li id=\"t-wikibase\" class=\"mw-list-item\"><a href=\"https://www.wikidata.org/wiki/Special:EntityPage/Q113660857\" title=\"Structured data on this page hosted by Wikidata [g]\" accesskey=\"g\"><span>Wikidata item</span></a></li>\n\t\t</ul>\n\t\t\n\t</div>\n</div>\n\n<div id=\"p-coll-print_export\" class=\"vector-menu mw-portlet mw-portlet-coll-print_export\"  >\n\t<div class=\"vector-menu-heading\">\n\t\tPrint/export\n\t</div>\n\t<div class=\"vector-menu-content\">\n\t\t\n\t\t<ul class=\"vector-menu-content-list\">\n\t\t\t\n\t\t\t<li id=\"coll-download-as-rl\" class=\"mw-list-item\"><a href=\"/w/index.php?title=Special:DownloadAsPdf&amp;page=Stable_Diffusion&amp;action=show-download-screen\" title=\"Download this page as a PDF file\"><span>Download as PDF</span></a></li><li id=\"t-print\" class=\"mw-list-item\"><a href=\"/w/index.php?title=Stable_Diffusion&amp;printable=yes\" title=\"Printable version of this page [p]\" accesskey=\"p\"><span>Printable version</span></a></li>\n\t\t</ul>\n\t\t\n\t</div>\n</div>\n\n<div id=\"p-wikibase-otherprojects\" class=\"vector-menu mw-portlet mw-portlet-wikibase-otherprojects\"  >\n\t<div class=\"vector-menu-heading\">\n\t\tIn other projects\n\t</div>\n\t<div class=\"vector-menu-content\">\n\t\t\n\t\t<ul class=\"vector-menu-content-list\">\n\t\t\t\n\t\t\t<li class=\"wb-otherproject-link wb-otherproject-commons mw-list-item\"><a href=\"https://commons.wikimedia.org/wiki/Category:Stable_Diffusion\" hreflang=\"en\"><span>Wikimedia Commons</span></a></li>\n\t\t</ul>\n\t\t\n\t</div>\n</div>\n\n</div>\n\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\n\t</div>\n</div>\n\n\t\t\t\t\t\t\t</nav>\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t\t<div class=\"vector-column-end\">\n\t\t\t\t\t<div class=\"vector-sticky-pinned-container\">\n\t\t\t\t\t\t<nav class=\"vector-page-tools-landmark\" aria-label=\"Page tools\">\n\t\t\t\t\t\t\t<div id=\"vector-page-tools-pinned-container\" class=\"vector-pinned-container\">\n\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t</nav>\n\t\t\t\t\t\t<nav class=\"vector-appearance-landmark\" aria-label=\"Appearance\">\n\t\t\t\t\t\t\t<div id=\"vector-appearance-pinned-container\" class=\"vector-pinned-container\">\n\t\t\t\t<div id=\"vector-appearance\" class=\"vector-appearance vector-pinnable-element\">\n\t<div\n\tclass=\"vector-pinnable-header vector-appearance-pinnable-header vector-pinnable-header-pinned\"\n\tdata-feature-name=\"appearance-pinned\"\n\tdata-pinnable-element-id=\"vector-appearance\"\n\tdata-pinned-container-id=\"vector-appearance-pinned-container\"\n\tdata-unpinned-container-id=\"vector-appearance-unpinned-container\"\n>\n\t<div class=\"vector-pinnable-header-label\">Appearance</div>\n\t<button class=\"vector-pinnable-header-toggle-button vector-pinnable-header-pin-button\" data-event-name=\"pinnable-header.vector-appearance.pin\">move to sidebar</button>\n\t<button class=\"vector-pinnable-header-toggle-button vector-pinnable-header-unpin-button\" data-event-name=\"pinnable-header.vector-appearance.unpin\">hide</button>\n</div>\n\n\n</div>\n\n\t\t\t\t\t\t\t</div>\n\t\t</nav>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t\t<div id=\"bodyContent\" class=\"vector-body\" aria-labelledby=\"firstHeading\" data-mw-ve-target-container>\n\t\t\t\t\t<div class=\"vector-body-before-content\">\n\t\t\t\t\t\t\t<div class=\"mw-indicators\">\n\t\t</div>\n\n\t\t\t\t\t\t<div id=\"siteSub\" class=\"noprint\">From Wikipedia, the free encyclopedia</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<div id=\"contentSub\"><div id=\"mw-content-subtitle\"></div></div>\n\t\t\t\t\t\n\t\t\t\t\t\n\t\t\t\t\t<div id=\"mw-content-text\" class=\"mw-body-content\"><div class=\"mw-content-ltr mw-parser-output\" lang=\"en\" dir=\"ltr\"><div class=\"shortdescription nomobile noexcerpt noprint searchaux\" style=\"display:none\">Image-generating machine learning model</div>\n<p class=\"mw-empty-elt\">\n</p>\n<style data-mw-deduplicate=\"TemplateStyles:r1229112069\">.mw-parser-output .infobox-subbox{padding:0;border:none;margin:-3px;width:auto;min-width:100%;font-size:100%;clear:none;float:none;background-color:transparent}.mw-parser-output .infobox-3cols-child{margin:auto}.mw-parser-output .infobox .navbar{font-size:100%}body.skin-minerva .mw-parser-output .infobox-header,body.skin-minerva .mw-parser-output .infobox-subheader,body.skin-minerva .mw-parser-output .infobox-above,body.skin-minerva .mw-parser-output .infobox-title,body.skin-minerva .mw-parser-output .infobox-image,body.skin-minerva .mw-parser-output .infobox-full-data,body.skin-minerva .mw-parser-output .infobox-below{text-align:center}html.skin-theme-clientpref-night .mw-parser-output .infobox-full-data:not(.notheme)>div:not(.notheme)[style]{background:#1f1f23!important;color:#f8f9fa}@media(prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .infobox-full-data:not(.notheme) div:not(.notheme){background:#1f1f23!important;color:#f8f9fa}}@media(min-width:640px){body.skin--responsive .mw-parser-output .infobox-table{display:table!important}body.skin--responsive .mw-parser-output .infobox-table>caption{display:table-caption!important}body.skin--responsive .mw-parser-output .infobox-table>tbody{display:table-row-group}body.skin--responsive .mw-parser-output .infobox-table tr{display:table-row!important}body.skin--responsive .mw-parser-output .infobox-table th,body.skin--responsive .mw-parser-output .infobox-table td{padding-left:inherit;padding-right:inherit}}</style><table class=\"infobox vevent\"><caption class=\"infobox-title summary\">Stable Diffusion</caption><tbody><tr><td colspan=\"2\" class=\"infobox-image logo\"><span typeof=\"mw:File\"><a href=\"/wiki/File:Astronaut_Riding_a_Horse_(SDXL).jpg\" class=\"mw-file-description\"><img src=\"//upload.wikimedia.org/wikipedia/commons/thumb/d/d3/Astronaut_Riding_a_Horse_%28SDXL%29.jpg/250px-Astronaut_Riding_a_Horse_%28SDXL%29.jpg\" decoding=\"async\" width=\"250\" height=\"250\" class=\"mw-file-element\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/d/d3/Astronaut_Riding_a_Horse_%28SDXL%29.jpg/375px-Astronaut_Riding_a_Horse_%28SDXL%29.jpg 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/d/d3/Astronaut_Riding_a_Horse_%28SDXL%29.jpg/500px-Astronaut_Riding_a_Horse_%28SDXL%29.jpg 2x\" data-file-width=\"1024\" data-file-height=\"1024\" /></a></span><div class=\"infobox-caption\">An image generated with Stable Diffusion based on the text prompt \"a photograph of an astronaut riding a horse\"</div></td></tr><tr><th scope=\"row\" class=\"infobox-label\" style=\"white-space: nowrap;\"><a href=\"/wiki/Programmer\" title=\"Programmer\">Original author(s)</a></th><td class=\"infobox-data\">Runway, CompVis, and Stability AI</td></tr><tr><th scope=\"row\" class=\"infobox-label\" style=\"white-space: nowrap;\"><a href=\"/wiki/Programmer\" title=\"Programmer\">Developer(s)</a></th><td class=\"infobox-data\">Stability AI</td></tr><tr><th scope=\"row\" class=\"infobox-label\" style=\"white-space: nowrap;\">Initial release</th><td class=\"infobox-data\">August 22, 2022</td></tr><tr style=\"display: none;\"><td colspan=\"2\" class=\"infobox-full-data\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1229112069\"></td></tr><tr><th scope=\"row\" class=\"infobox-label\" style=\"white-space: nowrap;\"><a href=\"/wiki/Software_release_life_cycle\" title=\"Software release life cycle\">Stable release</a></th><td class=\"infobox-data\"><div style=\"margin:0px;\">SDXL 1.0 (model)<sup id=\"cite_ref-release-sdxl1.0_1-0\" class=\"reference\"><a href=\"#cite_note-release-sdxl1.0-1\">&#91;1&#93;</a></sup>\n   / July 26, 2023</div></td></tr><tr style=\"display:none\"><td colspan=\"2\">\n</td></tr><tr><th scope=\"row\" class=\"infobox-label\" style=\"white-space: nowrap;\"><a href=\"/wiki/Repository_(version_control)\" title=\"Repository (version control)\">Repository</a></th><td class=\"infobox-data\"><style data-mw-deduplicate=\"TemplateStyles:r1126788409\">.mw-parser-output .plainlist ol,.mw-parser-output .plainlist ul{line-height:inherit;list-style:none;margin:0;padding:0}.mw-parser-output .plainlist ol li,.mw-parser-output .plainlist ul li{margin-bottom:0}</style><div class=\"plainlist\"><ul><li><span class=\"url\"><a rel=\"nofollow\" class=\"external text\" href=\"https://github.com/Stability-AI/generative-models\">github<wbr />.com<wbr />/Stability-AI<wbr />/generative-models</a></span> <span class=\"mw-valign-text-top noprint\" typeof=\"mw:File/Frameless\"><a href=\"https://www.wikidata.org/wiki/Q113660857#P1324\" title=\"Edit this at Wikidata\"><img alt=\"Edit this at Wikidata\" src=\"//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png\" decoding=\"async\" width=\"10\" height=\"10\" class=\"mw-file-element\" srcset=\"//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/15px-OOjs_UI_icon_edit-ltr-progressive.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/20px-OOjs_UI_icon_edit-ltr-progressive.svg.png 2x\" data-file-width=\"20\" data-file-height=\"20\" /></a></span></li></ul>\n</div></td></tr><tr><th scope=\"row\" class=\"infobox-label\" style=\"white-space: nowrap;\">Written in</th><td class=\"infobox-data\"><a href=\"/wiki/Python_(programming_language)\" title=\"Python (programming language)\">Python</a><sup id=\"cite_ref-2\" class=\"reference\"><a href=\"#cite_note-2\">&#91;2&#93;</a></sup></td></tr><tr><th scope=\"row\" class=\"infobox-label\" style=\"white-space: nowrap;\"><a href=\"/wiki/Software_categories#Categorization_approaches\" title=\"Software categories\">Type</a></th><td class=\"infobox-data\"><a href=\"/wiki/Text-to-image_model\" title=\"Text-to-image model\">Text-to-image model</a></td></tr><tr><th scope=\"row\" class=\"infobox-label\" style=\"white-space: nowrap;\"><a href=\"/wiki/Software_license\" title=\"Software license\">License</a></th><td class=\"infobox-data\">Creative ML OpenRAIL-M</td></tr><tr><th scope=\"row\" class=\"infobox-label\" style=\"white-space: nowrap;\">Website</th><td class=\"infobox-data\"><span class=\"url\"><a rel=\"nofollow\" class=\"external text\" href=\"https://stability.ai/stable-image\">stability<wbr />.ai<wbr />/stable-image</a></span>&#160;<span class=\"penicon autoconfirmed-show\"><span class=\"mw-valign-text-top\" typeof=\"mw:File/Frameless\"><a href=\"https://www.wikidata.org/wiki/Q113660857?uselang=en#P856\" title=\"Edit this on Wikidata\"><img alt=\"Edit this on Wikidata\" src=\"//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png\" decoding=\"async\" width=\"10\" height=\"10\" class=\"mw-file-element\" srcset=\"//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/15px-OOjs_UI_icon_edit-ltr-progressive.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/20px-OOjs_UI_icon_edit-ltr-progressive.svg.png 2x\" data-file-width=\"20\" data-file-height=\"20\" /></a></span></span></td></tr></tbody></table>\n<p><b>Stable Diffusion</b> is a <a href=\"/wiki/Deep_learning\" title=\"Deep learning\">deep learning</a>, <a href=\"/wiki/Text-to-image_model\" title=\"Text-to-image model\">text-to-image model</a> released in 2022 based on <a href=\"/wiki/Diffusion_model\" title=\"Diffusion model\">diffusion</a> techniques. The <a href=\"/wiki/Generative_artificial_intelligence\" title=\"Generative artificial intelligence\">generative artificial intelligence</a> technology is the premier product of <b>Stability AI</b> and is considered to be a part of the ongoing <a href=\"/wiki/AI_boom\" title=\"AI boom\">artificial intelligence boom</a>.\n</p><p>It is primarily used to generate detailed images conditioned on text descriptions, though it can also be applied to other tasks such as <a href=\"/wiki/Inpainting\" title=\"Inpainting\">inpainting</a>, outpainting, and generating image-to-image translations guided by a <a href=\"/wiki/Prompt_engineering\" title=\"Prompt engineering\">text prompt</a>.<sup id=\"cite_ref-:0_3-0\" class=\"reference\"><a href=\"#cite_note-:0-3\">&#91;3&#93;</a></sup> Its development involved  researchers from the CompVis Group at <a href=\"/wiki/Ludwig_Maximilian_University_of_Munich\" title=\"Ludwig Maximilian University of Munich\">Ludwig Maximilian University of Munich</a> and <a href=\"/wiki/Runway_(company)\" title=\"Runway (company)\">Runway</a> with a computational donation from Stability and training data from non-profit organizations.<sup id=\"cite_ref-sifted_financialtimes_4-0\" class=\"reference\"><a href=\"#cite_note-sifted_financialtimes-4\">&#91;4&#93;</a></sup><sup id=\"cite_ref-lmu_lauch_5-0\" class=\"reference\"><a href=\"#cite_note-lmu_lauch-5\">&#91;5&#93;</a></sup><sup id=\"cite_ref-6\" class=\"reference\"><a href=\"#cite_note-6\">&#91;6&#93;</a></sup><sup id=\"cite_ref-stable-diffusion-launch_7-0\" class=\"reference\"><a href=\"#cite_note-stable-diffusion-launch-7\">&#91;7&#93;</a></sup>\n</p><p>Stable Diffusion is a <a href=\"/wiki/Latent_variable_model\" title=\"Latent variable model\">latent</a> <a href=\"/wiki/Diffusion_model\" title=\"Diffusion model\">diffusion model</a>, a kind of deep generative artificial <a href=\"/wiki/Neural_network\" title=\"Neural network\">neural network</a>. Its code and model weights have been released <a href=\"/wiki/Source-available_software\" title=\"Source-available software\">publicly</a>,<sup id=\"cite_ref-stable-diffusion-github_8-0\" class=\"reference\"><a href=\"#cite_note-stable-diffusion-github-8\">&#91;8&#93;</a></sup> and it can run on most consumer hardware equipped with a modest <a href=\"/wiki/Graphics_processing_unit\" title=\"Graphics processing unit\">GPU</a> with at least 4&#160;GB <a href=\"/wiki/Video_random_access_memory\" class=\"mw-redirect\" title=\"Video random access memory\">VRAM</a>. This marked a departure from previous proprietary text-to-image models such as <a href=\"/wiki/DALL-E\" title=\"DALL-E\">DALL-E</a> and <a href=\"/wiki/Midjourney\" title=\"Midjourney\">Midjourney</a> which were accessible only via <a href=\"/wiki/Cloud_service\" class=\"mw-redirect\" title=\"Cloud service\">cloud services</a>.<sup id=\"cite_ref-pcworld_9-0\" class=\"reference\"><a href=\"#cite_note-pcworld-9\">&#91;9&#93;</a></sup><sup id=\"cite_ref-verge_10-0\" class=\"reference\"><a href=\"#cite_note-verge-10\">&#91;10&#93;</a></sup>\n</p>\n<meta property=\"mw:PageProp/toc\" />\n<h2><span class=\"mw-headline\" id=\"Development\">Development</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Stable_Diffusion&amp;action=edit&amp;section=1\" title=\"Edit section: Development\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>Stable Diffusion, originated from a project called <b>Latent Diffusion</b>,<sup id=\"cite_ref-11\" class=\"reference\"><a href=\"#cite_note-11\">&#91;11&#93;</a></sup> developed in Germany by researchers at <a href=\"/wiki/Ludwig_Maximilian_University\" class=\"mw-redirect\" title=\"Ludwig Maximilian University\">Ludwig Maximilian University</a> in <a href=\"/wiki/Munich\" title=\"Munich\">Munich</a> and <a href=\"/wiki/Heidelberg_University\" title=\"Heidelberg University\">Heidelberg University</a>. Four of the original 5 authors (Robin Rombach, Andreas Blattmann, Patrick Esser and Dominik Lorenz) later joined Stability AI and released subsequent versions of Stable Diffusion.<sup id=\"cite_ref-12\" class=\"reference\"><a href=\"#cite_note-12\">&#91;12&#93;</a></sup>\n</p><p>The technical license for the model was released by the CompVis group at Ludwig Maximilian University of Munich.<sup id=\"cite_ref-verge_10-1\" class=\"reference\"><a href=\"#cite_note-verge-10\">&#91;10&#93;</a></sup> Development was led by Patrick Esser of <a href=\"/wiki/Runway_(company)\" title=\"Runway (company)\">Runway</a> and Robin Rombach of CompVis, who were among the researchers who had earlier invented the latent diffusion model architecture used by Stable Diffusion.<sup id=\"cite_ref-stable-diffusion-launch_7-1\" class=\"reference\"><a href=\"#cite_note-stable-diffusion-launch-7\">&#91;7&#93;</a></sup> Stability AI also credited <a href=\"/wiki/EleutherAI\" title=\"EleutherAI\">EleutherAI</a> and <a href=\"/wiki/LAION\" title=\"LAION\">LAION</a> (a German nonprofit which assembled the dataset on which Stable Diffusion was trained) as supporters of the project.<sup id=\"cite_ref-stable-diffusion-launch_7-2\" class=\"reference\"><a href=\"#cite_note-stable-diffusion-launch-7\">&#91;7&#93;</a></sup>\n</p>\n<h2><span class=\"mw-headline\" id=\"Technology\">Technology</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Stable_Diffusion&amp;action=edit&amp;section=2\" title=\"Edit section: Technology\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<figure class=\"mw-default-size mw-halign-right\" typeof=\"mw:File/Thumb\"><a href=\"/wiki/File:Stable_Diffusion_architecture.png\" class=\"mw-file-description\"><img src=\"//upload.wikimedia.org/wikipedia/commons/thumb/f/f6/Stable_Diffusion_architecture.png/290px-Stable_Diffusion_architecture.png\" decoding=\"async\" width=\"290\" height=\"142\" class=\"mw-file-element\" srcset=\"//upload.wikimedia.org/wikipedia/commons/f/f6/Stable_Diffusion_architecture.png 1.5x\" data-file-width=\"435\" data-file-height=\"213\" /></a><figcaption>Diagram of the latent diffusion architecture used by Stable Diffusion</figcaption></figure>\n<figure class=\"mw-halign-right\" typeof=\"mw:File/Thumb\"><a href=\"/wiki/File:X-Y_plot_of_algorithmically-generated_AI_art_of_European-style_castle_in_Japan_demonstrating_DDIM_diffusion_steps.png\" class=\"mw-file-description\"><img src=\"//upload.wikimedia.org/wikipedia/commons/thumb/9/99/X-Y_plot_of_algorithmically-generated_AI_art_of_European-style_castle_in_Japan_demonstrating_DDIM_diffusion_steps.png/300px-X-Y_plot_of_algorithmically-generated_AI_art_of_European-style_castle_in_Japan_demonstrating_DDIM_diffusion_steps.png\" decoding=\"async\" width=\"300\" height=\"203\" class=\"mw-file-element\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/9/99/X-Y_plot_of_algorithmically-generated_AI_art_of_European-style_castle_in_Japan_demonstrating_DDIM_diffusion_steps.png/450px-X-Y_plot_of_algorithmically-generated_AI_art_of_European-style_castle_in_Japan_demonstrating_DDIM_diffusion_steps.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/9/99/X-Y_plot_of_algorithmically-generated_AI_art_of_European-style_castle_in_Japan_demonstrating_DDIM_diffusion_steps.png/600px-X-Y_plot_of_algorithmically-generated_AI_art_of_European-style_castle_in_Japan_demonstrating_DDIM_diffusion_steps.png 2x\" data-file-width=\"2560\" data-file-height=\"1734\" /></a><figcaption>The <a href=\"/wiki/Denoising\" class=\"mw-redirect\" title=\"Denoising\">denoising</a> process used by Stable Diffusion. The model generates images by iteratively denoising <a href=\"/wiki/Random_noise\" class=\"mw-redirect\" title=\"Random noise\">random noise</a> until a configured number of steps have been reached, guided by the CLIP text encoder pretrained on <a href=\"/wiki/Concept\" title=\"Concept\">concepts</a> along with the attention mechanism, resulting in the desired image depicting a representation of the trained concept.</figcaption></figure>\n<h3><span class=\"mw-headline\" id=\"Architecture\">Architecture</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Stable_Diffusion&amp;action=edit&amp;section=3\" title=\"Edit section: Architecture\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>Stable Diffusion uses a kind of <a href=\"/wiki/Diffusion_model\" title=\"Diffusion model\">diffusion model</a> (DM), called a latent diffusion model (LDM), developed by the CompVis group at <a href=\"/wiki/LMU_Munich\" class=\"mw-redirect\" title=\"LMU Munich\">LMU Munich</a>.<sup id=\"cite_ref-paper_13-0\" class=\"reference\"><a href=\"#cite_note-paper-13\">&#91;13&#93;</a></sup><sup id=\"cite_ref-stable-diffusion-github_8-1\" class=\"reference\"><a href=\"#cite_note-stable-diffusion-github-8\">&#91;8&#93;</a></sup> Introduced in 2015, diffusion models are trained with the objective of removing successive applications of <a href=\"/wiki/Gaussian_noise\" title=\"Gaussian noise\">Gaussian noise</a> on training images, which can be thought of as a sequence of <a href=\"/wiki/Denoising_autoencoder\" class=\"mw-redirect\" title=\"Denoising autoencoder\">denoising autoencoders</a>. Stable Diffusion consists of 3 parts: the <a href=\"/wiki/Variational_autoencoder\" title=\"Variational autoencoder\">variational autoencoder</a> (VAE), <a href=\"/wiki/U-Net\" title=\"U-Net\">U-Net</a>, and an optional text encoder.<sup id=\"cite_ref-:02_14-0\" class=\"reference\"><a href=\"#cite_note-:02-14\">&#91;14&#93;</a></sup> The VAE encoder compresses the image from pixel space to a smaller dimensional <a href=\"/wiki/Latent_space\" title=\"Latent space\">latent space</a>, capturing a more fundamental semantic meaning of the image.<sup id=\"cite_ref-paper_13-1\" class=\"reference\"><a href=\"#cite_note-paper-13\">&#91;13&#93;</a></sup> Gaussian noise is iteratively applied to the compressed latent representation during forward diffusion.<sup id=\"cite_ref-:02_14-1\" class=\"reference\"><a href=\"#cite_note-:02-14\">&#91;14&#93;</a></sup> The U-Net block, composed of a <a href=\"/wiki/Residual_neural_network\" title=\"Residual neural network\">ResNet</a> backbone, <a href=\"/wiki/Noise_reduction\" title=\"Noise reduction\">denoises</a> the output from forward diffusion backwards to obtain a latent representation. Finally, the VAE decoder generates the final image by converting the representation back into pixel space.<sup id=\"cite_ref-:02_14-2\" class=\"reference\"><a href=\"#cite_note-:02-14\">&#91;14&#93;</a></sup>\n</p><p>The denoising step can be flexibly conditioned on a string of text, an image, or another modality. The encoded conditioning data is exposed to denoising U-Nets via a <a href=\"/wiki/Attention_(machine_learning)\" title=\"Attention (machine learning)\">cross-attention mechanism</a>.<sup id=\"cite_ref-:02_14-3\" class=\"reference\"><a href=\"#cite_note-:02-14\">&#91;14&#93;</a></sup> For conditioning on text, the fixed, pretrained CLIP ViT-L/14 text encoder is used to transform text prompts to an embedding space.<sup id=\"cite_ref-stable-diffusion-github_8-2\" class=\"reference\"><a href=\"#cite_note-stable-diffusion-github-8\">&#91;8&#93;</a></sup> Researchers point to increased computational efficiency for training and generation as an advantage of LDMs.<sup id=\"cite_ref-stable-diffusion-launch_7-3\" class=\"reference\"><a href=\"#cite_note-stable-diffusion-launch-7\">&#91;7&#93;</a></sup><sup id=\"cite_ref-paper_13-2\" class=\"reference\"><a href=\"#cite_note-paper-13\">&#91;13&#93;</a></sup>\n</p><p>The name <i>diffusion</i> takes inspiration from the <a href=\"/wiki/Thermodynamic\" class=\"mw-redirect\" title=\"Thermodynamic\">thermodynamic</a> <a href=\"/wiki/Diffusion\" title=\"Diffusion\">diffusion</a> and an important link was made between this purely physical field and deep learning in 2015.<sup id=\"cite_ref-15\" class=\"reference\"><a href=\"#cite_note-15\">&#91;15&#93;</a></sup><sup id=\"cite_ref-16\" class=\"reference\"><a href=\"#cite_note-16\">&#91;16&#93;</a></sup>\n</p><p>With 860<span class=\"nowrap\">&#160;</span>million parameters in the U-Net and 123<span class=\"nowrap\">&#160;</span>million in the text encoder, Stable Diffusion is considered relatively lightweight by 2022 standards, and unlike other diffusion models, it can run on <a href=\"/wiki/Consumer_electronics\" title=\"Consumer electronics\">consumer</a> GPUs,<sup id=\"cite_ref-17\" class=\"reference\"><a href=\"#cite_note-17\">&#91;17&#93;</a></sup> and even <a href=\"/wiki/CPU\" class=\"mw-redirect\" title=\"CPU\">CPU</a>-only if using the <a href=\"/wiki/OpenVINO\" title=\"OpenVINO\">OpenVINO</a> version of Stable Diffusion.<sup id=\"cite_ref-18\" class=\"reference\"><a href=\"#cite_note-18\">&#91;18&#93;</a></sup>\n</p>\n<h4><span class=\"mw-headline\" id=\"SD_XL\">SD XL</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Stable_Diffusion&amp;action=edit&amp;section=4\" title=\"Edit section: SD XL\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></h4>\n<p>The XL version uses the same architecture,<sup id=\"cite_ref-:4_19-0\" class=\"reference\"><a href=\"#cite_note-:4-19\">&#91;19&#93;</a></sup> except larger: larger UNet backbone, larger cross-attention context, two text encoders instead of one, and trained on multiple aspect ratios (not just the square aspect ratio like previous versions).\n</p><p>The SD XL Refiner, released at the same time, has the same architecture as SD XL, but it was trained for adding fine details to preexisting images via text-conditional img2img.\n</p>\n<h4><span class=\"mw-headline\" id=\"SD_3.0\">SD 3.0</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Stable_Diffusion&amp;action=edit&amp;section=5\" title=\"Edit section: SD 3.0\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></h4>\n<style data-mw-deduplicate=\"TemplateStyles:r1033289096\">.mw-parser-output .hatnote{font-style:italic}.mw-parser-output div.hatnote{padding-left:1.6em;margin-bottom:0.5em}.mw-parser-output .hatnote i{font-style:normal}.mw-parser-output .hatnote+link+.hatnote{margin-top:-0.5em}</style><div role=\"note\" class=\"hatnote navigation-not-searchable\">Main article: <a href=\"/wiki/Diffusion_model#Rectified_flow\" title=\"Diffusion model\">Diffusion model §&#160;Rectified flow</a></div>\n<p>The 3.0 version<sup id=\"cite_ref-:6_20-0\" class=\"reference\"><a href=\"#cite_note-:6-20\">&#91;20&#93;</a></sup> completely changes the backbone. Not a UNet, but a <i>Rectified Flow Transformer</i>, which implements the rectified flow method<sup id=\"cite_ref-:7_21-0\" class=\"reference\"><a href=\"#cite_note-:7-21\">&#91;21&#93;</a></sup><sup id=\"cite_ref-:8_22-0\" class=\"reference\"><a href=\"#cite_note-:8-22\">&#91;22&#93;</a></sup> with a <a href=\"/wiki/Transformer_(deep_learning_architecture)\" title=\"Transformer (deep learning architecture)\">Transformer</a>.\n</p><p>The Transformer architecture used for SD 3.0 has three \"tracks\", for original text encoding, transformed text encoding, and image encoding (in latent space). The transformed text encoding and image encoding are mixed during each transformer block.\n</p><p>The architecture is named \"multimodal diffusion transformer (MMDiT), where the \"multimodal\" means that it mixes text and image encodings inside its operations. This differs from previous versions of DiT, where the text encoding affects the image encoding, but not vice versa.\n</p>\n<h3><span class=\"mw-headline\" id=\"Training_data\">Training data</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Stable_Diffusion&amp;action=edit&amp;section=6\" title=\"Edit section: Training data\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>Stable Diffusion was trained on pairs of images and captions taken from LAION-5B, a publicly available dataset derived from <a href=\"/wiki/Common_Crawl\" title=\"Common Crawl\">Common Crawl</a> data scraped from the web, where 5 billion image-text pairs were classified based on language and filtered into separate datasets by resolution, a predicted likelihood of containing a watermark, and predicted \"aesthetic\" score (e.g. subjective visual quality).<sup id=\"cite_ref-Waxy_23-0\" class=\"reference\"><a href=\"#cite_note-Waxy-23\">&#91;23&#93;</a></sup> The dataset was created by <a href=\"/wiki/LAION\" title=\"LAION\">LAION</a>, a German non-profit which receives funding from Stability AI.<sup id=\"cite_ref-Waxy_23-1\" class=\"reference\"><a href=\"#cite_note-Waxy-23\">&#91;23&#93;</a></sup><sup id=\"cite_ref-24\" class=\"reference\"><a href=\"#cite_note-24\">&#91;24&#93;</a></sup> The Stable Diffusion model was trained on three subsets of LAION-5B: laion2B-en, laion-high-resolution, and laion-aesthetics v2 5+.<sup id=\"cite_ref-Waxy_23-2\" class=\"reference\"><a href=\"#cite_note-Waxy-23\">&#91;23&#93;</a></sup> A third-party analysis of the model's training data identified that out of a smaller subset of 12&#160;million images taken from the original wider dataset used, approximately 47% of the sample size of images came from 100 different domains, with <a href=\"/wiki/Pinterest\" title=\"Pinterest\">Pinterest</a> taking up 8.5% of the subset, followed by websites such as <a href=\"/wiki/WordPress\" title=\"WordPress\">WordPress</a>, <a href=\"/wiki/Blogspot\" class=\"mw-redirect\" title=\"Blogspot\">Blogspot</a>, <a href=\"/wiki/Flickr\" title=\"Flickr\">Flickr</a>, <a href=\"/wiki/DeviantArt\" title=\"DeviantArt\">DeviantArt</a> and <a href=\"/wiki/Wikimedia_Commons\" title=\"Wikimedia Commons\">Wikimedia Commons</a>.<sup class=\"noprint Inline-Template Template-Fact\" style=\"white-space:nowrap;\">&#91;<i><a href=\"/wiki/Wikipedia:Citation_needed\" title=\"Wikipedia:Citation needed\"><span title=\"This claim needs references to reliable sources. (October 2023)\">citation needed</span></a></i>&#93;</sup>  An investigation by Bayerischer Rundfunk showed that LAION's datasets, hosted on Hugging Face, contain large amounts of private and sensitive data.<sup id=\"cite_ref-:2_25-0\" class=\"reference\"><a href=\"#cite_note-:2-25\">&#91;25&#93;</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Training_procedures\">Training procedures</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Stable_Diffusion&amp;action=edit&amp;section=7\" title=\"Edit section: Training procedures\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>The model was initially trained on the laion2B-en and laion-high-resolution subsets, with the last few rounds of training done on LAION-Aesthetics v2 5+, a subset of 600&#160;million captioned images which the LAION-Aesthetics Predictor V2 predicted that humans would, on average, give a score of at least 5 out of 10 when asked to rate how much they liked them.<sup id=\"cite_ref-26\" class=\"reference\"><a href=\"#cite_note-26\">&#91;26&#93;</a></sup><sup id=\"cite_ref-Waxy_23-3\" class=\"reference\"><a href=\"#cite_note-Waxy-23\">&#91;23&#93;</a></sup><sup id=\"cite_ref-LAION-Aesthetics_27-0\" class=\"reference\"><a href=\"#cite_note-LAION-Aesthetics-27\">&#91;27&#93;</a></sup> The LAION-Aesthetics v2 5+ subset also excluded low-resolution images and images which LAION-5B-WatermarkDetection identified as carrying a <a href=\"/wiki/Watermark\" title=\"Watermark\">watermark</a> with greater than 80% probability.<sup id=\"cite_ref-Waxy_23-4\" class=\"reference\"><a href=\"#cite_note-Waxy-23\">&#91;23&#93;</a></sup> Final rounds of training additionally dropped 10% of text conditioning to improve Classifier-Free Diffusion Guidance.<sup id=\"cite_ref-:5_28-0\" class=\"reference\"><a href=\"#cite_note-:5-28\">&#91;28&#93;</a></sup>\n</p><p>The model was trained using 256 <a href=\"/wiki/Ampere_(microarchitecture)\" title=\"Ampere (microarchitecture)\">Nvidia A100</a> GPUs on <a href=\"/wiki/Amazon_Web_Services\" title=\"Amazon Web Services\">Amazon Web Services</a> for a total of 150,000 GPU-hours, at a cost of $600,000.<sup id=\"cite_ref-29\" class=\"reference\"><a href=\"#cite_note-29\">&#91;29&#93;</a></sup><sup id=\"cite_ref-stable-diffusion-model-card-1-4_30-0\" class=\"reference\"><a href=\"#cite_note-stable-diffusion-model-card-1-4-30\">&#91;30&#93;</a></sup><sup id=\"cite_ref-31\" class=\"reference\"><a href=\"#cite_note-31\">&#91;31&#93;</a></sup>\n</p><p>SD3 was trained at a cost of around $10 million.<sup id=\"cite_ref-32\" class=\"reference\"><a href=\"#cite_note-32\">&#91;32&#93;</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Limitations\">Limitations</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Stable_Diffusion&amp;action=edit&amp;section=8\" title=\"Edit section: Limitations\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>Stable Diffusion has issues with degradation and inaccuracies in certain scenarios. Initial releases of the model were trained on a dataset that consists of 512×512 resolution images, meaning that the quality of generated images noticeably degrades when user specifications deviate from its \"expected\" 512×512 resolution;<sup id=\"cite_ref-diffusers_33-0\" class=\"reference\"><a href=\"#cite_note-diffusers-33\">&#91;33&#93;</a></sup> the version 2.0 update of the Stable Diffusion model later introduced the ability to natively generate images at 768×768 resolution.<sup id=\"cite_ref-release2.0_34-0\" class=\"reference\"><a href=\"#cite_note-release2.0-34\">&#91;34&#93;</a></sup> Another challenge is in generating human limbs due to poor data quality of limbs in the LAION database.<sup id=\"cite_ref-35\" class=\"reference\"><a href=\"#cite_note-35\">&#91;35&#93;</a></sup> The model is insufficiently trained to understand human limbs and faces due to the lack of representative features in the database, and prompting the model to generate images of such type can confound the model.<sup id=\"cite_ref-36\" class=\"reference\"><a href=\"#cite_note-36\">&#91;36&#93;</a></sup> Stable Diffusion XL (SDXL) version 1.0, released in July 2023, introduced native 1024x1024 resolution and improved generation for limbs and text.<sup id=\"cite_ref-37\" class=\"reference\"><a href=\"#cite_note-37\">&#91;37&#93;</a></sup><sup id=\"cite_ref-38\" class=\"reference\"><a href=\"#cite_note-38\">&#91;38&#93;</a></sup>\n</p><p>Accessibility for individual developers can also be a problem. In order to customize the model for new use cases that are not included in the dataset, such as generating <a href=\"/wiki/Anime\" title=\"Anime\">anime</a> characters (\"waifu diffusion\"),<sup id=\"cite_ref-39\" class=\"reference\"><a href=\"#cite_note-39\">&#91;39&#93;</a></sup> new data and further training are required. <a href=\"/wiki/Fine-tuning_(machine_learning)\" class=\"mw-redirect\" title=\"Fine-tuning (machine learning)\">Fine-tuned</a> adaptations of Stable Diffusion created through additional retraining have been used for a variety of different use-cases, from medical imaging<sup id=\"cite_ref-40\" class=\"reference\"><a href=\"#cite_note-40\">&#91;40&#93;</a></sup> to <a href=\"/wiki/Riffusion\" title=\"Riffusion\">algorithmically generated music</a>.<sup id=\"cite_ref-41\" class=\"reference\"><a href=\"#cite_note-41\">&#91;41&#93;</a></sup> However, this fine-tuning process is sensitive to the quality of new data; low resolution images or different resolutions from the original data can not only fail to learn the new task but degrade the overall performance of the model. Even when the model is additionally trained on high quality images, it is difficult for individuals to run models in consumer electronics. For example, the training process for waifu-diffusion requires a minimum 30&#160;GB of <a href=\"/wiki/VRAM\" class=\"mw-redirect\" title=\"VRAM\">VRAM</a>,<sup id=\"cite_ref-42\" class=\"reference\"><a href=\"#cite_note-42\">&#91;42&#93;</a></sup> which exceeds the usual resource provided in such consumer GPUs as <a href=\"/wiki/Nvidia\" title=\"Nvidia\">Nvidia</a>'s <a href=\"/wiki/GeForce_30_series\" title=\"GeForce 30 series\">GeForce 30 series</a>, which has only about 12&#160;GB.<sup id=\"cite_ref-43\" class=\"reference\"><a href=\"#cite_note-43\">&#91;43&#93;</a></sup>\n</p><p>The creators of Stable Diffusion acknowledge the potential for <a href=\"/wiki/Algorithmic_bias\" title=\"Algorithmic bias\">algorithmic bias</a>, as the model was primarily trained on images with English descriptions.<sup id=\"cite_ref-stable-diffusion-model-card-1-4_30-1\" class=\"reference\"><a href=\"#cite_note-stable-diffusion-model-card-1-4-30\">&#91;30&#93;</a></sup> As a result, generated images reinforce social biases and are from a western perspective, as the creators note that the model lacks data from other communities and cultures. The model gives more accurate results for prompts that are written in English in comparison to those written in other languages, with western or white cultures often being the default representation.<sup id=\"cite_ref-stable-diffusion-model-card-1-4_30-2\" class=\"reference\"><a href=\"#cite_note-stable-diffusion-model-card-1-4-30\">&#91;30&#93;</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"End-user_fine-tuning\">End-user fine-tuning</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Stable_Diffusion&amp;action=edit&amp;section=9\" title=\"Edit section: End-user fine-tuning\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>To address the limitations of the model's initial training, end-users may opt to implement additional training to <a href=\"/wiki/Fine-tuning_(machine_learning)\" class=\"mw-redirect\" title=\"Fine-tuning (machine learning)\">fine-tune</a> generation outputs to match more specific use-cases, a process also referred to as <a href=\"/wiki/Text-to-image_personalization\" title=\"Text-to-image personalization\">personalization</a>. There are three methods in which user-accessible fine-tuning can be applied to a Stable Diffusion model checkpoint:\n</p>\n<ul><li>An \"embedding\" can be trained from a collection of user-provided images, and allows the model to generate visually similar images whenever the name of the embedding is used within a generation prompt.<sup id=\"cite_ref-44\" class=\"reference\"><a href=\"#cite_note-44\">&#91;44&#93;</a></sup> Embeddings are based on the \"textual inversion\" concept developed by researchers from <a href=\"/wiki/Tel_Aviv_University\" title=\"Tel Aviv University\">Tel Aviv University</a> in 2022 with support from <a href=\"/wiki/Nvidia\" title=\"Nvidia\">Nvidia</a>, where vector representations for specific tokens used by the model's text encoder are linked to new pseudo-words. Embeddings can be used to reduce biases within the original model, or mimic visual styles.<sup id=\"cite_ref-45\" class=\"reference\"><a href=\"#cite_note-45\">&#91;45&#93;</a></sup></li>\n<li>A \"hypernetwork\" is a small pretrained neural network that is applied to various points within a larger neural network, and refers to the technique created by <a href=\"/wiki/NovelAI\" title=\"NovelAI\">NovelAI</a> developer Kurumuz in 2021, originally intended for text-generation <a href=\"/wiki/Transformer_(machine_learning_model)\" class=\"mw-redirect\" title=\"Transformer (machine learning model)\">transformer models</a>. Hypernetworks steer results towards a particular direction, allowing Stable Diffusion-based models to imitate the art style of specific artists, even if the artist is not recognised by the original model; they process the image by finding key areas of importance such as hair and eyes, and then patch these areas in secondary latent space.<sup id=\"cite_ref-46\" class=\"reference\"><a href=\"#cite_note-46\">&#91;46&#93;</a></sup></li>\n<li><a href=\"/wiki/DreamBooth\" title=\"DreamBooth\">DreamBooth</a> is a deep learning generation model developed by researchers from <a href=\"/wiki/Google\" title=\"Google\">Google Research</a> and <a href=\"/wiki/Boston_University\" title=\"Boston University\">Boston University</a> in 2022 which can fine-tune the model to generate precise, personalised outputs that depict a specific subject, following training via a set of images which depict the subject.<sup id=\"cite_ref-47\" class=\"reference\"><a href=\"#cite_note-47\">&#91;47&#93;</a></sup></li></ul>\n<h2><span class=\"mw-headline\" id=\"Capabilities\">Capabilities</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Stable_Diffusion&amp;action=edit&amp;section=10\" title=\"Edit section: Capabilities\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>The Stable Diffusion model supports the ability to generate new images from scratch through the use of a text prompt describing elements to be included or omitted from the output.<sup id=\"cite_ref-stable-diffusion-github_8-3\" class=\"reference\"><a href=\"#cite_note-stable-diffusion-github-8\">&#91;8&#93;</a></sup> Existing images can be re-drawn by the model to incorporate new elements described by a text prompt (a process known as \"guided image synthesis\"<sup id=\"cite_ref-48\" class=\"reference\"><a href=\"#cite_note-48\">&#91;48&#93;</a></sup>) through its diffusion-denoising mechanism.<sup id=\"cite_ref-stable-diffusion-github_8-4\" class=\"reference\"><a href=\"#cite_note-stable-diffusion-github-8\">&#91;8&#93;</a></sup> In addition, the model also allows the use of prompts to partially alter existing images via <a href=\"/wiki/Inpainting\" title=\"Inpainting\">inpainting</a> and outpainting, when used with an appropriate user interface that supports such features, of which numerous different open source implementations exist.<sup id=\"cite_ref-webui_showcase_49-0\" class=\"reference\"><a href=\"#cite_note-webui_showcase-49\">&#91;49&#93;</a></sup>\n</p><p>Stable Diffusion is recommended to be run with 10&#160;GB or more VRAM, however users with less VRAM may opt to load the weights in <a href=\"/wiki/Float16\" class=\"mw-redirect\" title=\"Float16\">float16</a> precision instead of the default <a href=\"/wiki/Float32\" class=\"mw-redirect\" title=\"Float32\">float32</a> to tradeoff model performance with lower VRAM usage.<sup id=\"cite_ref-diffusers_33-1\" class=\"reference\"><a href=\"#cite_note-diffusers-33\">&#91;33&#93;</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Text_to_image_generation\">Text to image generation</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Stable_Diffusion&amp;action=edit&amp;section=11\" title=\"Edit section: Text to image generation\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<style data-mw-deduplicate=\"TemplateStyles:r1096954695/mw-parser-output/.tmulti\">.mw-parser-output .tmulti .multiimageinner{display:flex;flex-direction:column}.mw-parser-output .tmulti .trow{display:flex;flex-direction:row;clear:left;flex-wrap:wrap;width:100%;box-sizing:border-box}.mw-parser-output .tmulti .tsingle{margin:1px;float:left}.mw-parser-output .tmulti .theader{clear:both;font-weight:bold;text-align:center;align-self:center;background-color:transparent;width:100%}.mw-parser-output .tmulti .thumbcaption{background-color:transparent}.mw-parser-output .tmulti .text-align-left{text-align:left}.mw-parser-output .tmulti .text-align-right{text-align:right}.mw-parser-output .tmulti .text-align-center{text-align:center}@media all and (max-width:720px){.mw-parser-output .tmulti .thumbinner{width:100%!important;box-sizing:border-box;max-width:none!important;align-items:center}.mw-parser-output .tmulti .trow{justify-content:center}.mw-parser-output .tmulti .tsingle{float:none!important;max-width:100%!important;box-sizing:border-box;text-align:center}.mw-parser-output .tmulti .tsingle .thumbcaption{text-align:left}.mw-parser-output .tmulti .trow>.thumbcaption{text-align:center}}</style><div class=\"thumb tmulti tright\"><div class=\"thumbinner multiimageinner\" style=\"width:192px;max-width:192px\"><div class=\"trow\"><div class=\"tsingle\" style=\"width:190px;max-width:190px\"><div class=\"thumbimage\" style=\"height:94px;overflow:hidden\"><span typeof=\"mw:File\"><a href=\"/wiki/File:Algorithmically-generated_landscape_artwork_of_forest_with_Shinto_shrine.png\" class=\"mw-file-description\"><img alt=\"\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/6/6d/Algorithmically-generated_landscape_artwork_of_forest_with_Shinto_shrine.png/188px-Algorithmically-generated_landscape_artwork_of_forest_with_Shinto_shrine.png\" decoding=\"async\" width=\"188\" height=\"94\" class=\"mw-file-element\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/6/6d/Algorithmically-generated_landscape_artwork_of_forest_with_Shinto_shrine.png/282px-Algorithmically-generated_landscape_artwork_of_forest_with_Shinto_shrine.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/6/6d/Algorithmically-generated_landscape_artwork_of_forest_with_Shinto_shrine.png/376px-Algorithmically-generated_landscape_artwork_of_forest_with_Shinto_shrine.png 2x\" data-file-width=\"4096\" data-file-height=\"2048\" /></a></span></div></div></div><div class=\"trow\"><div class=\"tsingle\" style=\"width:190px;max-width:190px\"><div class=\"thumbimage\" style=\"height:94px;overflow:hidden\"><span typeof=\"mw:File\"><a href=\"/wiki/File:Algorithmically-generated_landscape_artwork_of_forest_with_Shinto_shrine_using_negative_prompt_for_green_trees.png\" class=\"mw-file-description\"><img alt=\"\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/6/6d/Algorithmically-generated_landscape_artwork_of_forest_with_Shinto_shrine_using_negative_prompt_for_green_trees.png/188px-Algorithmically-generated_landscape_artwork_of_forest_with_Shinto_shrine_using_negative_prompt_for_green_trees.png\" decoding=\"async\" width=\"188\" height=\"94\" class=\"mw-file-element\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/6/6d/Algorithmically-generated_landscape_artwork_of_forest_with_Shinto_shrine_using_negative_prompt_for_green_trees.png/282px-Algorithmically-generated_landscape_artwork_of_forest_with_Shinto_shrine_using_negative_prompt_for_green_trees.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/6/6d/Algorithmically-generated_landscape_artwork_of_forest_with_Shinto_shrine_using_negative_prompt_for_green_trees.png/376px-Algorithmically-generated_landscape_artwork_of_forest_with_Shinto_shrine_using_negative_prompt_for_green_trees.png 2x\" data-file-width=\"4096\" data-file-height=\"2048\" /></a></span></div></div></div><div class=\"trow\"><div class=\"tsingle\" style=\"width:190px;max-width:190px\"><div class=\"thumbimage\" style=\"height:94px;overflow:hidden\"><span typeof=\"mw:File\"><a href=\"/wiki/File:Algorithmically-generated_landscape_artwork_of_forest_with_Shinto_shrine_using_negative_prompt_for_round_stones.png\" class=\"mw-file-description\"><img alt=\"\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/9/94/Algorithmically-generated_landscape_artwork_of_forest_with_Shinto_shrine_using_negative_prompt_for_round_stones.png/188px-Algorithmically-generated_landscape_artwork_of_forest_with_Shinto_shrine_using_negative_prompt_for_round_stones.png\" decoding=\"async\" width=\"188\" height=\"94\" class=\"mw-file-element\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/9/94/Algorithmically-generated_landscape_artwork_of_forest_with_Shinto_shrine_using_negative_prompt_for_round_stones.png/282px-Algorithmically-generated_landscape_artwork_of_forest_with_Shinto_shrine_using_negative_prompt_for_round_stones.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/9/94/Algorithmically-generated_landscape_artwork_of_forest_with_Shinto_shrine_using_negative_prompt_for_round_stones.png/376px-Algorithmically-generated_landscape_artwork_of_forest_with_Shinto_shrine_using_negative_prompt_for_round_stones.png 2x\" data-file-width=\"4096\" data-file-height=\"2048\" /></a></span></div></div></div><div class=\"trow\" style=\"display:flex\"><div class=\"thumbcaption\">Demonstration of the effect of negative prompts on image generation\n<ul><li><b>Top</b>: no negative prompt</li>\n<li><b>Centre</b>: \"green trees\"</li>\n<li><b>Bottom</b>: \"round stones, round rocks\"</li></ul></div></div></div></div>\n<p>The text to image sampling script within Stable Diffusion, known as \"txt2img\", consumes a text prompt in addition to assorted option parameters covering sampling types, output image dimensions, and seed values. The script outputs an image file based on the model's interpretation of the prompt.<sup id=\"cite_ref-stable-diffusion-github_8-5\" class=\"reference\"><a href=\"#cite_note-stable-diffusion-github-8\">&#91;8&#93;</a></sup> Generated images are tagged with an invisible <a href=\"/wiki/Digital_watermark\" class=\"mw-redirect\" title=\"Digital watermark\">digital watermark</a> to allow users to identify an image as generated by Stable Diffusion,<sup id=\"cite_ref-stable-diffusion-github_8-6\" class=\"reference\"><a href=\"#cite_note-stable-diffusion-github-8\">&#91;8&#93;</a></sup> although this watermark loses its efficacy if the image is resized or rotated.<sup id=\"cite_ref-50\" class=\"reference\"><a href=\"#cite_note-50\">&#91;50&#93;</a></sup>\n</p><p>Each txt2img generation will involve a specific <a href=\"/wiki/Random_seed\" title=\"Random seed\">seed value</a> which affects the output image. Users may opt to randomize the seed in order to explore different generated outputs, or use the same seed to obtain the same image output as a previously generated image.<sup id=\"cite_ref-diffusers_33-2\" class=\"reference\"><a href=\"#cite_note-diffusers-33\">&#91;33&#93;</a></sup> Users are also able to adjust the number of inference steps for the sampler; a higher value takes a longer duration of time, however a smaller value may result in visual defects.<sup id=\"cite_ref-diffusers_33-3\" class=\"reference\"><a href=\"#cite_note-diffusers-33\">&#91;33&#93;</a></sup> Another configurable option, the classifier-free guidance scale value, allows the user to adjust how closely the output image adheres to the prompt.<sup id=\"cite_ref-:5_28-1\" class=\"reference\"><a href=\"#cite_note-:5-28\">&#91;28&#93;</a></sup> More experimentative use cases may opt for a lower scale value, while use cases aiming for more specific outputs may use a higher value.<sup id=\"cite_ref-diffusers_33-4\" class=\"reference\"><a href=\"#cite_note-diffusers-33\">&#91;33&#93;</a></sup>\n</p><p>Additional text2img features are provided by <a href=\"/wiki/Frontend_and_backend\" title=\"Frontend and backend\">front-end</a> implementations of Stable Diffusion, which allow users to modify the weight given to specific parts of the text prompt. Emphasis markers allow users to add or reduce emphasis to keywords by enclosing them with brackets.<sup id=\"cite_ref-51\" class=\"reference\"><a href=\"#cite_note-51\">&#91;51&#93;</a></sup> An alternative method of adjusting weight to parts of the prompt are \"negative prompts\". Negative prompts are a feature included in some front-end implementations, including Stability AI's own DreamStudio cloud service, and allow the user to specify prompts which the model should avoid during image generation. The specified prompts may be undesirable image features that would otherwise be present within image outputs due to the positive prompts provided by the user, or due to how the model was originally trained, with mangled human hands being a common example.<sup id=\"cite_ref-webui_showcase_49-1\" class=\"reference\"><a href=\"#cite_note-webui_showcase-49\">&#91;49&#93;</a></sup><sup id=\"cite_ref-release2.1_52-0\" class=\"reference\"><a href=\"#cite_note-release2.1-52\">&#91;52&#93;</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Image_modification\">Image modification</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Stable_Diffusion&amp;action=edit&amp;section=12\" title=\"Edit section: Image modification\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1096954695/mw-parser-output/.tmulti\"><div class=\"thumb tmulti tright\"><div class=\"thumbinner multiimageinner\" style=\"width:392px;max-width:392px\"><div class=\"trow\"><div class=\"tsingle\" style=\"width:194px;max-width:194px\"><div class=\"thumbimage\" style=\"height:192px;overflow:hidden\"><span typeof=\"mw:File\"><a href=\"/wiki/File:NightCitySphere_(SD1.5).jpg\" class=\"mw-file-description\"><img alt=\"\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/8/82/NightCitySphere_%28SD1.5%29.jpg/192px-NightCitySphere_%28SD1.5%29.jpg\" decoding=\"async\" width=\"192\" height=\"192\" class=\"mw-file-element\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/8/82/NightCitySphere_%28SD1.5%29.jpg/288px-NightCitySphere_%28SD1.5%29.jpg 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/8/82/NightCitySphere_%28SD1.5%29.jpg/384px-NightCitySphere_%28SD1.5%29.jpg 2x\" data-file-width=\"4096\" data-file-height=\"4096\" /></a></span></div></div><div class=\"tsingle\" style=\"width:194px;max-width:194px\"><div class=\"thumbimage\" style=\"height:192px;overflow:hidden\"><span typeof=\"mw:File\"><a href=\"/wiki/File:NightCitySphere_(SDXL).jpg\" class=\"mw-file-description\"><img alt=\"\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/5/56/NightCitySphere_%28SDXL%29.jpg/192px-NightCitySphere_%28SDXL%29.jpg\" decoding=\"async\" width=\"192\" height=\"192\" class=\"mw-file-element\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/5/56/NightCitySphere_%28SDXL%29.jpg/288px-NightCitySphere_%28SDXL%29.jpg 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/5/56/NightCitySphere_%28SDXL%29.jpg/384px-NightCitySphere_%28SDXL%29.jpg 2x\" data-file-width=\"8000\" data-file-height=\"8000\" /></a></span></div></div></div><div class=\"trow\" style=\"display:flex\"><div class=\"thumbcaption\">Demonstration of img2img modification\n<ul><li><b>Left</b>: Original image created with Stable Diffusion 1.5</li>\n<li><b>Right</b>: Modified image created with Stable Diffusion XL 1.0</li></ul></div></div></div></div>\n<p>Stable Diffusion also includes another sampling script, \"img2img\", which consumes a text prompt, path to an existing image, and strength value between 0.0 and 1.0. The script outputs a new image based on the original image that also features elements provided within the text prompt. The strength value denotes the amount of noise added to the output image. A higher strength value produces more variation within the image but may produce an image that is not semantically consistent with the prompt provided.<sup id=\"cite_ref-stable-diffusion-github_8-7\" class=\"reference\"><a href=\"#cite_note-stable-diffusion-github-8\">&#91;8&#93;</a></sup>\n</p><p>The ability of img2img to add noise to the original image makes it potentially useful for <a href=\"/wiki/Data_anonymization\" title=\"Data anonymization\">data anonymization</a> and <a href=\"/wiki/Data_augmentation\" title=\"Data augmentation\">data augmentation</a>, in which the visual features of image data are changed and anonymized.<sup id=\"cite_ref-:1_53-0\" class=\"reference\"><a href=\"#cite_note-:1-53\">&#91;53&#93;</a></sup> The same process may also be useful for image upscaling, in which the resolution of an image is increased, with more detail potentially being added to the image.<sup id=\"cite_ref-:1_53-1\" class=\"reference\"><a href=\"#cite_note-:1-53\">&#91;53&#93;</a></sup> Additionally, Stable Diffusion has been experimented with as a tool for image compression. Compared to <a href=\"/wiki/JPEG\" title=\"JPEG\">JPEG</a> and <a href=\"/wiki/WebP\" title=\"WebP\">WebP</a>, the recent methods used for image compression in Stable Diffusion face limitations in preserving small text and faces.<sup id=\"cite_ref-54\" class=\"reference\"><a href=\"#cite_note-54\">&#91;54&#93;</a></sup>\n</p><p>Additional use-cases for image modification via img2img are offered by numerous front-end implementations of the Stable Diffusion model. Inpainting involves selectively modifying a portion of an existing image delineated by a user-provided <a href=\"/wiki/Layers_(digital_image_editing)#Layer_mask\" title=\"Layers (digital image editing)\">layer mask</a>, which fills the masked space with newly generated content based on the provided prompt.<sup id=\"cite_ref-webui_showcase_49-2\" class=\"reference\"><a href=\"#cite_note-webui_showcase-49\">&#91;49&#93;</a></sup> A dedicated model specifically fine-tuned for inpainting use-cases was created by Stability AI alongside the release of Stable Diffusion 2.0.<sup id=\"cite_ref-release2.0_34-1\" class=\"reference\"><a href=\"#cite_note-release2.0-34\">&#91;34&#93;</a></sup> Conversely, outpainting extends an image beyond its original dimensions, filling the previously empty space with content generated based on the provided prompt.<sup id=\"cite_ref-webui_showcase_49-3\" class=\"reference\"><a href=\"#cite_note-webui_showcase-49\">&#91;49&#93;</a></sup>\n</p><p>A depth-guided model, named \"depth2img\", was introduced with the release of Stable Diffusion 2.0 on November 24, 2022; this model infers the <a href=\"/wiki/Depth_map\" title=\"Depth map\">depth</a> of the provided input image, and generates a new output image based on both the text prompt and the depth information, which allows the coherence and depth of the original input image to be maintained in the generated output.<sup id=\"cite_ref-release2.0_34-2\" class=\"reference\"><a href=\"#cite_note-release2.0-34\">&#91;34&#93;</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"ControlNet\">ControlNet</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Stable_Diffusion&amp;action=edit&amp;section=13\" title=\"Edit section: ControlNet\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>ControlNet<sup id=\"cite_ref-controlnet-paper_55-0\" class=\"reference\"><a href=\"#cite_note-controlnet-paper-55\">&#91;55&#93;</a></sup> is a neural network architecture designed to manage diffusion models by incorporating additional conditions. It duplicates the weights of neural network blocks into a \"locked\" copy and a \"trainable\" copy. The \"trainable\" copy learns the desired condition, while the \"locked\" copy preserves the original model. This approach ensures that training with small datasets of image pairs does not compromise the integrity of production-ready diffusion models. The \"zero convolution\" is a 1×1 convolution with both weight and bias initialized to zero. Before training, all zero convolutions produce zero output, preventing any distortion caused by ControlNet. No layer is trained from scratch; the process is still fine-tuning, keeping the original model secure. This method enables training on small-scale or even personal devices.\n</p>\n<h2><span class=\"mw-headline\" id=\"Releases\">Releases</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Stable_Diffusion&amp;action=edit&amp;section=14\" title=\"Edit section: Releases\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<table class=\"wikitable\">\n<caption>\n</caption>\n<tbody><tr>\n<th>Version number\n</th>\n<th>Release date\n</th>\n<th>Notes\n</th></tr>\n<tr>\n<td>1.1, 1.2, 1.3, 1.4<sup id=\"cite_ref-56\" class=\"reference\"><a href=\"#cite_note-56\">&#91;56&#93;</a></sup>\n</td>\n<td>August 2022\n</td>\n<td>All released by CompVis. There is no \"version 1.0\". 1.1 gave rise to 1.2, and 1.2 gave rise to both 1.3 and 1.4.<sup id=\"cite_ref-57\" class=\"reference\"><a href=\"#cite_note-57\">&#91;57&#93;</a></sup>\n</td></tr>\n<tr>\n<td>1.5<sup id=\"cite_ref-58\" class=\"reference\"><a href=\"#cite_note-58\">&#91;58&#93;</a></sup>\n</td>\n<td>October 2022\n</td>\n<td>Initialized with the weights of 1.2, not 1.4. Released by RunwayML.\n</td></tr>\n<tr>\n<td>2.0<sup id=\"cite_ref-:3_59-0\" class=\"reference\"><a href=\"#cite_note-:3-59\">&#91;59&#93;</a></sup>\n</td>\n<td>November 2022\n</td>\n<td>Retrained from scratch on a filtered dataset.<sup id=\"cite_ref-60\" class=\"reference\"><a href=\"#cite_note-60\">&#91;60&#93;</a></sup>\n</td></tr>\n<tr>\n<td>2.1<sup id=\"cite_ref-61\" class=\"reference\"><a href=\"#cite_note-61\">&#91;61&#93;</a></sup>\n</td>\n<td>December 2022\n</td>\n<td>Initialized with the weights of 2.0.\n</td></tr>\n<tr>\n<td>XL 1.0<sup id=\"cite_ref-62\" class=\"reference\"><a href=\"#cite_note-62\">&#91;62&#93;</a></sup><sup id=\"cite_ref-:4_19-1\" class=\"reference\"><a href=\"#cite_note-:4-19\">&#91;19&#93;</a></sup>\n</td>\n<td>July 2023\n</td>\n<td>The XL 1.0 base model has 3.5 billion parameters, making it\n<p>around 3.5x larger than previous versions.<sup id=\"cite_ref-63\" class=\"reference\"><a href=\"#cite_note-63\">&#91;63&#93;</a></sup>\n</p>\n</td></tr>\n<tr>\n<td>XL Turbo<sup id=\"cite_ref-64\" class=\"reference\"><a href=\"#cite_note-64\">&#91;64&#93;</a></sup>\n</td>\n<td>November 2023\n</td>\n<td>Distilled from XL 1.0 to run in fewer diffusion steps.<sup id=\"cite_ref-65\" class=\"reference\"><a href=\"#cite_note-65\">&#91;65&#93;</a></sup>\n</td></tr>\n<tr>\n<td>3.0<sup id=\"cite_ref-66\" class=\"reference\"><a href=\"#cite_note-66\">&#91;66&#93;</a></sup><sup id=\"cite_ref-:6_20-1\" class=\"reference\"><a href=\"#cite_note-:6-20\">&#91;20&#93;</a></sup>\n</td>\n<td>February 2024 (early preview)\n</td>\n<td>A family of models, ranging from 800M to 8B parameters.\n</td></tr></tbody></table>\n<p>Key papers\n</p>\n<ul><li><i>Learning Transferable Visual Models From Natural Language Supervision</i> (2021).<sup id=\"cite_ref-67\" class=\"reference\"><a href=\"#cite_note-67\">&#91;67&#93;</a></sup> This paper describes the CLIP method for training text encoders, which convert text into floating point vectors. Such text encodings are used by the diffusion model to create images.</li>\n<li><i>SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations</i> (2021).<sup id=\"cite_ref-68\" class=\"reference\"><a href=\"#cite_note-68\">&#91;68&#93;</a></sup> This paper describes SDEdit, aka \"img2img\".</li>\n<li><i>High-Resolution Image Synthesis with Latent Diffusion Models</i> (2021, updated in 2022).<sup id=\"cite_ref-69\" class=\"reference\"><a href=\"#cite_note-69\">&#91;69&#93;</a></sup> This paper describes the latent diffusion model (LDM). This is the backbone of the Stable Diffusion architecture.</li>\n<li><i>Classifier-Free Diffusion Guidance</i> (2022).<sup id=\"cite_ref-:5_28-2\" class=\"reference\"><a href=\"#cite_note-:5-28\">&#91;28&#93;</a></sup> This paper describes CFG, which allows the text encoding vector to steer the diffusion model towards creating the image described by the text.</li>\n<li><i>SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis</i> (2023).<sup id=\"cite_ref-:4_19-2\" class=\"reference\"><a href=\"#cite_note-:4-19\">&#91;19&#93;</a></sup> Describes SDXL.</li>\n<li><i>Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow</i> (2022).<sup id=\"cite_ref-:7_21-1\" class=\"reference\"><a href=\"#cite_note-:7-21\">&#91;21&#93;</a></sup><sup id=\"cite_ref-:8_22-1\" class=\"reference\"><a href=\"#cite_note-:8-22\">&#91;22&#93;</a></sup> Describes rectified flow, which is used for the backbone architecture of SD 3.0.</li>\n<li><i>Scaling Rectified Flow Transformers for High-resolution Image Synthesis</i> (2024).<sup id=\"cite_ref-:6_20-2\" class=\"reference\"><a href=\"#cite_note-:6-20\">&#91;20&#93;</a></sup> Describes SD 3.0.</li></ul>\n<p>Training cost\n</p>\n<ul><li>SD 2.0: 0.2 million hours on A100 (40GB).<sup id=\"cite_ref-:3_59-1\" class=\"reference\"><a href=\"#cite_note-:3-59\">&#91;59&#93;</a></sup></li></ul>\n<h2><span class=\"mw-headline\" id=\"Usage_and_controversy\">Usage and controversy</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Stable_Diffusion&amp;action=edit&amp;section=15\" title=\"Edit section: Usage and controversy\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>Stable Diffusion claims no rights on generated images and freely gives users the rights of usage to any generated images from the model provided that the image content is not illegal or harmful to individuals.<sup id=\"cite_ref-70\" class=\"reference\"><a href=\"#cite_note-70\">&#91;70&#93;</a></sup>\n</p><p>The images Stable Diffusion was trained on have been filtered without human input, leading to some harmful images and large amounts of private and sensitive information appearing in the training data.<sup id=\"cite_ref-:2_25-1\" class=\"reference\"><a href=\"#cite_note-:2-25\">&#91;25&#93;</a></sup>\n</p><p>More traditional visual artists have expressed concern that widespread usage of image synthesis software such as Stable Diffusion may eventually lead to human artists, along with photographers, models, cinematographers, and actors, gradually losing commercial viability against AI-based competitors.<sup id=\"cite_ref-MIT-LAION_71-0\" class=\"reference\"><a href=\"#cite_note-MIT-LAION-71\">&#91;71&#93;</a></sup>\n</p><p>Stable Diffusion is notably more permissive in the types of content users may generate, such as violent or sexually explicit imagery, in comparison to other commercial products based on generative AI.<sup id=\"cite_ref-bijapan_72-0\" class=\"reference\"><a href=\"#cite_note-bijapan-72\">&#91;72&#93;</a></sup> Addressing the concerns that the model may be used for abusive purposes, CEO of Stability AI, <a href=\"/wiki/Emad_Mostaque\" title=\"Emad Mostaque\">Emad Mostaque</a>, argues that \"[it is] peoples' responsibility as to whether they are ethical, moral, and legal in how they operate this technology\",<sup id=\"cite_ref-verge_10-2\" class=\"reference\"><a href=\"#cite_note-verge-10\">&#91;10&#93;</a></sup> and that putting the capabilities of Stable Diffusion into the hands of the public would result in the technology providing a net benefit, in spite of the potential negative consequences.<sup id=\"cite_ref-verge_10-3\" class=\"reference\"><a href=\"#cite_note-verge-10\">&#91;10&#93;</a></sup> In addition, Mostaque argues that the intention behind the open availability of Stable Diffusion is to end corporate control and dominance over such technologies, who have previously only developed closed AI systems for image synthesis.<sup id=\"cite_ref-verge_10-4\" class=\"reference\"><a href=\"#cite_note-verge-10\">&#91;10&#93;</a></sup><sup id=\"cite_ref-bijapan_72-1\" class=\"reference\"><a href=\"#cite_note-bijapan-72\">&#91;72&#93;</a></sup> This is reflected by the fact that any restrictions Stability AI places on the content that users may generate can easily be bypassed due to the availability of the source code.<sup id=\"cite_ref-:13_73-0\" class=\"reference\"><a href=\"#cite_note-:13-73\">&#91;73&#93;</a></sup>\n</p><p>Controversy around photorealistic <a href=\"/wiki/Lolicon\" title=\"Lolicon\">sexualized depictions of underage characters</a> have been brought up, due to such images generated by Stable Diffusion being shared on websites such as <a href=\"/wiki/Pixiv\" title=\"Pixiv\">Pixiv</a>.<sup id=\"cite_ref-74\" class=\"reference\"><a href=\"#cite_note-74\">&#91;74&#93;</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"ComfyUI_extension_compromise\">ComfyUI extension compromise</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Stable_Diffusion&amp;action=edit&amp;section=16\" title=\"Edit section: ComfyUI extension compromise\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>In June 2024, A hacker group called \"Nullbulge\" compromised an extension of a <a href=\"/wiki/Graphical_user_interface\" title=\"Graphical user interface\">graphic user interface</a> for Stable Diffusion called ComfyUI to add malicious code to it. The compromised extension, called ComfyUI_LLMVISION, was used for integrating the interface with AI language models <a href=\"/wiki/GPT-4\" title=\"GPT-4\">GPT-4</a> and <a href=\"/wiki/Claude_3\" class=\"mw-redirect\" title=\"Claude 3\">Claude 3</a>, and was hosted on <a href=\"/wiki/GitHub\" title=\"GitHub\">Github</a>. Nullbulge hosted a list of hundreds of ComfyUI users' login details across multiple services on its website, while users of the extension reported receiving numerous login notifications. vpnMentor conducted security research on the extension and claimed it could \"steal <a href=\"/wiki/Cryptocurrency_wallet\" title=\"Cryptocurrency wallet\">crypto wallets</a>, screenshot the user’s screen, expose device information and <a href=\"/wiki/IP_address\" title=\"IP address\">IP addresses</a>, and steal files that contain certain keywords or extensions\". \n</p><p>Nullbulge's website claims they targeted users who committed \"one of our sins\", which included AI-art generation, art theft, promoting cryptocurrency, and any other kind of theft from artists such as from <a href=\"/wiki/Patreon\" title=\"Patreon\">Patreon</a>. They claimed that they were \"a collective of individuals who believe in the importance of protecting artists' rights and ensuring fair compensation for their work\" and that they believed that \"AI-generated artwork is detrimental to the creative industry and should be discouraged\".<sup id=\"cite_ref-75\" class=\"reference\"><a href=\"#cite_note-75\">&#91;75&#93;</a></sup>\n</p>\n<h2><span class=\"mw-headline\" id=\"Litigation\">Litigation</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Stable_Diffusion&amp;action=edit&amp;section=17\" title=\"Edit section: Litigation\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>In January 2023, three artists, <a href=\"/wiki/Sarah_Andersen\" title=\"Sarah Andersen\">Sarah Andersen</a>, <a href=\"/wiki/Kelly_McKernan\" title=\"Kelly McKernan\">Kelly McKernan</a>, and Karla Ortiz, filed a <a href=\"/wiki/Copyright_infringement\" title=\"Copyright infringement\">copyright infringement</a> lawsuit against Stability AI, <a href=\"/wiki/Midjourney\" title=\"Midjourney\">Midjourney</a>, and <a href=\"/wiki/DeviantArt\" title=\"DeviantArt\">DeviantArt</a>, claiming that these companies have infringed the rights of millions of artists by training AI tools on five billion images scraped from the web without the consent of the original artists.<sup id=\"cite_ref-76\" class=\"reference\"><a href=\"#cite_note-76\">&#91;76&#93;</a></sup> The same month, Stability AI was also sued by <a href=\"/wiki/Getty_Images\" title=\"Getty Images\">Getty Images</a> for using its images in the training data.<sup id=\"cite_ref-CNN-Getty_77-0\" class=\"reference\"><a href=\"#cite_note-CNN-Getty-77\">&#91;77&#93;</a></sup>\n</p><p>In July 2023, U.S. District Judge <a href=\"/wiki/William_Orrick_III\" title=\"William Orrick III\">William Orrick</a> inclined to dismiss most of the lawsuit filed by Andersen, McKernan, and Ortiz but allowed them to file a new complaint.<sup id=\"cite_ref-Reuters-SDLawsuit_78-0\" class=\"reference\"><a href=\"#cite_note-Reuters-SDLawsuit-78\">&#91;78&#93;</a></sup>\n</p>\n<h2><span class=\"mw-headline\" id=\"License\">License</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Stable_Diffusion&amp;action=edit&amp;section=18\" title=\"Edit section: License\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>Unlike models like <a href=\"/wiki/DALL-E\" title=\"DALL-E\">DALL-E</a>, Stable Diffusion makes its <a href=\"/wiki/Source-available_software\" title=\"Source-available software\">source code available</a>,<sup id=\"cite_ref-stability_79-0\" class=\"reference\"><a href=\"#cite_note-stability-79\">&#91;79&#93;</a></sup><sup id=\"cite_ref-stable-diffusion-github_8-8\" class=\"reference\"><a href=\"#cite_note-stable-diffusion-github-8\">&#91;8&#93;</a></sup> along with the model (pretrained weights). It applies the Creative ML OpenRAIL-M license, a form of Responsible AI License (RAIL), to the model (M).<sup id=\"cite_ref-80\" class=\"reference\"><a href=\"#cite_note-80\">&#91;80&#93;</a></sup> The license prohibits certain use cases, including crime, <a href=\"/wiki/Libel\" class=\"mw-redirect\" title=\"Libel\">libel</a>, <a href=\"/wiki/Harassment\" title=\"Harassment\">harassment</a>, <a href=\"/wiki/Doxing\" title=\"Doxing\">doxing</a>, \"<a href=\"/wiki/Child_pornography\" title=\"Child pornography\">exploiting ... minors</a>\", giving medical advice, automatically creating legal obligations, producing legal evidence, and \"discriminating against or harming individuals or groups based on ... social behavior or ... personal or personality characteristics ... [or] <a href=\"/wiki/Anti-discrimination_law\" title=\"Anti-discrimination law\">legally protected characteristics or categories</a>\".<sup id=\"cite_ref-washingtonpost_81-0\" class=\"reference\"><a href=\"#cite_note-washingtonpost-81\">&#91;81&#93;</a></sup><sup id=\"cite_ref-82\" class=\"reference\"><a href=\"#cite_note-82\">&#91;82&#93;</a></sup> The user owns the rights to their generated output images, and is free to use them commercially.<sup id=\"cite_ref-83\" class=\"reference\"><a href=\"#cite_note-83\">&#91;83&#93;</a></sup>\n</p>\n<h2><span class=\"mw-headline\" id=\"See_also\">See also</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Stable_Diffusion&amp;action=edit&amp;section=19\" title=\"Edit section: See also\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<ul><li><a href=\"/wiki/Artificial_intelligence_art\" title=\"Artificial intelligence art\">Artificial intelligence art</a></li>\n<li><a href=\"/wiki/Midjourney\" title=\"Midjourney\">Midjourney</a></li>\n<li><a href=\"/wiki/Craiyon\" class=\"mw-redirect\" title=\"Craiyon\">Craiyon</a></li>\n<li><a href=\"/wiki/Hugging_Face\" title=\"Hugging Face\">Hugging Face</a></li>\n<li><a href=\"/wiki/Imagen_(Google_Brain)\" class=\"mw-redirect\" title=\"Imagen (Google Brain)\">Imagen (Google Brain)</a></li></ul>\n<h2><span class=\"mw-headline\" id=\"References\">References</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Stable_Diffusion&amp;action=edit&amp;section=20\" title=\"Edit section: References\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<style data-mw-deduplicate=\"TemplateStyles:r1217336898\">.mw-parser-output .reflist{font-size:90%;margin-bottom:0.5em;list-style-type:decimal}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman}</style><div class=\"reflist\">\n<div class=\"mw-references-wrap mw-references-columns\"><ol class=\"references\">\n<li id=\"cite_note-release-sdxl1.0-1\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-release-sdxl1.0_1-0\">^</a></b></span> <span class=\"reference-text\"><style data-mw-deduplicate=\"TemplateStyles:r1215172403\">.mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:\"\\\"\"\"\\\"\"\"'\"\"'\"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free.id-lock-free a{background:url(\"//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg\")right 0.1em center/9px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-free a{background-size:contain}.mw-parser-output .id-lock-limited.id-lock-limited a,.mw-parser-output .id-lock-registration.id-lock-registration a{background:url(\"//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg\")right 0.1em center/9px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-limited a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-registration a{background-size:contain}.mw-parser-output .id-lock-subscription.id-lock-subscription a{background:url(\"//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg\")right 0.1em center/9px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-subscription a{background-size:contain}.mw-parser-output .cs1-ws-icon a{background:url(\"//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg\")right 0.1em center/12px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .cs1-ws-icon a{background-size:contain}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#2C882D;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}html.skin-theme-clientpref-night .mw-parser-output .cs1-maint{color:#18911F}html.skin-theme-clientpref-night .mw-parser-output .cs1-visible-error,html.skin-theme-clientpref-night .mw-parser-output .cs1-hidden-error{color:#f8a397}@media(prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .cs1-visible-error,html.skin-theme-clientpref-os .mw-parser-output .cs1-hidden-error{color:#f8a397}html.skin-theme-clientpref-os .mw-parser-output .cs1-maint{color:#18911F}}</style><cite class=\"citation web cs1\"><a rel=\"nofollow\" class=\"external text\" href=\"https://stability.ai/blog/stable-diffusion-sdxl-1-announcement\">\"Announcing SDXL 1.0\"</a>. <i>stability.ai</i>. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20230726215239/https://stability.ai/blog/stable-diffusion-sdxl-1-announcement\">Archived</a> from the original on July 26, 2023.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=stability.ai&amp;rft.atitle=Announcing+SDXL+1.0&amp;rft_id=https%3A%2F%2Fstability.ai%2Fblog%2Fstable-diffusion-sdxl-1-announcement&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-2\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-2\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite id=\"CITEREFRyan_O&#39;Connor2022\" class=\"citation web cs1\">Ryan O'Connor (August 23, 2022). <a rel=\"nofollow\" class=\"external text\" href=\"https://www.assemblyai.com/blog/how-to-run-stable-diffusion-locally-to-generate-images/\">\"How to Run Stable Diffusion Locally to Generate Images\"</a>. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20231013123717/https://www.assemblyai.com/blog/how-to-run-stable-diffusion-locally-to-generate-images/\">Archived</a> from the original on October 13, 2023<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">May 4,</span> 2023</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=How+to+Run+Stable+Diffusion+Locally+to+Generate+Images&amp;rft.date=2022-08-23&amp;rft.au=Ryan+O%27Connor&amp;rft_id=https%3A%2F%2Fwww.assemblyai.com%2Fblog%2Fhow-to-run-stable-diffusion-locally-to-generate-images%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-:0-3\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-:0_3-0\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite class=\"citation web cs1\"><a rel=\"nofollow\" class=\"external text\" href=\"https://huggingface.co/spaces/huggingface/diffuse-the-rest\">\"Diffuse The Rest - a Hugging Face Space by huggingface\"</a>. <i>huggingface.co</i>. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20220905141431/https://huggingface.co/spaces/huggingface/diffuse-the-rest\">Archived</a> from the original on September 5, 2022<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">September 5,</span> 2022</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=huggingface.co&amp;rft.atitle=Diffuse+The+Rest+-+a+Hugging+Face+Space+by+huggingface&amp;rft_id=https%3A%2F%2Fhuggingface.co%2Fspaces%2Fhuggingface%2Fdiffuse-the-rest&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-sifted_financialtimes-4\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-sifted_financialtimes_4-0\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite class=\"citation web cs1\"><a rel=\"nofollow\" class=\"external text\" href=\"https://sifted.eu/articles/stability-ai-fundraise-leak\">\"Leaked deck raises questions over Stability AI's Series A pitch to investors\"</a>. <i>sifted.eu</i>. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20230629201917/https://sifted.eu/articles/stability-ai-fundraise-leak\">Archived</a> from the original on June 29, 2023<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">June 20,</span> 2023</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=sifted.eu&amp;rft.atitle=Leaked+deck+raises+questions+over+Stability+AI%27s+Series+A+pitch+to+investors&amp;rft_id=https%3A%2F%2Fsifted.eu%2Farticles%2Fstability-ai-fundraise-leak&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-lmu_lauch-5\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-lmu_lauch_5-0\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite class=\"citation web cs1\"><a rel=\"nofollow\" class=\"external text\" href=\"https://www.lmu.de/en/newsroom/news-overview/news/revolutionizing-image-generation-by-ai-turning-text-into-images.html\">\"Revolutionizing image generation by AI: Turning text into images\"</a>. <i>www.lmu.de</i>. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20220917200820/https://www.lmu.de/en/newsroom/news-overview/news/revolutionizing-image-generation-by-ai-turning-text-into-images.html\">Archived</a> from the original on September 17, 2022<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">June 21,</span> 2023</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=www.lmu.de&amp;rft.atitle=Revolutionizing+image+generation+by+AI%3A+Turning+text+into+images&amp;rft_id=https%3A%2F%2Fwww.lmu.de%2Fen%2Fnewsroom%2Fnews-overview%2Fnews%2Frevolutionizing-image-generation-by-ai-turning-text-into-images.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-6\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-6\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite id=\"CITEREFMostaque2022\" class=\"citation web cs1\">Mostaque, Emad (November 2, 2022). <a rel=\"nofollow\" class=\"external text\" href=\"https://twitter.com/EMostaque/status/1587844074064822274?lang=en\">\"Stable Diffusion came from the Machine Vision &amp; Learning research group (CompVis) @LMU_Muenchen\"</a>. <i>Twitter</i>. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20230720002303/https://twitter.com/EMostaque/status/1587844074064822274?lang=en\">Archived</a> from the original on July 20, 2023<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">June 22,</span> 2023</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Twitter&amp;rft.atitle=Stable+Diffusion+came+from+the+Machine+Vision+%26+Learning+research+group+%28CompVis%29+%40LMU_Muenchen&amp;rft.date=2022-11-02&amp;rft.aulast=Mostaque&amp;rft.aufirst=Emad&amp;rft_id=https%3A%2F%2Ftwitter.com%2FEMostaque%2Fstatus%2F1587844074064822274%3Flang%3Den&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-stable-diffusion-launch-7\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-stable-diffusion-launch_7-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-stable-diffusion-launch_7-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-stable-diffusion-launch_7-2\"><sup><i><b>c</b></i></sup></a> <a href=\"#cite_ref-stable-diffusion-launch_7-3\"><sup><i><b>d</b></i></sup></a></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite class=\"citation web cs1\"><a rel=\"nofollow\" class=\"external text\" href=\"https://stability.ai/blog/stable-diffusion-announcement\">\"Stable Diffusion Launch Announcement\"</a>. <i>Stability.Ai</i>. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20220905105009/https://stability.ai/blog/stable-diffusion-announcement\">Archived</a> from the original on September 5, 2022<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">September 6,</span> 2022</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Stability.Ai&amp;rft.atitle=Stable+Diffusion+Launch+Announcement&amp;rft_id=https%3A%2F%2Fstability.ai%2Fblog%2Fstable-diffusion-announcement&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-stable-diffusion-github-8\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-stable-diffusion-github_8-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-stable-diffusion-github_8-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-stable-diffusion-github_8-2\"><sup><i><b>c</b></i></sup></a> <a href=\"#cite_ref-stable-diffusion-github_8-3\"><sup><i><b>d</b></i></sup></a> <a href=\"#cite_ref-stable-diffusion-github_8-4\"><sup><i><b>e</b></i></sup></a> <a href=\"#cite_ref-stable-diffusion-github_8-5\"><sup><i><b>f</b></i></sup></a> <a href=\"#cite_ref-stable-diffusion-github_8-6\"><sup><i><b>g</b></i></sup></a> <a href=\"#cite_ref-stable-diffusion-github_8-7\"><sup><i><b>h</b></i></sup></a> <a href=\"#cite_ref-stable-diffusion-github_8-8\"><sup><i><b>i</b></i></sup></a></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite class=\"citation web cs1\"><a rel=\"nofollow\" class=\"external text\" href=\"https://github.com/CompVis/stable-diffusion\">\"Stable Diffusion Repository on GitHub\"</a>. CompVis - Machine Vision and Learning Research Group, LMU Munich. September 17, 2022. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20230118183342/https://github.com/CompVis/stable-diffusion\">Archived</a> from the original on January 18, 2023<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">September 17,</span> 2022</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Stable+Diffusion+Repository+on+GitHub&amp;rft.pub=CompVis+-+Machine+Vision+and+Learning+Research+Group%2C+LMU+Munich&amp;rft.date=2022-09-17&amp;rft_id=https%3A%2F%2Fgithub.com%2FCompVis%2Fstable-diffusion&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-pcworld-9\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-pcworld_9-0\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite class=\"citation web cs1\"><a rel=\"nofollow\" class=\"external text\" href=\"https://www.pcworld.com/article/916785/creating-ai-art-local-pc-stable-diffusion.html\">\"The new killer app: Creating AI art will absolutely crush your PC\"</a>. <i>PCWorld</i>. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20220831065139/https://www.pcworld.com/article/916785/creating-ai-art-local-pc-stable-diffusion.html\">Archived</a> from the original on August 31, 2022<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">August 31,</span> 2022</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=PCWorld&amp;rft.atitle=The+new+killer+app%3A+Creating+AI+art+will+absolutely+crush+your+PC&amp;rft_id=https%3A%2F%2Fwww.pcworld.com%2Farticle%2F916785%2Fcreating-ai-art-local-pc-stable-diffusion.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-verge-10\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-verge_10-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-verge_10-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-verge_10-2\"><sup><i><b>c</b></i></sup></a> <a href=\"#cite_ref-verge_10-3\"><sup><i><b>d</b></i></sup></a> <a href=\"#cite_ref-verge_10-4\"><sup><i><b>e</b></i></sup></a></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite id=\"CITEREFVincent2022\" class=\"citation web cs1\">Vincent, James (September 15, 2022). <a rel=\"nofollow\" class=\"external text\" href=\"https://www.theverge.com/2022/9/15/23340673/ai-image-generation-stable-diffusion-explained-ethics-copyright-data\">\"Anyone can use this AI art generator — that's the risk\"</a>. <i>The Verge</i>. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20230121153021/https://www.theverge.com/2022/9/15/23340673/ai-image-generation-stable-diffusion-explained-ethics-copyright-data\">Archived</a> from the original on January 21, 2023<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">September 30,</span> 2022</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=The+Verge&amp;rft.atitle=Anyone+can+use+this+AI+art+generator+%E2%80%94+that%27s+the+risk&amp;rft.date=2022-09-15&amp;rft.aulast=Vincent&amp;rft.aufirst=James&amp;rft_id=https%3A%2F%2Fwww.theverge.com%2F2022%2F9%2F15%2F23340673%2Fai-image-generation-stable-diffusion-explained-ethics-copyright-data&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-11\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-11\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite class=\"citation web cs1\"><a rel=\"nofollow\" class=\"external text\" href=\"https://github.com/CompVis/latent-diffusion\">\"CompVis/Latent-diffusion\"</a>. <i><a href=\"/wiki/GitHub\" title=\"GitHub\">GitHub</a></i>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=GitHub&amp;rft.atitle=CompVis%2FLatent-diffusion&amp;rft_id=https%3A%2F%2Fgithub.com%2FCompVis%2Flatent-diffusion&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-12\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-12\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite class=\"citation web cs1\"><a rel=\"nofollow\" class=\"external text\" href=\"https://stability.ai/news/stable-diffusion-3-research-paper\">\"Stable Diffusion 3: Research Paper\"</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Stable+Diffusion+3%3A+Research+Paper&amp;rft_id=https%3A%2F%2Fstability.ai%2Fnews%2Fstable-diffusion-3-research-paper&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-paper-13\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-paper_13-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-paper_13-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-paper_13-2\"><sup><i><b>c</b></i></sup></a></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite id=\"CITEREFRombachBlattmannLorenzEsser2022\" class=\"citation conference cs1\">Rombach; Blattmann; Lorenz; Esser; Ommer (June 2022). <a rel=\"nofollow\" class=\"external text\" href=\"https://openaccess.thecvf.com/content/CVPR2022/papers/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf\"><i>High-Resolution Image Synthesis with Latent Diffusion Models</i></a> <span class=\"cs1-format\">(PDF)</span>. International Conference on Computer Vision and Pattern Recognition (CVPR). New Orleans, LA. pp.&#160;10684–10695. <a href=\"/wiki/ArXiv_(identifier)\" class=\"mw-redirect\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"id-lock-free\" title=\"Freely accessible\"><a rel=\"nofollow\" class=\"external text\" href=\"https://arxiv.org/abs/2112.10752\">2112.10752</a></span>. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20230120163151/https://openaccess.thecvf.com/content/CVPR2022/papers/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf\">Archived</a> <span class=\"cs1-format\">(PDF)</span> from the original on January 20, 2023<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">September 17,</span> 2022</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=High-Resolution+Image+Synthesis+with+Latent+Diffusion+Models&amp;rft.place=New+Orleans%2C+LA&amp;rft.pages=10684-10695&amp;rft.date=2022-06&amp;rft_id=info%3Aarxiv%2F2112.10752&amp;rft.au=Rombach&amp;rft.au=Blattmann&amp;rft.au=Lorenz&amp;rft.au=Esser&amp;rft.au=Ommer&amp;rft_id=https%3A%2F%2Fopenaccess.thecvf.com%2Fcontent%2FCVPR2022%2Fpapers%2FRombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-:02-14\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-:02_14-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-:02_14-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-:02_14-2\"><sup><i><b>c</b></i></sup></a> <a href=\"#cite_ref-:02_14-3\"><sup><i><b>d</b></i></sup></a></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite id=\"CITEREFAlammar\" class=\"citation web cs1\">Alammar, Jay. <a rel=\"nofollow\" class=\"external text\" href=\"https://jalammar.github.io/illustrated-stable-diffusion/\">\"The Illustrated Stable Diffusion\"</a>. <i>jalammar.github.io</i>. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20221101104342/https://jalammar.github.io/illustrated-stable-diffusion/\">Archived</a> from the original on November 1, 2022<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">October 31,</span> 2022</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=jalammar.github.io&amp;rft.atitle=The+Illustrated+Stable+Diffusion&amp;rft.aulast=Alammar&amp;rft.aufirst=Jay&amp;rft_id=https%3A%2F%2Fjalammar.github.io%2Fillustrated-stable-diffusion%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-15\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-15\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite id=\"CITEREFDavid\" class=\"citation book cs1\">David, Foster. \"8. Diffusion Models\". <i>Generative Deep Learning</i> (2&#160;ed.). O'Reilly.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=8.+Diffusion+Models&amp;rft.btitle=Generative+Deep+Learning&amp;rft.edition=2&amp;rft.pub=O%27Reilly&amp;rft.aulast=David&amp;rft.aufirst=Foster&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-16\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-16\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite id=\"CITEREFJascha_Sohl-Dickstein,_Eric_A._Weiss,_Niru_Maheswaranathan,_Surya_Ganguli2015\" class=\"citation journal cs1\">Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, Surya Ganguli (March 12, 2015). \"Deep Unsupervised Learning using Nonequilibrium Thermodynamics\". <i>Arxiv</i>. <a href=\"/wiki/ArXiv_(identifier)\" class=\"mw-redirect\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"id-lock-free\" title=\"Freely accessible\"><a rel=\"nofollow\" class=\"external text\" href=\"https://arxiv.org/abs/1503.03585\">1503.03585</a></span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Arxiv&amp;rft.atitle=Deep+Unsupervised+Learning+using+Nonequilibrium+Thermodynamics&amp;rft.date=2015-03-12&amp;rft_id=info%3Aarxiv%2F1503.03585&amp;rft.au=Jascha+Sohl-Dickstein%2C+Eric+A.+Weiss%2C+Niru+Maheswaranathan%2C+Surya+Ganguli&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span><span class=\"cs1-maint citation-comment\"><code class=\"cs1-code\">{{<a href=\"/wiki/Template:Cite_journal\" title=\"Template:Cite journal\">cite journal</a>}}</code>:  CS1 maint: multiple names: authors list (<a href=\"/wiki/Category:CS1_maint:_multiple_names:_authors_list\" title=\"Category:CS1 maint: multiple names: authors list\">link</a>)</span></span>\n</li>\n<li id=\"cite_note-17\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-17\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite class=\"citation web cs1\"><a rel=\"nofollow\" class=\"external text\" href=\"https://huggingface.co/docs/diffusers/v0.5.1/en/api/pipelines/stable_diffusion\">\"Stable diffusion pipelines\"</a>. <i>huggingface.co</i>. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20230625030241/https://huggingface.co/docs/diffusers/v0.5.1/en/api/pipelines/stable_diffusion\">Archived</a> from the original on June 25, 2023<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">June 22,</span> 2023</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=huggingface.co&amp;rft.atitle=Stable+diffusion+pipelines&amp;rft_id=https%3A%2F%2Fhuggingface.co%2Fdocs%2Fdiffusers%2Fv0.5.1%2Fen%2Fapi%2Fpipelines%2Fstable_diffusion&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-18\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-18\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite class=\"citation web cs1\"><a rel=\"nofollow\" class=\"external text\" href=\"https://docs.openvino.ai/2023.3/notebooks/225-stable-diffusion-text-to-image-with-output.html\">\"Text-to-Image Generation with Stable Diffusion and OpenVINO™\"</a>. <i>openvino.ai</i>. <a href=\"/wiki/Intel\" title=\"Intel\">Intel</a><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">February 10,</span> 2024</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=openvino.ai&amp;rft.atitle=Text-to-Image+Generation+with+Stable+Diffusion+and+OpenVINO%E2%84%A2&amp;rft_id=https%3A%2F%2Fdocs.openvino.ai%2F2023.3%2Fnotebooks%2F225-stable-diffusion-text-to-image-with-output.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-:4-19\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-:4_19-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-:4_19-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-:4_19-2\"><sup><i><b>c</b></i></sup></a></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite id=\"CITEREFPodellEnglishLaceyBlattmann2023\" class=\"citation arxiv cs1\">Podell, Dustin; English, Zion; Lacey, Kyle; Blattmann, Andreas; Dockhorn, Tim; Müller, Jonas; Penna, Joe; Rombach, Robin (July 4, 2023). \"SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis\". <a href=\"/wiki/ArXiv_(identifier)\" class=\"mw-redirect\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"id-lock-free\" title=\"Freely accessible\"><a rel=\"nofollow\" class=\"external text\" href=\"https://arxiv.org/abs/2307.01952\">2307.01952</a></span> [<a rel=\"nofollow\" class=\"external text\" href=\"https://arxiv.org/archive/cs.CV\">cs.CV</a>].</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=SDXL%3A+Improving+Latent+Diffusion+Models+for+High-Resolution+Image+Synthesis&amp;rft.date=2023-07-04&amp;rft_id=info%3Aarxiv%2F2307.01952&amp;rft.aulast=Podell&amp;rft.aufirst=Dustin&amp;rft.au=English%2C+Zion&amp;rft.au=Lacey%2C+Kyle&amp;rft.au=Blattmann%2C+Andreas&amp;rft.au=Dockhorn%2C+Tim&amp;rft.au=M%C3%BCller%2C+Jonas&amp;rft.au=Penna%2C+Joe&amp;rft.au=Rombach%2C+Robin&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-:6-20\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-:6_20-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-:6_20-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-:6_20-2\"><sup><i><b>c</b></i></sup></a></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite id=\"CITEREFEsserKulalBlattmannEntezari2024\" class=\"citation cs2\">Esser, Patrick; Kulal, Sumith; Blattmann, Andreas; Entezari, Rahim; Müller, Jonas; Saini, Harry; Levi, Yam; Lorenz, Dominik; Sauer, Axel (March 5, 2024), <i>Scaling Rectified Flow Transformers for High-Resolution Image Synthesis</i>, <a href=\"/wiki/ArXiv_(identifier)\" class=\"mw-redirect\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"id-lock-free\" title=\"Freely accessible\"><a rel=\"nofollow\" class=\"external text\" href=\"https://arxiv.org/abs/2403.03206\">2403.03206</a></span></cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Scaling+Rectified+Flow+Transformers+for+High-Resolution+Image+Synthesis&amp;rft.date=2024-03-05&amp;rft_id=info%3Aarxiv%2F2403.03206&amp;rft.aulast=Esser&amp;rft.aufirst=Patrick&amp;rft.au=Kulal%2C+Sumith&amp;rft.au=Blattmann%2C+Andreas&amp;rft.au=Entezari%2C+Rahim&amp;rft.au=M%C3%BCller%2C+Jonas&amp;rft.au=Saini%2C+Harry&amp;rft.au=Levi%2C+Yam&amp;rft.au=Lorenz%2C+Dominik&amp;rft.au=Sauer%2C+Axel&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-:7-21\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-:7_21-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-:7_21-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite id=\"CITEREFLiuGongLiu2022\" class=\"citation cs2\">Liu, Xingchao; Gong, Chengyue; Liu, Qiang (September 7, 2022), <i>Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow</i>, <a href=\"/wiki/ArXiv_(identifier)\" class=\"mw-redirect\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"id-lock-free\" title=\"Freely accessible\"><a rel=\"nofollow\" class=\"external text\" href=\"https://arxiv.org/abs/2209.03003\">2209.03003</a></span></cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Flow+Straight+and+Fast%3A+Learning+to+Generate+and+Transfer+Data+with+Rectified+Flow&amp;rft.date=2022-09-07&amp;rft_id=info%3Aarxiv%2F2209.03003&amp;rft.aulast=Liu&amp;rft.aufirst=Xingchao&amp;rft.au=Gong%2C+Chengyue&amp;rft.au=Liu%2C+Qiang&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-:8-22\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-:8_22-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-:8_22-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite class=\"citation web cs1\"><a rel=\"nofollow\" class=\"external text\" href=\"https://www.cs.utexas.edu/~lqiang/rectflow/html/intro.html\">\"Rectified Flow — Rectified Flow\"</a>. <i>www.cs.utexas.edu</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">March 6,</span> 2024</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=www.cs.utexas.edu&amp;rft.atitle=Rectified+Flow+%E2%80%94+Rectified+Flow&amp;rft_id=https%3A%2F%2Fwww.cs.utexas.edu%2F~lqiang%2Frectflow%2Fhtml%2Fintro.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-Waxy-23\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-Waxy_23-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-Waxy_23-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-Waxy_23-2\"><sup><i><b>c</b></i></sup></a> <a href=\"#cite_ref-Waxy_23-3\"><sup><i><b>d</b></i></sup></a> <a href=\"#cite_ref-Waxy_23-4\"><sup><i><b>e</b></i></sup></a></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite id=\"CITEREFBaio2022\" class=\"citation web cs1\">Baio, Andy (August 30, 2022). <a rel=\"nofollow\" class=\"external text\" href=\"https://waxy.org/2022/08/exploring-12-million-of-the-images-used-to-train-stable-diffusions-image-generator/\">\"Exploring 12 Million of the 2.3 Billion Images Used to Train Stable Diffusion's Image Generator\"</a>. <i>Waxy.org</i>. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20230120124332/https://waxy.org/2022/08/exploring-12-million-of-the-images-used-to-train-stable-diffusions-image-generator/\">Archived</a> from the original on January 20, 2023<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">November 2,</span> 2022</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Waxy.org&amp;rft.atitle=Exploring+12+Million+of+the+2.3+Billion+Images+Used+to+Train+Stable+Diffusion%27s+Image+Generator&amp;rft.date=2022-08-30&amp;rft.aulast=Baio&amp;rft.aufirst=Andy&amp;rft_id=https%3A%2F%2Fwaxy.org%2F2022%2F08%2Fexploring-12-million-of-the-images-used-to-train-stable-diffusions-image-generator%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-24\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-24\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite class=\"citation web cs1\"><a rel=\"nofollow\" class=\"external text\" href=\"https://www.technologyreview.com/2022/09/16/1059598/this-artist-is-dominating-ai-generated-art-and-hes-not-happy-about-it/\">\"This artist is dominating AI-generated art. And he's not happy about it\"</a>. <i>MIT Technology Review</i>. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20230114125952/https://www.technologyreview.com/2022/09/16/1059598/this-artist-is-dominating-ai-generated-art-and-hes-not-happy-about-it/\">Archived</a> from the original on January 14, 2023<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">November 2,</span> 2022</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=MIT+Technology+Review&amp;rft.atitle=This+artist+is+dominating+AI-generated+art.+And+he%27s+not+happy+about+it.&amp;rft_id=https%3A%2F%2Fwww.technologyreview.com%2F2022%2F09%2F16%2F1059598%2Fthis-artist-is-dominating-ai-generated-art-and-hes-not-happy-about-it%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-:2-25\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-:2_25-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-:2_25-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite id=\"CITEREFBrunnerHarlan2023\" class=\"citation web cs1\">Brunner, Katharina; Harlan, Elisa (July 7, 2023). <a rel=\"nofollow\" class=\"external text\" href=\"https://interaktiv.br.de/ki-trainingsdaten/en/index.html\">\"We Are All Raw Material for AI\"</a>. Bayerischer Rundfunk (BR). <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20230912092308/https://interaktiv.br.de/ki-trainingsdaten/en/index.html\">Archived</a> from the original on September 12, 2023<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">September 12,</span> 2023</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=We+Are+All+Raw+Material+for+AI&amp;rft.pub=Bayerischer+Rundfunk+%28BR%29&amp;rft.date=2023-07-07&amp;rft.aulast=Brunner&amp;rft.aufirst=Katharina&amp;rft.au=Harlan%2C+Elisa&amp;rft_id=https%3A%2F%2Finteraktiv.br.de%2Fki-trainingsdaten%2Fen%2Findex.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-26\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-26\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite id=\"CITEREFSchuhmann2022\" class=\"citation cs2\">Schuhmann, Christoph (November 2, 2022), <a rel=\"nofollow\" class=\"external text\" href=\"https://github.com/christophschuhmann/improved-aesthetic-predictor\"><i>CLIP+MLP Aesthetic Score Predictor</i></a>, <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20230608005334/http://github.com/christophschuhmann/improved-aesthetic-predictor/\">archived</a> from the original on June 8, 2023<span class=\"reference-accessdate\">, retrieved <span class=\"nowrap\">November 2,</span> 2022</span></cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=CLIP%2BMLP+Aesthetic+Score+Predictor&amp;rft.date=2022-11-02&amp;rft.aulast=Schuhmann&amp;rft.aufirst=Christoph&amp;rft_id=https%3A%2F%2Fgithub.com%2Fchristophschuhmann%2Fimproved-aesthetic-predictor&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-LAION-Aesthetics-27\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-LAION-Aesthetics_27-0\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite class=\"citation web cs1\"><a rel=\"nofollow\" class=\"external text\" href=\"https://laion.ai/blog/laion-aesthetics\">\"LAION-Aesthetics | LAION\"</a>. <i>laion.ai</i>. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20220826121216/https://laion.ai/blog/laion-aesthetics/\">Archived</a> from the original on August 26, 2022<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">September 2,</span> 2022</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=laion.ai&amp;rft.atitle=LAION-Aesthetics+%7C+LAION&amp;rft_id=https%3A%2F%2Flaion.ai%2Fblog%2Flaion-aesthetics&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-:5-28\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-:5_28-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-:5_28-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-:5_28-2\"><sup><i><b>c</b></i></sup></a></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite id=\"CITEREFHoSalimans2022\" class=\"citation arxiv cs1\">Ho, Jonathan; Salimans, Tim (July 25, 2022). \"Classifier-Free Diffusion Guidance\". <a href=\"/wiki/ArXiv_(identifier)\" class=\"mw-redirect\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"id-lock-free\" title=\"Freely accessible\"><a rel=\"nofollow\" class=\"external text\" href=\"https://arxiv.org/abs/2207.12598\">2207.12598</a></span> [<a rel=\"nofollow\" class=\"external text\" href=\"https://arxiv.org/archive/cs.LG\">cs.LG</a>].</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Classifier-Free+Diffusion+Guidance&amp;rft.date=2022-07-25&amp;rft_id=info%3Aarxiv%2F2207.12598&amp;rft.aulast=Ho&amp;rft.aufirst=Jonathan&amp;rft.au=Salimans%2C+Tim&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-29\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-29\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite id=\"CITEREFMostaque2022\" class=\"citation web cs1\">Mostaque, Emad (August 28, 2022). <a rel=\"nofollow\" class=\"external text\" href=\"https://twitter.com/emostaque/status/1563870674111832066\">\"Cost of construction\"</a>. <i>Twitter</i>. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20220906155426/https://twitter.com/EMostaque/status/1563870674111832066\">Archived</a> from the original on September 6, 2022<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">September 6,</span> 2022</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Twitter&amp;rft.atitle=Cost+of+construction&amp;rft.date=2022-08-28&amp;rft.aulast=Mostaque&amp;rft.aufirst=Emad&amp;rft_id=https%3A%2F%2Ftwitter.com%2Femostaque%2Fstatus%2F1563870674111832066&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-stable-diffusion-model-card-1-4-30\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-stable-diffusion-model-card-1-4_30-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-stable-diffusion-model-card-1-4_30-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-stable-diffusion-model-card-1-4_30-2\"><sup><i><b>c</b></i></sup></a></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite class=\"citation web cs1\"><a rel=\"nofollow\" class=\"external text\" href=\"https://huggingface.co/CompVis/stable-diffusion-v1-4\">\"CompVis/stable-diffusion-v1-4 · Hugging Face\"</a>. <i>huggingface.co</i>. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20230111161920/https://huggingface.co/CompVis/stable-diffusion-v1-4\">Archived</a> from the original on January 11, 2023<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">November 2,</span> 2022</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=huggingface.co&amp;rft.atitle=CompVis%2Fstable-diffusion-v1-4+%C2%B7+Hugging+Face&amp;rft_id=https%3A%2F%2Fhuggingface.co%2FCompVis%2Fstable-diffusion-v1-4&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-31\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-31\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite id=\"CITEREFWiggers2022\" class=\"citation web cs1\">Wiggers, Kyle (August 12, 2022). <a rel=\"nofollow\" class=\"external text\" href=\"https://techcrunch.com/2022/08/12/a-startup-wants-to-democratize-the-tech-behind-dall-e-2-consequences-be-damned/\">\"A startup wants to democratize the tech behind DALL-E 2, consequences be damned\"</a>. <i>TechCrunch</i>. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20230119005503/https://techcrunch.com/2022/08/12/a-startup-wants-to-democratize-the-tech-behind-dall-e-2-consequences-be-damned/\">Archived</a> from the original on January 19, 2023<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">November 2,</span> 2022</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=TechCrunch&amp;rft.atitle=A+startup+wants+to+democratize+the+tech+behind+DALL-E+2%2C+consequences+be+damned&amp;rft.date=2022-08-12&amp;rft.aulast=Wiggers&amp;rft.aufirst=Kyle&amp;rft_id=https%3A%2F%2Ftechcrunch.com%2F2022%2F08%2F12%2Fa-startup-wants-to-democratize-the-tech-behind-dall-e-2-consequences-be-damned%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-32\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-32\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite id=\"CITEREFemad_96082024\" class=\"citation web cs1\">emad_9608 (April 19, 2024). <a rel=\"nofollow\" class=\"external text\" href=\"https://www.reddit.com/r/StableDiffusion/comments/1c870a5/any_estimate_on_how_much_money_they_spent_to/l0dc2ni/\">\"10m is about right\"</a>. <i>r/StableDiffusion</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">April 25,</span> 2024</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=r%2FStableDiffusion&amp;rft.atitle=10m+is+about+right&amp;rft.date=2024-04-19&amp;rft.au=emad_9608&amp;rft_id=http%3A%2F%2Fwww.reddit.com%2Fr%2FStableDiffusion%2Fcomments%2F1c870a5%2Fany_estimate_on_how_much_money_they_spent_to%2Fl0dc2ni%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span><span class=\"cs1-maint citation-comment\"><code class=\"cs1-code\">{{<a href=\"/wiki/Template:Cite_web\" title=\"Template:Cite web\">cite web</a>}}</code>:  CS1 maint: numeric names: authors list (<a href=\"/wiki/Category:CS1_maint:_numeric_names:_authors_list\" title=\"Category:CS1 maint: numeric names: authors list\">link</a>)</span></span>\n</li>\n<li id=\"cite_note-diffusers-33\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-diffusers_33-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-diffusers_33-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-diffusers_33-2\"><sup><i><b>c</b></i></sup></a> <a href=\"#cite_ref-diffusers_33-3\"><sup><i><b>d</b></i></sup></a> <a href=\"#cite_ref-diffusers_33-4\"><sup><i><b>e</b></i></sup></a></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite class=\"citation web cs1\"><a rel=\"nofollow\" class=\"external text\" href=\"https://huggingface.co/blog/stable_diffusion\">\"Stable Diffusion with 🧨 Diffusers\"</a>. <i>huggingface.co</i>. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20230117222142/https://huggingface.co/blog/stable_diffusion\">Archived</a> from the original on January 17, 2023<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">October 31,</span> 2022</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=huggingface.co&amp;rft.atitle=Stable+Diffusion+with+%F0%9F%A7%A8+Diffusers&amp;rft_id=https%3A%2F%2Fhuggingface.co%2Fblog%2Fstable_diffusion&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-release2.0-34\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-release2.0_34-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-release2.0_34-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-release2.0_34-2\"><sup><i><b>c</b></i></sup></a></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite class=\"citation web cs1\"><a rel=\"nofollow\" class=\"external text\" href=\"https://stability.ai/blog/stable-diffusion-v2-release\">\"Stable Diffusion 2.0 Release\"</a>. <i>stability.ai</i>. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20221210062729/https://stability.ai/blog/stable-diffusion-v2-release\">Archived</a> from the original on December 10, 2022.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=stability.ai&amp;rft.atitle=Stable+Diffusion+2.0+Release&amp;rft_id=https%3A%2F%2Fstability.ai%2Fblog%2Fstable-diffusion-v2-release&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-35\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-35\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite class=\"citation web cs1\"><a rel=\"nofollow\" class=\"external text\" href=\"https://laion.ai/\">\"LAION\"</a>. <i>laion.ai</i>. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20231016082902/https://laion.ai/\">Archived</a> from the original on October 16, 2023<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">October 31,</span> 2022</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=laion.ai&amp;rft.atitle=LAION&amp;rft_id=https%3A%2F%2Flaion.ai%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-36\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-36\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite class=\"citation web cs1\"><a rel=\"nofollow\" class=\"external text\" href=\"https://blog.paperspace.com/generating-images-with-stable-diffusion/\">\"Generating images with Stable Diffusion\"</a>. <i>Paperspace Blog</i>. August 24, 2022. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20221031231727/https://blog.paperspace.com/generating-images-with-stable-diffusion/\">Archived</a> from the original on October 31, 2022<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">October 31,</span> 2022</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Paperspace+Blog&amp;rft.atitle=Generating+images+with+Stable+Diffusion&amp;rft.date=2022-08-24&amp;rft_id=https%3A%2F%2Fblog.paperspace.com%2Fgenerating-images-with-stable-diffusion%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-37\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-37\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite class=\"citation web cs1\"><a rel=\"nofollow\" class=\"external text\" href=\"https://stability.ai/blog/stable-diffusion-sdxl-1-announcement\">\"Announcing SDXL 1.0\"</a>. <i>Stability AI</i>. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20230726215239/https://stability.ai/blog/stable-diffusion-sdxl-1-announcement\">Archived</a> from the original on July 26, 2023<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">August 21,</span> 2023</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Stability+AI&amp;rft.atitle=Announcing+SDXL+1.0&amp;rft_id=https%3A%2F%2Fstability.ai%2Fblog%2Fstable-diffusion-sdxl-1-announcement&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-38\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-38\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite id=\"CITEREFEdwards2023\" class=\"citation web cs1\">Edwards, Benj (July 27, 2023). <a rel=\"nofollow\" class=\"external text\" href=\"https://arstechnica.com/information-technology/2023/07/stable-diffusion-xl-puts-ai-generated-visual-worlds-at-your-gpus-command/\">\"Stability AI releases Stable Diffusion XL, its next-gen image synthesis model\"</a>. <i>Ars Technica</i>. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20230821011216/https://arstechnica.com/information-technology/2023/07/stable-diffusion-xl-puts-ai-generated-visual-worlds-at-your-gpus-command/\">Archived</a> from the original on August 21, 2023<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">August 21,</span> 2023</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Ars+Technica&amp;rft.atitle=Stability+AI+releases+Stable+Diffusion+XL%2C+its+next-gen+image+synthesis+model&amp;rft.date=2023-07-27&amp;rft.aulast=Edwards&amp;rft.aufirst=Benj&amp;rft_id=https%3A%2F%2Farstechnica.com%2Finformation-technology%2F2023%2F07%2Fstable-diffusion-xl-puts-ai-generated-visual-worlds-at-your-gpus-command%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-39\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-39\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite class=\"citation web cs1\"><a rel=\"nofollow\" class=\"external text\" href=\"https://huggingface.co/hakurei/waifu-diffusion\">\"hakurei/waifu-diffusion · Hugging Face\"</a>. <i>huggingface.co</i>. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20231008120655/https://huggingface.co/hakurei/waifu-diffusion\">Archived</a> from the original on October 8, 2023<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">October 31,</span> 2022</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=huggingface.co&amp;rft.atitle=hakurei%2Fwaifu-diffusion+%C2%B7+Hugging+Face&amp;rft_id=https%3A%2F%2Fhuggingface.co%2Fhakurei%2Fwaifu-diffusion&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-40\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-40\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite id=\"CITEREFChambonBluethgenLanglotzChaudhari2022\" class=\"citation arxiv cs1\">Chambon, Pierre; Bluethgen, Christian; Langlotz, Curtis P.; Chaudhari, Akshay (October 9, 2022). \"Adapting Pretrained Vision-Language Foundational Models to Medical Imaging Domains\". <a href=\"/wiki/ArXiv_(identifier)\" class=\"mw-redirect\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"id-lock-free\" title=\"Freely accessible\"><a rel=\"nofollow\" class=\"external text\" href=\"https://arxiv.org/abs/2210.04133\">2210.04133</a></span> [<a rel=\"nofollow\" class=\"external text\" href=\"https://arxiv.org/archive/cs.CV\">cs.CV</a>].</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Adapting+Pretrained+Vision-Language+Foundational+Models+to+Medical+Imaging+Domains&amp;rft.date=2022-10-09&amp;rft_id=info%3Aarxiv%2F2210.04133&amp;rft.aulast=Chambon&amp;rft.aufirst=Pierre&amp;rft.au=Bluethgen%2C+Christian&amp;rft.au=Langlotz%2C+Curtis+P.&amp;rft.au=Chaudhari%2C+Akshay&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-41\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-41\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite id=\"CITEREFSeth_ForsgrenHayk_Martiros\" class=\"citation web cs1\">Seth Forsgren; Hayk Martiros. <a rel=\"nofollow\" class=\"external text\" href=\"https://www.riffusion.com/about\">\"Riffusion - Stable diffusion for real-time music generation\"</a>. <i>Riffusion</i>. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20221216092717/https://www.riffusion.com/about\">Archived</a> from the original on December 16, 2022.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Riffusion&amp;rft.atitle=Riffusion+-+Stable+diffusion+for+real-time+music+generation&amp;rft.au=Seth+Forsgren&amp;rft.au=Hayk+Martiros&amp;rft_id=https%3A%2F%2Fwww.riffusion.com%2Fabout&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-42\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-42\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite id=\"CITEREFMercurio2022\" class=\"citation cs2\">Mercurio, Anthony (October 31, 2022), <a rel=\"nofollow\" class=\"external text\" href=\"https://github.com/harubaru/waifu-diffusion/blob/6bf942eb6368ebf6bcbbb24b6ba8197bda6582a0/docs/en/training/README.md\"><i>Waifu Diffusion</i></a>, <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20221031234225/https://github.com/harubaru/waifu-diffusion/blob/6bf942eb6368ebf6bcbbb24b6ba8197bda6582a0/docs/en/training/README.md\">archived</a> from the original on October 31, 2022<span class=\"reference-accessdate\">, retrieved <span class=\"nowrap\">October 31,</span> 2022</span></cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Waifu+Diffusion&amp;rft.date=2022-10-31&amp;rft.aulast=Mercurio&amp;rft.aufirst=Anthony&amp;rft_id=https%3A%2F%2Fgithub.com%2Fharubaru%2Fwaifu-diffusion%2Fblob%2F6bf942eb6368ebf6bcbbb24b6ba8197bda6582a0%2Fdocs%2Fen%2Ftraining%2FREADME.md&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-43\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-43\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite id=\"CITEREFSmith\" class=\"citation web cs1\">Smith, Ryan. <a rel=\"nofollow\" class=\"external text\" href=\"https://www.anandtech.com/show/17204/nvidia-quietly-launches-geforce-rtx-3080-12gb-more-vram-more-power-more-money\">\"NVIDIA Quietly Launches GeForce RTX 3080 12GB: More VRAM, More Power, More Money\"</a>. <i>www.anandtech.com</i>. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20230827092451/https://www.anandtech.com/show/17204/nvidia-quietly-launches-geforce-rtx-3080-12gb-more-vram-more-power-more-money\">Archived</a> from the original on August 27, 2023<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">October 31,</span> 2022</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=www.anandtech.com&amp;rft.atitle=NVIDIA+Quietly+Launches+GeForce+RTX+3080+12GB%3A+More+VRAM%2C+More+Power%2C+More+Money&amp;rft.aulast=Smith&amp;rft.aufirst=Ryan&amp;rft_id=https%3A%2F%2Fwww.anandtech.com%2Fshow%2F17204%2Fnvidia-quietly-launches-geforce-rtx-3080-12gb-more-vram-more-power-more-money&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-44\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-44\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite id=\"CITEREFDave_James2022\" class=\"citation web cs1\">Dave James (October 28, 2022). <a rel=\"nofollow\" class=\"external text\" href=\"https://www.pcgamer.com/nvidia-rtx-4090-stable-diffusion-training-aharon-kahana/\">\"I thrashed the RTX 4090 for 8 hours straight training Stable Diffusion to paint like my uncle Hermann\"</a>. <i><a href=\"/wiki/PC_Gamer\" title=\"PC Gamer\">PC Gamer</a></i>. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20221109154310/https://www.pcgamer.com/nvidia-rtx-4090-stable-diffusion-training-aharon-kahana/\">Archived</a> from the original on November 9, 2022.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=PC+Gamer&amp;rft.atitle=I+thrashed+the+RTX+4090+for+8+hours+straight+training+Stable+Diffusion+to+paint+like+my+uncle+Hermann&amp;rft.date=2022-10-28&amp;rft.au=Dave+James&amp;rft_id=https%3A%2F%2Fwww.pcgamer.com%2Fnvidia-rtx-4090-stable-diffusion-training-aharon-kahana%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-45\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-45\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite id=\"CITEREFGalAlalufAtzmonPatashnik2022\" class=\"citation arxiv cs1\">Gal, Rinon; Alaluf, Yuval; Atzmon, Yuval; Patashnik, Or; Bermano, Amit H.; Chechik, Gal; Cohen-Or, Daniel (August 2, 2022). \"An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion\". <a href=\"/wiki/ArXiv_(identifier)\" class=\"mw-redirect\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"id-lock-free\" title=\"Freely accessible\"><a rel=\"nofollow\" class=\"external text\" href=\"https://arxiv.org/abs/2208.01618\">2208.01618</a></span> [<a rel=\"nofollow\" class=\"external text\" href=\"https://arxiv.org/archive/cs.CV\">cs.CV</a>].</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=An+Image+is+Worth+One+Word%3A+Personalizing+Text-to-Image+Generation+using+Textual+Inversion&amp;rft.date=2022-08-02&amp;rft_id=info%3Aarxiv%2F2208.01618&amp;rft.aulast=Gal&amp;rft.aufirst=Rinon&amp;rft.au=Alaluf%2C+Yuval&amp;rft.au=Atzmon%2C+Yuval&amp;rft.au=Patashnik%2C+Or&amp;rft.au=Bermano%2C+Amit+H.&amp;rft.au=Chechik%2C+Gal&amp;rft.au=Cohen-Or%2C+Daniel&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-46\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-46\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite class=\"citation web cs1\"><a rel=\"nofollow\" class=\"external text\" href=\"https://blog.novelai.net/novelai-improvements-on-stable-diffusion-e10d38db82ac\">\"NovelAI Improvements on Stable Diffusion\"</a>. <i>NovelAI</i>. October 11, 2022. <a rel=\"nofollow\" class=\"external text\" href=\"https://archive.today/20221027041603/https://blog.novelai.net/novelai-improvements-on-stable-diffusion-e10d38db82ac\">Archived</a> from the original on October 27, 2022.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=NovelAI&amp;rft.atitle=NovelAI+Improvements+on+Stable+Diffusion&amp;rft.date=2022-10-11&amp;rft_id=https%3A%2F%2Fblog.novelai.net%2Fnovelai-improvements-on-stable-diffusion-e10d38db82ac&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-47\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-47\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite id=\"CITEREFYuki_Yamashita2022\" class=\"citation web cs1 cs1-prop-foreign-lang-source\">Yuki Yamashita (September 1, 2022). <a rel=\"nofollow\" class=\"external text\" href=\"https://www.itmedia.co.jp/news/articles/2209/01/news041.html\">\"愛犬の合成画像を生成できるAI 文章で指示するだけでコスプレ 米Googleが開発\"</a>. <i>ITmedia Inc.</i> (in Japanese). <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20220831232021/https://www.itmedia.co.jp/news/articles/2209/01/news041.html\">Archived</a> from the original on August 31, 2022.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=ITmedia+Inc.&amp;rft.atitle=%E6%84%9B%E7%8A%AC%E3%81%AE%E5%90%88%E6%88%90%E7%94%BB%E5%83%8F%E3%82%92%E7%94%9F%E6%88%90%E3%81%A7%E3%81%8D%E3%82%8BAI+%E6%96%87%E7%AB%A0%E3%81%A7%E6%8C%87%E7%A4%BA%E3%81%99%E3%82%8B%E3%81%A0%E3%81%91%E3%81%A7%E3%82%B3%E3%82%B9%E3%83%97%E3%83%AC+%E7%B1%B3Google%E3%81%8C%E9%96%8B%E7%99%BA&amp;rft.date=2022-09-01&amp;rft.au=Yuki+Yamashita&amp;rft_id=https%3A%2F%2Fwww.itmedia.co.jp%2Fnews%2Farticles%2F2209%2F01%2Fnews041.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-48\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-48\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite id=\"CITEREFMengHeSongSong2021\" class=\"citation arxiv cs1\">Meng, Chenlin; He, Yutong; Song, Yang; Song, Jiaming; Wu, Jiajun; Zhu, Jun-Yan; Ermon, Stefano (August 2, 2021). \"SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations\". <a href=\"/wiki/ArXiv_(identifier)\" class=\"mw-redirect\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"id-lock-free\" title=\"Freely accessible\"><a rel=\"nofollow\" class=\"external text\" href=\"https://arxiv.org/abs/2108.01073\">2108.01073</a></span> [<a rel=\"nofollow\" class=\"external text\" href=\"https://arxiv.org/archive/cs.CV\">cs.CV</a>].</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=SDEdit%3A+Guided+Image+Synthesis+and+Editing+with+Stochastic+Differential+Equations&amp;rft.date=2021-08-02&amp;rft_id=info%3Aarxiv%2F2108.01073&amp;rft.aulast=Meng&amp;rft.aufirst=Chenlin&amp;rft.au=He%2C+Yutong&amp;rft.au=Song%2C+Yang&amp;rft.au=Song%2C+Jiaming&amp;rft.au=Wu%2C+Jiajun&amp;rft.au=Zhu%2C+Jun-Yan&amp;rft.au=Ermon%2C+Stefano&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-webui_showcase-49\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-webui_showcase_49-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-webui_showcase_49-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-webui_showcase_49-2\"><sup><i><b>c</b></i></sup></a> <a href=\"#cite_ref-webui_showcase_49-3\"><sup><i><b>d</b></i></sup></a></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite class=\"citation web cs1\"><a rel=\"nofollow\" class=\"external text\" href=\"https://github.com/AUTOMATIC1111/stable-diffusion-webui-feature-showcase\">\"Stable Diffusion web UI\"</a>. <i>GitHub</i>. November 10, 2022. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20230120032734/https://github.com/AUTOMATIC1111/stable-diffusion-webui-feature-showcase\">Archived</a> from the original on January 20, 2023<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">September 27,</span> 2022</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=GitHub&amp;rft.atitle=Stable+Diffusion+web+UI&amp;rft.date=2022-11-10&amp;rft_id=https%3A%2F%2Fgithub.com%2FAUTOMATIC1111%2Fstable-diffusion-webui-feature-showcase&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-50\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-50\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite class=\"citation cs2\"><a rel=\"nofollow\" class=\"external text\" href=\"https://github.com/ShieldMnt/invisible-watermark/blob/9802ce3e0c3a5ec43b41d503f156717f0c739584/README.md\"><i>invisible-watermark</i></a>, Shield Mountain, November 2, 2022, <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20221018062806/https://github.com/ShieldMnt/invisible-watermark/blob/9802ce3e0c3a5ec43b41d503f156717f0c739584/README.md\">archived</a> from the original on October 18, 2022<span class=\"reference-accessdate\">, retrieved <span class=\"nowrap\">November 2,</span> 2022</span></cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=invisible-watermark&amp;rft.pub=Shield+Mountain&amp;rft.date=2022-11-02&amp;rft_id=https%3A%2F%2Fgithub.com%2FShieldMnt%2Finvisible-watermark%2Fblob%2F9802ce3e0c3a5ec43b41d503f156717f0c739584%2FREADME.md&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-51\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-51\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite class=\"citation web cs1\"><a rel=\"nofollow\" class=\"external text\" href=\"https://github.com/JohannesGaessler/stable-diffusion-tools\">\"stable-diffusion-tools/emphasis at master · JohannesGaessler/stable-diffusion-tools\"</a>. <i>GitHub</i>. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20221002081041/https://github.com/JohannesGaessler/stable-diffusion-tools\">Archived</a> from the original on October 2, 2022<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">November 2,</span> 2022</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=GitHub&amp;rft.atitle=stable-diffusion-tools%2Femphasis+at+master+%C2%B7+JohannesGaessler%2Fstable-diffusion-tools&amp;rft_id=https%3A%2F%2Fgithub.com%2FJohannesGaessler%2Fstable-diffusion-tools&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-release2.1-52\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-release2.1_52-0\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite class=\"citation web cs1\"><a rel=\"nofollow\" class=\"external text\" href=\"https://stability.ai/blog/stablediffusion2-1-release7-dec-2022\">\"Stable Diffusion v2.1 and DreamStudio Updates 7-Dec 22\"</a>. <i>stability.ai</i>. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20221210062732/https://stability.ai/blog/stablediffusion2-1-release7-dec-2022\">Archived</a> from the original on December 10, 2022.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=stability.ai&amp;rft.atitle=Stable+Diffusion+v2.1+and+DreamStudio+Updates+7-Dec+22&amp;rft_id=https%3A%2F%2Fstability.ai%2Fblog%2Fstablediffusion2-1-release7-dec-2022&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-:1-53\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-:1_53-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-:1_53-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite id=\"CITEREFLuziSiahkoohiMayerCasco-Rodriguez2022\" class=\"citation arxiv cs1\">Luzi, Lorenzo; Siahkoohi, Ali; Mayer, Paul M.; Casco-Rodriguez, Josue; Baraniuk, Richard (October 21, 2022). \"Boomerang: Local sampling on image manifolds using diffusion models\". <a href=\"/wiki/ArXiv_(identifier)\" class=\"mw-redirect\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"id-lock-free\" title=\"Freely accessible\"><a rel=\"nofollow\" class=\"external text\" href=\"https://arxiv.org/abs/2210.12100\">2210.12100</a></span> [<a rel=\"nofollow\" class=\"external text\" href=\"https://arxiv.org/archive/cs.CV\">cs.CV</a>].</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Boomerang%3A+Local+sampling+on+image+manifolds+using+diffusion+models&amp;rft.date=2022-10-21&amp;rft_id=info%3Aarxiv%2F2210.12100&amp;rft.aulast=Luzi&amp;rft.aufirst=Lorenzo&amp;rft.au=Siahkoohi%2C+Ali&amp;rft.au=Mayer%2C+Paul+M.&amp;rft.au=Casco-Rodriguez%2C+Josue&amp;rft.au=Baraniuk%2C+Richard&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-54\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-54\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite id=\"CITEREFBühlmann2022\" class=\"citation web cs1\">Bühlmann, Matthias (September 28, 2022). <a rel=\"nofollow\" class=\"external text\" href=\"https://pub.towardsai.net/stable-diffusion-based-image-compresssion-6f1f0a399202\">\"Stable Diffusion Based Image Compression\"</a>. <i>Medium</i>. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20221102231642/https://pub.towardsai.net/stable-diffusion-based-image-compresssion-6f1f0a399202\">Archived</a> from the original on November 2, 2022<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">November 2,</span> 2022</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Medium&amp;rft.atitle=Stable+Diffusion+Based+Image+Compression&amp;rft.date=2022-09-28&amp;rft.aulast=B%C3%BChlmann&amp;rft.aufirst=Matthias&amp;rft_id=https%3A%2F%2Fpub.towardsai.net%2Fstable-diffusion-based-image-compresssion-6f1f0a399202&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-controlnet-paper-55\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-controlnet-paper_55-0\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite id=\"CITEREFZhang2023\" class=\"citation arxiv cs1\">Zhang, Lvmin (February 10, 2023). \"Adding Conditional Control to Text-to-Image Diffusion Models\". <a href=\"/wiki/ArXiv_(identifier)\" class=\"mw-redirect\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"id-lock-free\" title=\"Freely accessible\"><a rel=\"nofollow\" class=\"external text\" href=\"https://arxiv.org/abs/2302.05543\">2302.05543</a></span> [<a rel=\"nofollow\" class=\"external text\" href=\"https://arxiv.org/archive/cs.CV\">cs.CV</a>].</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Adding+Conditional+Control+to+Text-to-Image+Diffusion+Models&amp;rft.date=2023-02-10&amp;rft_id=info%3Aarxiv%2F2302.05543&amp;rft.aulast=Zhang&amp;rft.aufirst=Lvmin&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-56\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-56\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite class=\"citation web cs1\"><a rel=\"nofollow\" class=\"external text\" href=\"https://huggingface.co/CompVis/stable-diffusion-v1-4\">\"CompVis/stable-diffusion-v1-4 · Hugging Face\"</a>. <i>huggingface.co</i>. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20230111161920/https://huggingface.co/CompVis/stable-diffusion-v1-4\">Archived</a> from the original on January 11, 2023<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">August 17,</span> 2023</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=huggingface.co&amp;rft.atitle=CompVis%2Fstable-diffusion-v1-4+%C2%B7+Hugging+Face&amp;rft_id=https%3A%2F%2Fhuggingface.co%2FCompVis%2Fstable-diffusion-v1-4&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-57\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-57\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite class=\"citation web cs1\"><a rel=\"nofollow\" class=\"external text\" href=\"https://huggingface.co/CompVis\">\"CompVis (CompVis)\"</a>. <i>huggingface.co</i>. August 23, 2023<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">March 6,</span> 2024</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=huggingface.co&amp;rft.atitle=CompVis+%28CompVis%29&amp;rft.date=2023-08-23&amp;rft_id=https%3A%2F%2Fhuggingface.co%2FCompVis&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-58\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-58\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite class=\"citation web cs1\"><a rel=\"nofollow\" class=\"external text\" href=\"https://huggingface.co/runwayml/stable-diffusion-v1-5\">\"runwayml/stable-diffusion-v1-5 · Hugging Face\"</a>. <i>huggingface.co</i>. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20230921025150/https://huggingface.co/runwayml/stable-diffusion-v1-5\">Archived</a> from the original on September 21, 2023<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">August 17,</span> 2023</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=huggingface.co&amp;rft.atitle=runwayml%2Fstable-diffusion-v1-5+%C2%B7+Hugging+Face&amp;rft_id=https%3A%2F%2Fhuggingface.co%2Frunwayml%2Fstable-diffusion-v1-5&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-:3-59\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-:3_59-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-:3_59-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite class=\"citation web cs1\"><a rel=\"nofollow\" class=\"external text\" href=\"https://huggingface.co/stabilityai/stable-diffusion-2\">\"stabilityai/stable-diffusion-2 · Hugging Face\"</a>. <i>huggingface.co</i>. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20230921135247/https://huggingface.co/stabilityai/stable-diffusion-2\">Archived</a> from the original on September 21, 2023<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">August 17,</span> 2023</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=huggingface.co&amp;rft.atitle=stabilityai%2Fstable-diffusion-2+%C2%B7+Hugging+Face&amp;rft_id=https%3A%2F%2Fhuggingface.co%2Fstabilityai%2Fstable-diffusion-2&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-60\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-60\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite class=\"citation web cs1\"><a rel=\"nofollow\" class=\"external text\" href=\"https://huggingface.co/stabilityai/stable-diffusion-2-base\">\"stabilityai/stable-diffusion-2-base · Hugging Face\"</a>. <i>huggingface.co</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">January 1,</span> 2024</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=huggingface.co&amp;rft.atitle=stabilityai%2Fstable-diffusion-2-base+%C2%B7+Hugging+Face&amp;rft_id=https%3A%2F%2Fhuggingface.co%2Fstabilityai%2Fstable-diffusion-2-base&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-61\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-61\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite class=\"citation web cs1\"><a rel=\"nofollow\" class=\"external text\" href=\"https://huggingface.co/stabilityai/stable-diffusion-2-1\">\"stabilityai/stable-diffusion-2-1 · Hugging Face\"</a>. <i>huggingface.co</i>. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20230921025146/https://huggingface.co/stabilityai/stable-diffusion-2-1\">Archived</a> from the original on September 21, 2023<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">August 17,</span> 2023</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=huggingface.co&amp;rft.atitle=stabilityai%2Fstable-diffusion-2-1+%C2%B7+Hugging+Face&amp;rft_id=https%3A%2F%2Fhuggingface.co%2Fstabilityai%2Fstable-diffusion-2-1&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-62\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-62\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite class=\"citation web cs1\"><a rel=\"nofollow\" class=\"external text\" href=\"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0\">\"stabilityai/stable-diffusion-xl-base-1.0 · Hugging Face\"</a>. <i>huggingface.co</i>. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20231008071719/https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0\">Archived</a> from the original on October 8, 2023<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">August 17,</span> 2023</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=huggingface.co&amp;rft.atitle=stabilityai%2Fstable-diffusion-xl-base-1.0+%C2%B7+Hugging+Face&amp;rft_id=https%3A%2F%2Fhuggingface.co%2Fstabilityai%2Fstable-diffusion-xl-base-1.0&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-63\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-63\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite class=\"citation web cs1\"><a rel=\"nofollow\" class=\"external text\" href=\"https://stability.ai/news/stable-diffusion-sdxl-1-announcement\">\"Announcing SDXL 1.0\"</a>. <i>Stability AI</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">January 1,</span> 2024</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Stability+AI&amp;rft.atitle=Announcing+SDXL+1.0&amp;rft_id=https%3A%2F%2Fstability.ai%2Fnews%2Fstable-diffusion-sdxl-1-announcement&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-64\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-64\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite class=\"citation web cs1\"><a rel=\"nofollow\" class=\"external text\" href=\"https://huggingface.co/stabilityai/sdxl-turbo\">\"stabilityai/sdxl-turbo · Hugging Face\"</a>. <i>huggingface.co</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">January 1,</span> 2024</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=huggingface.co&amp;rft.atitle=stabilityai%2Fsdxl-turbo+%C2%B7+Hugging+Face&amp;rft_id=https%3A%2F%2Fhuggingface.co%2Fstabilityai%2Fsdxl-turbo&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-65\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-65\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite class=\"citation web cs1\"><a rel=\"nofollow\" class=\"external text\" href=\"https://stability.ai/research/adversarial-diffusion-distillation\">\"Adversarial Diffusion Distillation\"</a>. <i>Stability AI</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">January 1,</span> 2024</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Stability+AI&amp;rft.atitle=Adversarial+Diffusion+Distillation&amp;rft_id=https%3A%2F%2Fstability.ai%2Fresearch%2Fadversarial-diffusion-distillation&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-66\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-66\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite class=\"citation web cs1\"><a rel=\"nofollow\" class=\"external text\" href=\"https://stability.ai/news/stable-diffusion-3\">\"Stable Diffusion 3\"</a>. <i>Stability AI</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">March 5,</span> 2024</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Stability+AI&amp;rft.atitle=Stable+Diffusion+3&amp;rft_id=https%3A%2F%2Fstability.ai%2Fnews%2Fstable-diffusion-3&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-67\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-67\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite id=\"CITEREFRadfordKimHallacyRamesh2021\" class=\"citation arxiv cs1\">Radford, Alec; Kim, Jong Wook; Hallacy, Chris; Ramesh, Aditya; Goh, Gabriel; Agarwal, Sandhini; Sastry, Girish; Askell, Amanda; Mishkin, Pamela (February 26, 2021). \"Learning Transferable Visual Models From Natural Language Supervision\". <a href=\"/wiki/ArXiv_(identifier)\" class=\"mw-redirect\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"id-lock-free\" title=\"Freely accessible\"><a rel=\"nofollow\" class=\"external text\" href=\"https://arxiv.org/abs/2103.00020\">2103.00020</a></span> [<a rel=\"nofollow\" class=\"external text\" href=\"https://arxiv.org/archive/cs.CV\">cs.CV</a>].</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Learning+Transferable+Visual+Models+From+Natural+Language+Supervision&amp;rft.date=2021-02-26&amp;rft_id=info%3Aarxiv%2F2103.00020&amp;rft.aulast=Radford&amp;rft.aufirst=Alec&amp;rft.au=Kim%2C+Jong+Wook&amp;rft.au=Hallacy%2C+Chris&amp;rft.au=Ramesh%2C+Aditya&amp;rft.au=Goh%2C+Gabriel&amp;rft.au=Agarwal%2C+Sandhini&amp;rft.au=Sastry%2C+Girish&amp;rft.au=Askell%2C+Amanda&amp;rft.au=Mishkin%2C+Pamela&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-68\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-68\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite id=\"CITEREFMengHeSongSong2022\" class=\"citation arxiv cs1\">Meng, Chenlin; He, Yutong; Song, Yang; Song, Jiaming; Wu, Jiajun; Zhu, Jun-Yan; Ermon, Stefano (January 4, 2022). \"SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations\". <a href=\"/wiki/ArXiv_(identifier)\" class=\"mw-redirect\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"id-lock-free\" title=\"Freely accessible\"><a rel=\"nofollow\" class=\"external text\" href=\"https://arxiv.org/abs/2108.01073\">2108.01073</a></span> [<a rel=\"nofollow\" class=\"external text\" href=\"https://arxiv.org/archive/cs.CV\">cs.CV</a>].</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=SDEdit%3A+Guided+Image+Synthesis+and+Editing+with+Stochastic+Differential+Equations&amp;rft.date=2022-01-04&amp;rft_id=info%3Aarxiv%2F2108.01073&amp;rft.aulast=Meng&amp;rft.aufirst=Chenlin&amp;rft.au=He%2C+Yutong&amp;rft.au=Song%2C+Yang&amp;rft.au=Song%2C+Jiaming&amp;rft.au=Wu%2C+Jiajun&amp;rft.au=Zhu%2C+Jun-Yan&amp;rft.au=Ermon%2C+Stefano&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-69\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-69\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite id=\"CITEREFRombachBlattmannLorenzEsser2022\" class=\"citation book cs1\">Rombach, Robin; Blattmann, Andreas; Lorenz, Dominik; Esser, Patrick; Ommer, Björn (2022). <a rel=\"nofollow\" class=\"external text\" href=\"https://openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html\">\"High-Resolution Image Synthesis With Latent Diffusion Models\"</a>. <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</i>. pp.&#160;10684–10695. <a href=\"/wiki/ArXiv_(identifier)\" class=\"mw-redirect\" title=\"ArXiv (identifier)\">arXiv</a>:<span class=\"id-lock-free\" title=\"Freely accessible\"><a rel=\"nofollow\" class=\"external text\" href=\"https://arxiv.org/abs/2112.10752\">2112.10752</a></span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=High-Resolution+Image+Synthesis+With+Latent+Diffusion+Models&amp;rft.btitle=Proceedings+of+the+IEEE%2FCVF+Conference+on+Computer+Vision+and+Pattern+Recognition+%28CVPR%29&amp;rft.pages=10684-10695&amp;rft.date=2022&amp;rft_id=info%3Aarxiv%2F2112.10752&amp;rft.aulast=Rombach&amp;rft.aufirst=Robin&amp;rft.au=Blattmann%2C+Andreas&amp;rft.au=Lorenz%2C+Dominik&amp;rft.au=Esser%2C+Patrick&amp;rft.au=Ommer%2C+Bj%C3%B6rn&amp;rft_id=https%3A%2F%2Fopenaccess.thecvf.com%2Fcontent%2FCVPR2022%2Fhtml%2FRombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-70\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-70\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite class=\"citation web cs1\"><a rel=\"nofollow\" class=\"external text\" href=\"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/blob/main/LICENSE.md\">\"LICENSE.md · stabilityai/stable-diffusion-xl-base-1.0 at main\"</a>. <i>huggingface.co</i>. July 26, 2023<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">January 1,</span> 2024</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=huggingface.co&amp;rft.atitle=LICENSE.md+%C2%B7+stabilityai%2Fstable-diffusion-xl-base-1.0+at+main&amp;rft.date=2023-07-26&amp;rft_id=https%3A%2F%2Fhuggingface.co%2Fstabilityai%2Fstable-diffusion-xl-base-1.0%2Fblob%2Fmain%2FLICENSE.md&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-MIT-LAION-71\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-MIT-LAION_71-0\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite id=\"CITEREFHeikkilä2022\" class=\"citation web cs1\">Heikkilä, Melissa (September 16, 2022). <a rel=\"nofollow\" class=\"external text\" href=\"https://www.technologyreview.com/2022/09/16/1059598/this-artist-is-dominating-ai-generated-art-and-hes-not-happy-about-it/\">\"This artist is dominating AI-generated art. And he's not happy about it\"</a>. <i>MIT Technology Review</i>. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20230114125952/https://www.technologyreview.com/2022/09/16/1059598/this-artist-is-dominating-ai-generated-art-and-hes-not-happy-about-it/\">Archived</a> from the original on January 14, 2023<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">September 26,</span> 2022</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=MIT+Technology+Review&amp;rft.atitle=This+artist+is+dominating+AI-generated+art.+And+he%27s+not+happy+about+it.&amp;rft.date=2022-09-16&amp;rft.aulast=Heikkil%C3%A4&amp;rft.aufirst=Melissa&amp;rft_id=https%3A%2F%2Fwww.technologyreview.com%2F2022%2F09%2F16%2F1059598%2Fthis-artist-is-dominating-ai-generated-art-and-hes-not-happy-about-it%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-bijapan-72\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-bijapan_72-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-bijapan_72-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite id=\"CITEREFRyo_Shimizu2022\" class=\"citation web cs1 cs1-prop-foreign-lang-source\">Ryo Shimizu (August 26, 2022). <a rel=\"nofollow\" class=\"external text\" href=\"https://www.businessinsider.jp/post-258369\">\"Midjourneyを超えた？ 無料の作画AI｢ #StableDiffusion ｣が｢AIを民主化した｣と断言できる理由\"</a>. <i>Business Insider Japan</i> (in Japanese). <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20221210192453/https://www.businessinsider.jp/post-258369\">Archived</a> from the original on December 10, 2022<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">October 4,</span> 2022</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Business+Insider+Japan&amp;rft.atitle=Midjourney%E3%82%92%E8%B6%85%E3%81%88%E3%81%9F%EF%BC%9F+%E7%84%A1%E6%96%99%E3%81%AE%E4%BD%9C%E7%94%BBAI%EF%BD%A2+%23StableDiffusion+%EF%BD%A3%E3%81%8C%EF%BD%A2AI%E3%82%92%E6%B0%91%E4%B8%BB%E5%8C%96%E3%81%97%E3%81%9F%EF%BD%A3%E3%81%A8%E6%96%AD%E8%A8%80%E3%81%A7%E3%81%8D%E3%82%8B%E7%90%86%E7%94%B1&amp;rft.date=2022-08-26&amp;rft.au=Ryo+Shimizu&amp;rft_id=https%3A%2F%2Fwww.businessinsider.jp%2Fpost-258369&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-:13-73\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-:13_73-0\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite id=\"CITEREFCai\" class=\"citation web cs1\">Cai, Kenrick. <a rel=\"nofollow\" class=\"external text\" href=\"https://www.forbes.com/sites/kenrickcai/2022/09/07/stability-ai-funding-round-1-billion-valuation-stable-diffusion-text-to-image/\">\"Startup Behind AI Image Generator Stable Diffusion Is In Talks To Raise At A Valuation Up To $1 Billion\"</a>. <i>Forbes</i>. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20230930125226/https://www.forbes.com/sites/kenrickcai/2022/09/07/stability-ai-funding-round-1-billion-valuation-stable-diffusion-text-to-image/\">Archived</a> from the original on September 30, 2023<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">October 31,</span> 2022</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Forbes&amp;rft.atitle=Startup+Behind+AI+Image+Generator+Stable+Diffusion+Is+In+Talks+To+Raise+At+A+Valuation+Up+To+%241+Billion&amp;rft.aulast=Cai&amp;rft.aufirst=Kenrick&amp;rft_id=https%3A%2F%2Fwww.forbes.com%2Fsites%2Fkenrickcai%2F2022%2F09%2F07%2Fstability-ai-funding-round-1-billion-valuation-stable-diffusion-text-to-image%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-74\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-74\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite class=\"citation web cs1\"><a rel=\"nofollow\" class=\"external text\" href=\"https://www.bbc.com/news/uk-65932372\">\"Illegal trade in AI child sex abuse images exposed\"</a>. <i>BBC News</i>. June 27, 2023. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20230921100213/https://www.bbc.com/news/uk-65932372\">Archived</a> from the original on September 21, 2023<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">September 26,</span> 2023</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=BBC+News&amp;rft.atitle=Illegal+trade+in+AI+child+sex+abuse+images+exposed&amp;rft.date=2023-06-27&amp;rft_id=https%3A%2F%2Fwww.bbc.com%2Fnews%2Fuk-65932372&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-75\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-75\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite id=\"CITEREFMaiberg2024\" class=\"citation web cs1\">Maiberg, Emanuel (June 11, 2024). <a rel=\"nofollow\" class=\"external text\" href=\"https://www.404media.co/hackers-target-ai-users-with-malicious-stable-diffusion-tool-on-github/\">\"Hackers Target AI Users With Malicious Stable Diffusion Tool on GitHub to Protest 'Art Theft'<span class=\"cs1-kern-right\"></span>\"</a>. <i>404 Media</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">June 14,</span> 2024</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=404+Media&amp;rft.atitle=Hackers+Target+AI+Users+With+Malicious+Stable+Diffusion+Tool+on+GitHub+to+Protest+%27Art+Theft%27&amp;rft.date=2024-06-11&amp;rft.aulast=Maiberg&amp;rft.aufirst=Emanuel&amp;rft_id=https%3A%2F%2Fwww.404media.co%2Fhackers-target-ai-users-with-malicious-stable-diffusion-tool-on-github%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-76\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-76\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite id=\"CITEREFVincent2023\" class=\"citation web cs1\">Vincent, James (January 16, 2023). <a rel=\"nofollow\" class=\"external text\" href=\"https://www.theverge.com/2023/1/16/23557098/generative-ai-art-copyright-legal-lawsuit-stable-diffusion-midjourney-deviantart\">\"AI art tools Stable Diffusion and Midjourney targeted with copyright lawsuit\"</a>. <i>The Verge</i>. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20230309010528/https://www.theverge.com/2023/1/16/23557098/generative-ai-art-copyright-legal-lawsuit-stable-diffusion-midjourney-deviantart\">Archived</a> from the original on March 9, 2023<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">January 16,</span> 2023</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=The+Verge&amp;rft.atitle=AI+art+tools+Stable+Diffusion+and+Midjourney+targeted+with+copyright+lawsuit&amp;rft.date=2023-01-16&amp;rft.aulast=Vincent&amp;rft.aufirst=James&amp;rft_id=https%3A%2F%2Fwww.theverge.com%2F2023%2F1%2F16%2F23557098%2Fgenerative-ai-art-copyright-legal-lawsuit-stable-diffusion-midjourney-deviantart&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-CNN-Getty-77\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-CNN-Getty_77-0\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite id=\"CITEREFKorn2023\" class=\"citation web cs1\">Korn, Jennifer (January 17, 2023). <a rel=\"nofollow\" class=\"external text\" href=\"https://www.cnn.com/2023/01/17/tech/getty-images-stability-ai-lawsuit/index.html\">\"Getty Images suing the makers of popular AI art tool for allegedly stealing photos\"</a>. <i>CNN</i>. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20230301184246/https://www.cnn.com/2023/01/17/tech/getty-images-stability-ai-lawsuit/index.html\">Archived</a> from the original on March 1, 2023<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">January 22,</span> 2023</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=CNN&amp;rft.atitle=Getty+Images+suing+the+makers+of+popular+AI+art+tool+for+allegedly+stealing+photos&amp;rft.date=2023-01-17&amp;rft.aulast=Korn&amp;rft.aufirst=Jennifer&amp;rft_id=https%3A%2F%2Fwww.cnn.com%2F2023%2F01%2F17%2Ftech%2Fgetty-images-stability-ai-lawsuit%2Findex.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-Reuters-SDLawsuit-78\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-Reuters-SDLawsuit_78-0\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite id=\"CITEREFBrittain2023\" class=\"citation news cs1\">Brittain, Blake (July 19, 2023). <a rel=\"nofollow\" class=\"external text\" href=\"https://www.reuters.com/legal/litigation/us-judge-finds-flaws-artists-lawsuit-against-ai-companies-2023-07-19/\">\"US judge finds flaws in artists' lawsuit against AI companies\"</a>. <i>Reuters</i>. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20230906193839/https://www.reuters.com/legal/litigation/us-judge-finds-flaws-artists-lawsuit-against-ai-companies-2023-07-19/\">Archived</a> from the original on September 6, 2023<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">August 6,</span> 2023</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Reuters&amp;rft.atitle=US+judge+finds+flaws+in+artists%27+lawsuit+against+AI+companies&amp;rft.date=2023-07-19&amp;rft.aulast=Brittain&amp;rft.aufirst=Blake&amp;rft_id=https%3A%2F%2Fwww.reuters.com%2Flegal%2Flitigation%2Fus-judge-finds-flaws-artists-lawsuit-against-ai-companies-2023-07-19%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-stability-79\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-stability_79-0\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite class=\"citation web cs1\"><a rel=\"nofollow\" class=\"external text\" href=\"https://stability.ai/blog/stable-diffusion-public-release\">\"Stable Diffusion Public Release\"</a>. <i>Stability.Ai</i>. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20220830210535/https://stability.ai/blog/stable-diffusion-public-release\">Archived</a> from the original on August 30, 2022<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">August 31,</span> 2022</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Stability.Ai&amp;rft.atitle=Stable+Diffusion+Public+Release&amp;rft_id=https%3A%2F%2Fstability.ai%2Fblog%2Fstable-diffusion-public-release&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-80\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-80\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite class=\"citation web cs1\"><a rel=\"nofollow\" class=\"external text\" href=\"https://www.licenses.ai/blog/2022/8/18/naming-convention-of-responsible-ai-licenses\">\"From RAIL to Open RAIL: Topologies of RAIL Licenses\"</a>. <i>Responsible AI Licenses (RAIL)</i>. August 18, 2022. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20230727145215/https://www.licenses.ai/blog/2022/8/18/naming-convention-of-responsible-ai-licenses\">Archived</a> from the original on July 27, 2023<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">February 20,</span> 2023</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Responsible+AI+Licenses+%28RAIL%29&amp;rft.atitle=From+RAIL+to+Open+RAIL%3A+Topologies+of+RAIL+Licenses&amp;rft.date=2022-08-18&amp;rft_id=https%3A%2F%2Fwww.licenses.ai%2Fblog%2F2022%2F8%2F18%2Fnaming-convention-of-responsible-ai-licenses&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-washingtonpost-81\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-washingtonpost_81-0\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite class=\"citation news cs1\"><a rel=\"nofollow\" class=\"external text\" href=\"https://www.washingtonpost.com/technology/2022/08/30/deep-fake-video-on-agt/\">\"Ready or not, mass video deepfakes are coming\"</a>. <i>The Washington Post</i>. August 30, 2022. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20220831115010/https://www.washingtonpost.com/technology/2022/08/30/deep-fake-video-on-agt/\">Archived</a> from the original on August 31, 2022<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">August 31,</span> 2022</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Washington+Post&amp;rft.atitle=Ready+or+not%2C+mass+video+deepfakes+are+coming&amp;rft.date=2022-08-30&amp;rft_id=https%3A%2F%2Fwww.washingtonpost.com%2Ftechnology%2F2022%2F08%2F30%2Fdeep-fake-video-on-agt%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-82\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-82\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite class=\"citation web cs1\"><a rel=\"nofollow\" class=\"external text\" href=\"https://huggingface.co/spaces/CompVis/stable-diffusion-license\">\"License - a Hugging Face Space by CompVis\"</a>. <i>huggingface.co</i>. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20220904215616/https://huggingface.co/spaces/CompVis/stable-diffusion-license\">Archived</a> from the original on September 4, 2022<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">September 5,</span> 2022</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=huggingface.co&amp;rft.atitle=License+-+a+Hugging+Face+Space+by+CompVis&amp;rft_id=https%3A%2F%2Fhuggingface.co%2Fspaces%2FCompVis%2Fstable-diffusion-license&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n<li id=\"cite_note-83\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-83\">^</a></b></span> <span class=\"reference-text\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1215172403\"><cite id=\"CITEREFKatsuo_Ishida2022\" class=\"citation web cs1 cs1-prop-foreign-lang-source\">Katsuo Ishida (August 26, 2022). <a rel=\"nofollow\" class=\"external text\" href=\"https://forest.watch.impress.co.jp/docs/review/1434893.html\">\"言葉で指示した画像を凄いAIが描き出す「Stable Diffusion」 ～画像は商用利用も可能\"</a>. <i>Impress Corporation</i> (in Japanese). <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20221114020520/https://forest.watch.impress.co.jp/docs/review/1434893.html\">Archived</a> from the original on November 14, 2022<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">October 4,</span> 2022</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Impress+Corporation&amp;rft.atitle=%E8%A8%80%E8%91%89%E3%81%A7%E6%8C%87%E7%A4%BA%E3%81%97%E3%81%9F%E7%94%BB%E5%83%8F%E3%82%92%E5%87%84%E3%81%84AI%E3%81%8C%E6%8F%8F%E3%81%8D%E5%87%BA%E3%81%99%E3%80%8CStable+Diffusion%E3%80%8D+%EF%BD%9E%E7%94%BB%E5%83%8F%E3%81%AF%E5%95%86%E7%94%A8%E5%88%A9%E7%94%A8%E3%82%82%E5%8F%AF%E8%83%BD&amp;rft.date=2022-08-26&amp;rft.au=Katsuo+Ishida&amp;rft_id=https%3A%2F%2Fforest.watch.impress.co.jp%2Fdocs%2Freview%2F1434893.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AStable+Diffusion\" class=\"Z3988\"></span></span>\n</li>\n</ol></div></div>\n<h2><span class=\"mw-headline\" id=\"External_links\">External links</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Stable_Diffusion&amp;action=edit&amp;section=21\" title=\"Edit section: External links\"><span>edit</span></a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<style data-mw-deduplicate=\"TemplateStyles:r1217611005\">.mw-parser-output .side-box{margin:4px 0;box-sizing:border-box;border:1px solid #aaa;font-size:88%;line-height:1.25em;background-color:#f9f9f9;display:flow-root}.mw-parser-output .side-box-abovebelow,.mw-parser-output .side-box-text{padding:0.25em 0.9em}.mw-parser-output .side-box-image{padding:2px 0 2px 0.9em;text-align:center}.mw-parser-output .side-box-imageright{padding:2px 0.9em 2px 0;text-align:center}@media(min-width:500px){.mw-parser-output .side-box-flex{display:flex;align-items:center}.mw-parser-output .side-box-text{flex:1;min-width:0}}@media(min-width:720px){.mw-parser-output .side-box{width:238px}.mw-parser-output .side-box-right{clear:right;float:right;margin-left:1em}.mw-parser-output .side-box-left{margin-right:1em}}</style><div class=\"side-box side-box-right plainlinks sistersitebox\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1126788409\">\n<div class=\"side-box-flex\">\n<div class=\"side-box-image\"><span class=\"noviewer\" typeof=\"mw:File\"><span><img alt=\"\" src=\"//upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/30px-Commons-logo.svg.png\" decoding=\"async\" width=\"30\" height=\"40\" class=\"mw-file-element\" srcset=\"//upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/45px-Commons-logo.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/59px-Commons-logo.svg.png 2x\" data-file-width=\"1024\" data-file-height=\"1376\" /></span></span></div>\n<div class=\"side-box-text plainlist\">Wikimedia Commons has media related to <span style=\"font-weight: bold; font-style: italic;\"><a href=\"https://commons.wikimedia.org/wiki/Category:Stable_Diffusion\" class=\"extiw\" title=\"commons:Category:Stable Diffusion\">Stable Diffusion</a></span>.</div></div>\n</div>\n<ul><li><a rel=\"nofollow\" class=\"external text\" href=\"https://huggingface.co/spaces/stabilityai/stable-diffusion\">Stable Diffusion Demo</a></li>\n<li><a rel=\"nofollow\" class=\"external text\" href=\"https://poloclub.github.io/diffusion-explainer/\">Interactive Explanation of Stable Diffusion</a></li>\n<li><a rel=\"nofollow\" class=\"external text\" href=\"https://interaktiv.br.de/ki-trainingsdaten/en/index.html\">\"We Are All Raw Material for AI\"</a>: Investigation on sensitive and private data in Stable Diffusions training data</li>\n<li>\"<a rel=\"nofollow\" class=\"external text\" href=\"https://talkdigital.com.au/ai/stable-diffusion-negative-prompt-list/\">Negative Prompts in Stable Diffusion</a>\"</li></ul>\n<div class=\"navbox-styles\"><style data-mw-deduplicate=\"TemplateStyles:r1129693374\">.mw-parser-output .hlist dl,.mw-parser-output .hlist ol,.mw-parser-output .hlist ul{margin:0;padding:0}.mw-parser-output .hlist dd,.mw-parser-output .hlist dt,.mw-parser-output .hlist li{margin:0;display:inline}.mw-parser-output .hlist.inline,.mw-parser-output .hlist.inline dl,.mw-parser-output .hlist.inline ol,.mw-parser-output .hlist.inline ul,.mw-parser-output .hlist dl dl,.mw-parser-output .hlist dl ol,.mw-parser-output .hlist dl ul,.mw-parser-output .hlist ol dl,.mw-parser-output .hlist ol ol,.mw-parser-output .hlist ol ul,.mw-parser-output .hlist ul dl,.mw-parser-output .hlist ul ol,.mw-parser-output .hlist ul ul{display:inline}.mw-parser-output .hlist .mw-empty-li{display:none}.mw-parser-output .hlist dt::after{content:\": \"}.mw-parser-output .hlist dd::after,.mw-parser-output .hlist li::after{content:\" · \";font-weight:bold}.mw-parser-output .hlist dd:last-child::after,.mw-parser-output .hlist dt:last-child::after,.mw-parser-output .hlist li:last-child::after{content:none}.mw-parser-output .hlist dd dd:first-child::before,.mw-parser-output .hlist dd dt:first-child::before,.mw-parser-output .hlist dd li:first-child::before,.mw-parser-output .hlist dt dd:first-child::before,.mw-parser-output .hlist dt dt:first-child::before,.mw-parser-output .hlist dt li:first-child::before,.mw-parser-output .hlist li dd:first-child::before,.mw-parser-output .hlist li dt:first-child::before,.mw-parser-output .hlist li li:first-child::before{content:\" (\";font-weight:normal}.mw-parser-output .hlist dd dd:last-child::after,.mw-parser-output .hlist dd dt:last-child::after,.mw-parser-output .hlist dd li:last-child::after,.mw-parser-output .hlist dt dd:last-child::after,.mw-parser-output .hlist dt dt:last-child::after,.mw-parser-output .hlist dt li:last-child::after,.mw-parser-output .hlist li dd:last-child::after,.mw-parser-output .hlist li dt:last-child::after,.mw-parser-output .hlist li li:last-child::after{content:\")\";font-weight:normal}.mw-parser-output .hlist ol{counter-reset:listitem}.mw-parser-output .hlist ol>li{counter-increment:listitem}.mw-parser-output .hlist ol>li::before{content:\" \"counter(listitem)\"\\a0 \"}.mw-parser-output .hlist dd ol>li:first-child::before,.mw-parser-output .hlist dt ol>li:first-child::before,.mw-parser-output .hlist li ol>li:first-child::before{content:\" (\"counter(listitem)\"\\a0 \"}</style><style data-mw-deduplicate=\"TemplateStyles:r1228936124\">.mw-parser-output .navbox{box-sizing:border-box;border:1px solid #a2a9b1;width:100%;clear:both;font-size:88%;text-align:center;padding:1px;margin:1em auto 0}.mw-parser-output .navbox .navbox{margin-top:0}.mw-parser-output .navbox+.navbox,.mw-parser-output .navbox+.navbox-styles+.navbox{margin-top:-1px}.mw-parser-output .navbox-inner,.mw-parser-output .navbox-subgroup{width:100%}.mw-parser-output .navbox-group,.mw-parser-output .navbox-title,.mw-parser-output .navbox-abovebelow{padding:0.25em 1em;line-height:1.5em;text-align:center}.mw-parser-output .navbox-group{white-space:nowrap;text-align:right}.mw-parser-output .navbox,.mw-parser-output .navbox-subgroup{background-color:#fdfdfd}.mw-parser-output .navbox-list{line-height:1.5em;border-color:#fdfdfd}.mw-parser-output .navbox-list-with-group{text-align:left;border-left-width:2px;border-left-style:solid}.mw-parser-output tr+tr>.navbox-abovebelow,.mw-parser-output tr+tr>.navbox-group,.mw-parser-output tr+tr>.navbox-image,.mw-parser-output tr+tr>.navbox-list{border-top:2px solid #fdfdfd}.mw-parser-output .navbox-title{background-color:#ccf}.mw-parser-output .navbox-abovebelow,.mw-parser-output .navbox-group,.mw-parser-output .navbox-subgroup .navbox-title{background-color:#ddf}.mw-parser-output .navbox-subgroup .navbox-group,.mw-parser-output .navbox-subgroup .navbox-abovebelow{background-color:#e6e6ff}.mw-parser-output .navbox-even{background-color:#f7f7f7}.mw-parser-output .navbox-odd{background-color:transparent}.mw-parser-output .navbox .hlist td dl,.mw-parser-output .navbox .hlist td ol,.mw-parser-output .navbox .hlist td ul,.mw-parser-output .navbox td.hlist dl,.mw-parser-output .navbox td.hlist ol,.mw-parser-output .navbox td.hlist ul{padding:0.125em 0}.mw-parser-output .navbox .navbar{display:block;font-size:100%}.mw-parser-output .navbox-title .navbar{float:left;text-align:left;margin-right:0.5em}body.skin--responsive .mw-parser-output .navbox-image img{max-width:none!important}</style></div><div role=\"navigation\" class=\"navbox\" aria-labelledby=\"Differentiable_computing\" style=\"padding:3px\"><table class=\"nowraplinks hlist mw-collapsible autocollapse navbox-inner\" style=\"border-spacing:0;background:transparent;color:inherit\"><tbody><tr><th scope=\"col\" class=\"navbox-title\" colspan=\"2\"><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1129693374\"><style data-mw-deduplicate=\"TemplateStyles:r1063604349\">.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:\"[ \"}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:\" ]\"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}</style><div class=\"navbar plainlinks hlist navbar-mini\"><ul><li class=\"nv-view\"><a href=\"/wiki/Template:Differentiable_computing\" title=\"Template:Differentiable computing\"><abbr title=\"View this template\">v</abbr></a></li><li class=\"nv-talk\"><a href=\"/wiki/Template_talk:Differentiable_computing\" title=\"Template talk:Differentiable computing\"><abbr title=\"Discuss this template\">t</abbr></a></li><li class=\"nv-edit\"><a href=\"/wiki/Special:EditPage/Template:Differentiable_computing\" title=\"Special:EditPage/Template:Differentiable computing\"><abbr title=\"Edit this template\">e</abbr></a></li></ul></div><div id=\"Differentiable_computing\" style=\"font-size:114%;margin:0 4em\">Differentiable computing</div></th></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\"><a href=\"/wiki/Differentiable_function\" title=\"Differentiable function\">General</a></th><td class=\"navbox-list-with-group navbox-list navbox-odd\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n<ul><li><b><a href=\"/wiki/Differentiable_programming\" title=\"Differentiable programming\">Differentiable programming</a></b></li>\n<li><a href=\"/wiki/Information_geometry\" title=\"Information geometry\">Information geometry</a></li>\n<li><a href=\"/wiki/Statistical_manifold\" title=\"Statistical manifold\">Statistical manifold</a></li>\n<li><a href=\"/wiki/Automatic_differentiation\" title=\"Automatic differentiation\">Automatic differentiation</a></li>\n<li><a href=\"/wiki/Neuromorphic_engineering\" title=\"Neuromorphic engineering\">Neuromorphic engineering</a></li>\n<li><a href=\"/wiki/Pattern_recognition\" title=\"Pattern recognition\">Pattern recognition</a></li>\n<li><a href=\"/wiki/Tensor_calculus\" title=\"Tensor calculus\">Tensor calculus</a></li>\n<li><a href=\"/wiki/Computational_learning_theory\" title=\"Computational learning theory\">Computational learning theory</a></li>\n<li><a href=\"/wiki/Inductive_bias\" title=\"Inductive bias\">Inductive bias</a></li></ul>\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\">Concepts</th><td class=\"navbox-list-with-group navbox-list navbox-even\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n<ul><li><a href=\"/wiki/Gradient_descent\" title=\"Gradient descent\">Gradient descent</a>\n<ul><li><a href=\"/wiki/Stochastic_gradient_descent\" title=\"Stochastic gradient descent\">SGD</a></li></ul></li>\n<li><a href=\"/wiki/Cluster_analysis\" title=\"Cluster analysis\">Clustering</a></li>\n<li><a href=\"/wiki/Regression_analysis\" title=\"Regression analysis\">Regression</a>\n<ul><li><a href=\"/wiki/Overfitting\" title=\"Overfitting\">Overfitting</a></li></ul></li>\n<li><a href=\"/wiki/Hallucination_(artificial_intelligence)\" title=\"Hallucination (artificial intelligence)\">Hallucination</a></li>\n<li><a href=\"/wiki/Adversarial_machine_learning\" title=\"Adversarial machine learning\">Adversary</a></li>\n<li><a href=\"/wiki/Attention_(machine_learning)\" title=\"Attention (machine learning)\">Attention</a></li>\n<li><a href=\"/wiki/Convolution\" title=\"Convolution\">Convolution</a></li>\n<li><a href=\"/wiki/Loss_functions_for_classification\" title=\"Loss functions for classification\">Loss functions</a></li>\n<li><a href=\"/wiki/Backpropagation\" title=\"Backpropagation\">Backpropagation</a></li>\n<li><a href=\"/wiki/Batch_normalization\" title=\"Batch normalization\">Batchnorm</a></li>\n<li><a href=\"/wiki/Activation_function\" title=\"Activation function\">Activation</a>\n<ul><li><a href=\"/wiki/Softmax_function\" title=\"Softmax function\">Softmax</a></li>\n<li><a href=\"/wiki/Sigmoid_function\" title=\"Sigmoid function\">Sigmoid</a></li>\n<li><a href=\"/wiki/Rectifier_(neural_networks)\" title=\"Rectifier (neural networks)\">Rectifier</a></li></ul></li>\n<li><a href=\"/wiki/Regularization_(mathematics)\" title=\"Regularization (mathematics)\">Regularization</a></li>\n<li><a href=\"/wiki/Training,_validation,_and_test_sets\" class=\"mw-redirect\" title=\"Training, validation, and test sets\">Datasets</a>\n<ul><li><a href=\"/wiki/Data_augmentation\" title=\"Data augmentation\">Augmentation</a></li></ul></li>\n<li><a href=\"/wiki/Diffusion_process\" title=\"Diffusion process\">Diffusion</a></li>\n<li><a href=\"/wiki/Autoregressive_model\" title=\"Autoregressive model\">Autoregression</a></li></ul>\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\">Applications</th><td class=\"navbox-list-with-group navbox-list navbox-odd\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n<ul><li><a href=\"/wiki/Machine_learning\" title=\"Machine learning\">Machine learning</a>\n<ul><li><a href=\"/wiki/Prompt_engineering#In-context_learning\" title=\"Prompt engineering\">In-context learning</a></li></ul></li>\n<li><a href=\"/wiki/Artificial_neural_network\" class=\"mw-redirect\" title=\"Artificial neural network\">Artificial neural network</a>\n<ul><li><a href=\"/wiki/Deep_learning\" title=\"Deep learning\">Deep learning</a></li></ul></li>\n<li><a href=\"/wiki/Computational_science\" title=\"Computational science\">Scientific computing</a></li>\n<li><a href=\"/wiki/Artificial_intelligence\" title=\"Artificial intelligence\">Artificial Intelligence</a></li>\n<li><a href=\"/wiki/Language_model\" title=\"Language model\">Language model</a>\n<ul><li><a href=\"/wiki/Large_language_model\" title=\"Large language model\">Large language model</a></li></ul></li></ul>\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\">Hardware</th><td class=\"navbox-list-with-group navbox-list navbox-even\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n<ul><li><a href=\"/wiki/Graphcore\" title=\"Graphcore\">IPU</a></li>\n<li><a href=\"/wiki/Tensor_Processing_Unit\" title=\"Tensor Processing Unit\">TPU</a></li>\n<li><a href=\"/wiki/Vision_processing_unit\" title=\"Vision processing unit\">VPU</a></li>\n<li><a href=\"/wiki/Memristor\" title=\"Memristor\">Memristor</a></li>\n<li><a href=\"/wiki/SpiNNaker\" title=\"SpiNNaker\">SpiNNaker</a></li></ul>\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\">Software libraries</th><td class=\"navbox-list-with-group navbox-list navbox-odd\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n<ul><li><a href=\"/wiki/TensorFlow\" title=\"TensorFlow\">TensorFlow</a></li>\n<li><a href=\"/wiki/PyTorch\" title=\"PyTorch\">PyTorch</a></li>\n<li><a href=\"/wiki/Keras\" title=\"Keras\">Keras</a></li>\n<li><a href=\"/wiki/Theano_(software)\" title=\"Theano (software)\">Theano</a></li>\n<li><a href=\"/wiki/Google_JAX\" title=\"Google JAX\">JAX</a></li>\n<li><a href=\"/wiki/Flux_(machine-learning_framework)\" title=\"Flux (machine-learning framework)\">Flux.jl</a></li>\n<li><a href=\"/wiki/MindSpore\" title=\"MindSpore\">MindSpore</a></li></ul>\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\">Implementations</th><td class=\"navbox-list-with-group navbox-list navbox-odd\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\"></div><table class=\"nowraplinks navbox-subgroup\" style=\"border-spacing:0\"><tbody><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\">Audio–visual</th><td class=\"navbox-list-with-group navbox-list navbox-even\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n<ul><li><a href=\"/wiki/AlexNet\" title=\"AlexNet\">AlexNet</a></li>\n<li><a href=\"/wiki/WaveNet\" title=\"WaveNet\">WaveNet</a></li>\n<li><a href=\"/wiki/Human_image_synthesis\" title=\"Human image synthesis\">Human image synthesis</a></li>\n<li><a href=\"/wiki/Handwriting_recognition\" title=\"Handwriting recognition\">HWR</a></li>\n<li><a href=\"/wiki/Optical_character_recognition\" title=\"Optical character recognition\">OCR</a></li>\n<li><a href=\"/wiki/Deep_learning_speech_synthesis\" title=\"Deep learning speech synthesis\">Speech synthesis</a></li>\n<li><a href=\"/wiki/Speech_recognition\" title=\"Speech recognition\">Speech recognition</a></li>\n<li><a href=\"/wiki/Facial_recognition_system\" title=\"Facial recognition system\">Facial recognition</a></li>\n<li><a href=\"/wiki/AlphaFold\" title=\"AlphaFold\">AlphaFold</a></li>\n<li><a href=\"/wiki/Text-to-image_model\" title=\"Text-to-image model\">Text-to-image models</a>\n<ul><li><a href=\"/wiki/DALL-E\" title=\"DALL-E\">DALL-E</a></li>\n<li><a href=\"/wiki/Midjourney\" title=\"Midjourney\">Midjourney</a></li>\n<li><a class=\"mw-selflink selflink\">Stable Diffusion</a></li></ul></li>\n<li><a href=\"/wiki/Text-to-video_model\" title=\"Text-to-video model\">Text-to-video models</a>\n<ul><li><a href=\"/wiki/Sora_(text-to-video_model)\" title=\"Sora (text-to-video model)\">Sora</a></li>\n<li><a href=\"/wiki/VideoPoet\" title=\"VideoPoet\">VideoPoet</a></li></ul></li>\n<li><a href=\"/wiki/Whisper_(speech_recognition_system)\" title=\"Whisper (speech recognition system)\">Whisper</a></li></ul>\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\">Verbal</th><td class=\"navbox-list-with-group navbox-list navbox-odd\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n<ul><li><a href=\"/wiki/Word2vec\" title=\"Word2vec\">Word2vec</a></li>\n<li><a href=\"/wiki/Seq2seq\" title=\"Seq2seq\">Seq2seq</a></li>\n<li><a href=\"/wiki/BERT_(language_model)\" title=\"BERT (language model)\">BERT</a></li>\n<li><a href=\"/wiki/Gemini_(language_model)\" title=\"Gemini (language model)\">Gemini</a></li>\n<li><a href=\"/wiki/LaMDA\" title=\"LaMDA\">LaMDA</a>\n<ul><li><a href=\"/wiki/Bard_(chatbot)\" class=\"mw-redirect\" title=\"Bard (chatbot)\">Bard</a></li></ul></li>\n<li><a href=\"/wiki/Neural_machine_translation\" title=\"Neural machine translation\">NMT</a></li>\n<li><a href=\"/wiki/Project_Debater\" title=\"Project Debater\">Project Debater</a></li>\n<li><a href=\"/wiki/IBM_Watson\" title=\"IBM Watson\">IBM Watson</a></li>\n<li><a href=\"/wiki/IBM_Watsonx\" title=\"IBM Watsonx\">IBM Watsonx</a></li>\n<li><a href=\"/wiki/IBM_Granite\" title=\"IBM Granite\">Granite</a></li>\n<li><a href=\"/wiki/GPT-1\" title=\"GPT-1\">GPT-1</a></li>\n<li><a href=\"/wiki/GPT-2\" title=\"GPT-2\">GPT-2</a></li>\n<li><a href=\"/wiki/GPT-3\" title=\"GPT-3\">GPT-3</a></li>\n<li><a href=\"/wiki/GPT-4\" title=\"GPT-4\">GPT-4</a></li>\n<li><a href=\"/wiki/ChatGPT\" title=\"ChatGPT\">ChatGPT</a></li>\n<li><a href=\"/wiki/GPT-J\" title=\"GPT-J\">GPT-J</a></li>\n<li><a href=\"/wiki/Chinchilla_AI\" class=\"mw-redirect\" title=\"Chinchilla AI\">Chinchilla AI</a></li>\n<li><a href=\"/wiki/PaLM\" title=\"PaLM\">PaLM</a></li>\n<li><a href=\"/wiki/BLOOM_(language_model)\" title=\"BLOOM (language model)\">BLOOM</a></li>\n<li><a href=\"/wiki/LLaMA\" class=\"mw-redirect\" title=\"LLaMA\">LLaMA</a></li>\n<li><a href=\"/wiki/Huawei_PanGu\" title=\"Huawei PanGu\">PanGu-Σ</a></li></ul>\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\">Decisional</th><td class=\"navbox-list-with-group navbox-list navbox-even\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n<ul><li><a href=\"/wiki/AlphaGo\" title=\"AlphaGo\">AlphaGo</a></li>\n<li><a href=\"/wiki/AlphaZero\" title=\"AlphaZero\">AlphaZero</a></li>\n<li><a href=\"/wiki/Q-learning\" title=\"Q-learning\">Q-learning</a></li>\n<li><a href=\"/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action\" title=\"State–action–reward–state–action\">SARSA</a></li>\n<li><a href=\"/wiki/OpenAI_Five\" title=\"OpenAI Five\">OpenAI Five</a></li>\n<li><a href=\"/wiki/Self-driving_car\" title=\"Self-driving car\">Self-driving car</a></li>\n<li><a href=\"/wiki/MuZero\" title=\"MuZero\">MuZero</a></li>\n<li><a href=\"/wiki/Action_selection\" title=\"Action selection\">Action selection</a>\n<ul><li><a href=\"/wiki/Auto-GPT\" title=\"Auto-GPT\">Auto-GPT</a></li></ul></li>\n<li><a href=\"/wiki/Robot_control\" title=\"Robot control\">Robot control</a></li></ul>\n</div></td></tr></tbody></table><div></div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\">People</th><td class=\"navbox-list-with-group navbox-list navbox-odd\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n<ul><li><a href=\"/wiki/Yoshua_Bengio\" title=\"Yoshua Bengio\">Yoshua Bengio</a></li>\n<li><a href=\"/wiki/Alex_Graves_(computer_scientist)\" title=\"Alex Graves (computer scientist)\">Alex Graves</a></li>\n<li><a href=\"/wiki/Ian_Goodfellow\" title=\"Ian Goodfellow\">Ian Goodfellow</a></li>\n<li><a href=\"/wiki/Stephen_Grossberg\" title=\"Stephen Grossberg\">Stephen Grossberg</a></li>\n<li><a href=\"/wiki/Demis_Hassabis\" title=\"Demis Hassabis\">Demis Hassabis</a></li>\n<li><a href=\"/wiki/Geoffrey_Hinton\" title=\"Geoffrey Hinton\">Geoffrey Hinton</a></li>\n<li><a href=\"/wiki/Yann_LeCun\" title=\"Yann LeCun\">Yann LeCun</a></li>\n<li><a href=\"/wiki/Fei-Fei_Li\" title=\"Fei-Fei Li\">Fei-Fei Li</a></li>\n<li><a href=\"/wiki/Andrew_Ng\" title=\"Andrew Ng\">Andrew Ng</a></li>\n<li><a href=\"/wiki/J%C3%BCrgen_Schmidhuber\" title=\"Jürgen Schmidhuber\">Jürgen Schmidhuber</a></li>\n<li><a href=\"/wiki/David_Silver_(computer_scientist)\" title=\"David Silver (computer scientist)\">David Silver</a></li>\n<li><a href=\"/wiki/Ilya_Sutskever\" title=\"Ilya Sutskever\">Ilya Sutskever</a></li></ul>\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\">Organizations</th><td class=\"navbox-list-with-group navbox-list navbox-even\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n<ul><li><a href=\"/wiki/Anthropic\" title=\"Anthropic\">Anthropic</a></li>\n<li><a href=\"/wiki/EleutherAI\" title=\"EleutherAI\">EleutherAI</a></li>\n<li><a href=\"/wiki/Google_DeepMind\" title=\"Google DeepMind\">Google DeepMind</a></li>\n<li><a href=\"/wiki/Hugging_Face\" title=\"Hugging Face\">Hugging Face</a></li>\n<li><a href=\"/wiki/OpenAI\" title=\"OpenAI\">OpenAI</a></li>\n<li><a href=\"/wiki/Meta_AI\" title=\"Meta AI\">Meta AI</a></li>\n<li><a href=\"/wiki/Mila_(research_institute)\" title=\"Mila (research institute)\">Mila</a></li>\n<li><a href=\"/wiki/MIT_Computer_Science_and_Artificial_Intelligence_Laboratory\" title=\"MIT Computer Science and Artificial Intelligence Laboratory\">MIT CSAIL</a></li>\n<li><a href=\"/wiki/Huawei\" title=\"Huawei\">Huawei</a></li></ul>\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\">Architectures</th><td class=\"navbox-list-with-group navbox-list navbox-odd\" style=\"width:100%;padding:0\"><div style=\"padding:0 0.25em\">\n<ul><li><a href=\"/wiki/Neural_Turing_machine\" title=\"Neural Turing machine\">Neural Turing machine</a></li>\n<li><a href=\"/wiki/Differentiable_neural_computer\" title=\"Differentiable neural computer\">Differentiable neural computer</a></li>\n<li><a href=\"/wiki/Transformer_(machine_learning_model)\" class=\"mw-redirect\" title=\"Transformer (machine learning model)\">Transformer</a></li>\n<li><a href=\"/wiki/Recurrent_neural_network\" title=\"Recurrent neural network\">Recurrent neural network (RNN)</a></li>\n<li><a href=\"/wiki/Long_short-term_memory\" title=\"Long short-term memory\">Long short-term memory (LSTM)</a></li>\n<li><a href=\"/wiki/Gated_recurrent_unit\" title=\"Gated recurrent unit\">Gated recurrent unit (GRU)</a></li>\n<li><a href=\"/wiki/Echo_state_network\" title=\"Echo state network\">Echo state network</a></li>\n<li><a href=\"/wiki/Multilayer_perceptron\" title=\"Multilayer perceptron\">Multilayer perceptron (MLP)</a></li>\n<li><a href=\"/wiki/Convolutional_neural_network\" title=\"Convolutional neural network\">Convolutional neural network</a></li>\n<li><a href=\"/wiki/Residual_neural_network\" title=\"Residual neural network\">Residual neural network</a></li>\n<li><a href=\"/wiki/Mamba_(deep_learning)\" class=\"mw-redirect\" title=\"Mamba (deep learning)\">Mamba</a></li>\n<li><a href=\"/wiki/Autoencoder\" title=\"Autoencoder\">Autoencoder</a></li>\n<li><a href=\"/wiki/Variational_autoencoder\" title=\"Variational autoencoder\">Variational autoencoder (VAE)</a></li>\n<li><a href=\"/wiki/Generative_adversarial_network\" title=\"Generative adversarial network\">Generative adversarial network (GAN)</a></li>\n<li><a href=\"/wiki/Graph_neural_network\" title=\"Graph neural network\">Graph neural network</a></li></ul>\n</div></td></tr><tr><td class=\"navbox-abovebelow\" colspan=\"2\"><div>\n<ul><li><span class=\"noviewer\" typeof=\"mw:File\"><a href=\"/wiki/File:Symbol_portal_class.svg\" class=\"mw-file-description\" title=\"Portal\"><img alt=\"\" src=\"//upload.wikimedia.org/wikipedia/en/thumb/e/e2/Symbol_portal_class.svg/16px-Symbol_portal_class.svg.png\" decoding=\"async\" width=\"16\" height=\"16\" class=\"mw-file-element\" srcset=\"//upload.wikimedia.org/wikipedia/en/thumb/e/e2/Symbol_portal_class.svg/23px-Symbol_portal_class.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/e/e2/Symbol_portal_class.svg/31px-Symbol_portal_class.svg.png 2x\" data-file-width=\"180\" data-file-height=\"185\" /></a></span> Portals\n<ul><li><a href=\"/wiki/Portal:Computer_programming\" title=\"Portal:Computer programming\">Computer programming</a></li>\n<li><a href=\"/wiki/Portal:Technology\" title=\"Portal:Technology\">Technology</a></li></ul></li>\n<li><span class=\"noviewer\" typeof=\"mw:File\"><span title=\"Category\"><img alt=\"\" src=\"//upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/16px-Symbol_category_class.svg.png\" decoding=\"async\" width=\"16\" height=\"16\" class=\"mw-file-element\" srcset=\"//upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/23px-Symbol_category_class.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/31px-Symbol_category_class.svg.png 2x\" data-file-width=\"180\" data-file-height=\"185\" /></span></span> Categories\n<ul><li><a href=\"/wiki/Category:Artificial_neural_networks\" title=\"Category:Artificial neural networks\">Artificial neural networks</a></li>\n<li><a href=\"/wiki/Category:Machine_learning\" title=\"Category:Machine learning\">Machine learning</a></li></ul></li></ul>\n</div></td></tr></tbody></table></div>\n<!-- \nNewPP limit report\nParsed by mw‐web.codfw.main‐6d9bc5f875‐htggl\nCached time: 20240616113910\nCache expiry: 2592000\nReduced expiry: false\nComplications: [vary‐revision‐sha1, show‐toc]\nCPU time usage: 0.891 seconds\nReal time usage: 1.078 seconds\nPreprocessor visited node count: 5608/1000000\nPost‐expand include size: 212394/2097152 bytes\nTemplate argument size: 4147/2097152 bytes\nHighest expansion depth: 18/100\nExpensive parser function count: 11/500\nUnstrip recursion depth: 1/20\nUnstrip post‐expand size: 335964/5000000 bytes\nLua time usage: 0.556/10.000 seconds\nLua memory usage: 7918086/52428800 bytes\nNumber of Wikibase entities loaded: 2/400\n-->\n<!--\nTransclusion expansion time report (%,ms,calls,template)\n100.00%  926.310      1 -total\n 51.44%  476.525      1 Template:Reflist\n 32.39%  300.056     63 Template:Cite_web\n 17.49%  162.010      2 Template:Infobox\n 14.79%  137.041      1 Template:Infobox_software\n  9.36%   86.727      1 Template:Differentiable_computing\n  9.29%   86.037      2 Template:Navbox\n  6.95%   64.340      1 Template:Short_description\n  5.62%   52.079      9 Template:Cite_arXiv\n  4.17%   38.642      2 Template:Pagetype\n-->\n\n<!-- Saved in parser cache with key enwiki:pcache:idhash:71642695-0!canonical and timestamp 20240616113910 and revision id 1229364285. Rendering was triggered because: page-view\n -->\n</div><!--esi <esi:include src=\"/esitest-fa8a495983347898/content\" /> --><noscript><img src=\"https://login.wikimedia.org/wiki/Special:CentralAutoLogin/start?type=1x1\" alt=\"\" width=\"1\" height=\"1\" style=\"border: none; position: absolute;\"></noscript>\n<div class=\"printfooter\" data-nosnippet=\"\">Retrieved from \"<a dir=\"ltr\" href=\"https://en.wikipedia.org/w/index.php?title=Stable_Diffusion&amp;oldid=1229364285\">https://en.wikipedia.org/w/index.php?title=Stable_Diffusion&amp;oldid=1229364285</a>\"</div></div>\n\t\t\t\t\t<div id=\"catlinks\" class=\"catlinks\" data-mw=\"interface\"><div id=\"mw-normal-catlinks\" class=\"mw-normal-catlinks\"><a href=\"/wiki/Help:Category\" title=\"Help:Category\">Categories</a>: <ul><li><a href=\"/wiki/Category:Artificial_intelligence_art\" title=\"Category:Artificial intelligence art\">Artificial intelligence art</a></li><li><a href=\"/wiki/Category:Deep_learning_software_applications\" title=\"Category:Deep learning software applications\">Deep learning software applications</a></li><li><a href=\"/wiki/Category:Text-to-image_generation\" title=\"Category:Text-to-image generation\">Text-to-image generation</a></li><li><a href=\"/wiki/Category:Unsupervised_learning\" title=\"Category:Unsupervised learning\">Unsupervised learning</a></li><li><a href=\"/wiki/Category:Art_controversies\" title=\"Category:Art controversies\">Art controversies</a></li><li><a href=\"/wiki/Category:Works_involved_in_plagiarism_controversies\" title=\"Category:Works involved in plagiarism controversies\">Works involved in plagiarism controversies</a></li><li><a href=\"/wiki/Category:2022_software\" title=\"Category:2022 software\">2022 software</a></li></ul></div><div id=\"mw-hidden-catlinks\" class=\"mw-hidden-catlinks mw-hidden-cats-hidden\">Hidden categories: <ul><li><a href=\"/wiki/Category:CS1_maint:_multiple_names:_authors_list\" title=\"Category:CS1 maint: multiple names: authors list\">CS1 maint: multiple names: authors list</a></li><li><a href=\"/wiki/Category:CS1_maint:_numeric_names:_authors_list\" title=\"Category:CS1 maint: numeric names: authors list\">CS1 maint: numeric names: authors list</a></li><li><a href=\"/wiki/Category:CS1_Japanese-language_sources_(ja)\" title=\"Category:CS1 Japanese-language sources (ja)\">CS1 Japanese-language sources (ja)</a></li><li><a href=\"/wiki/Category:Articles_with_short_description\" title=\"Category:Articles with short description\">Articles with short description</a></li><li><a href=\"/wiki/Category:Short_description_matches_Wikidata\" title=\"Category:Short description matches Wikidata\">Short description matches Wikidata</a></li><li><a href=\"/wiki/Category:Use_mdy_dates_from_October_2023\" title=\"Category:Use mdy dates from October 2023\">Use mdy dates from October 2023</a></li><li><a href=\"/wiki/Category:All_articles_with_unsourced_statements\" title=\"Category:All articles with unsourced statements\">All articles with unsourced statements</a></li><li><a href=\"/wiki/Category:Articles_with_unsourced_statements_from_October_2023\" title=\"Category:Articles with unsourced statements from October 2023\">Articles with unsourced statements from October 2023</a></li><li><a href=\"/wiki/Category:Pages_using_multiple_image_with_auto_scaled_images\" title=\"Category:Pages using multiple image with auto scaled images\">Pages using multiple image with auto scaled images</a></li><li><a href=\"/wiki/Category:Commons_category_link_from_Wikidata\" title=\"Category:Commons category link from Wikidata\">Commons category link from Wikidata</a></li></ul></div></div>\n\t\t\t\t</div>\n\t\t\t</main>\n\t\t\t\n\t\t</div>\n\t\t<div class=\"mw-footer-container\">\n\t\t\t\n<footer id=\"footer\" class=\"mw-footer\" >\n\t<ul id=\"footer-info\">\n\t<li id=\"footer-info-lastmod\"> This page was last edited on 16 June 2024, at 11:38<span class=\"anonymous-show\">&#160;(UTC)</span>.</li>\n\t<li id=\"footer-info-copyright\">Text is available under the <a rel=\"license\" href=\"//en.wikipedia.org/wiki/Wikipedia:Text_of_the_Creative_Commons_Attribution-ShareAlike_4.0_International_License\">Creative Commons Attribution-ShareAlike License 4.0</a><a rel=\"license\" href=\"//en.wikipedia.org/wiki/Wikipedia:Text_of_the_Creative_Commons_Attribution-ShareAlike_4.0_International_License\" style=\"display:none;\"></a>;\nadditional terms may apply. By using this site, you agree to the <a href=\"//foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Terms_of_Use\">Terms of Use</a> and <a href=\"//foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Privacy_policy\">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href=\"//www.wikimediafoundation.org/\">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>\n</ul>\n\n\t<ul id=\"footer-places\">\n\t<li id=\"footer-places-privacy\"><a href=\"https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Privacy_policy\">Privacy policy</a></li>\n\t<li id=\"footer-places-about\"><a href=\"/wiki/Wikipedia:About\">About Wikipedia</a></li>\n\t<li id=\"footer-places-disclaimers\"><a href=\"/wiki/Wikipedia:General_disclaimer\">Disclaimers</a></li>\n\t<li id=\"footer-places-contact\"><a href=\"//en.wikipedia.org/wiki/Wikipedia:Contact_us\">Contact Wikipedia</a></li>\n\t<li id=\"footer-places-wm-codeofconduct\"><a href=\"https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Universal_Code_of_Conduct\">Code of Conduct</a></li>\n\t<li id=\"footer-places-developers\"><a href=\"https://developer.wikimedia.org\">Developers</a></li>\n\t<li id=\"footer-places-statslink\"><a href=\"https://stats.wikimedia.org/#/en.wikipedia.org\">Statistics</a></li>\n\t<li id=\"footer-places-cookiestatement\"><a href=\"https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Cookie_statement\">Cookie statement</a></li>\n\t<li id=\"footer-places-mobileview\"><a href=\"//en.m.wikipedia.org/w/index.php?title=Stable_Diffusion&amp;mobileaction=toggle_view_mobile\" class=\"noprint stopMobileRedirectToggle\">Mobile view</a></li>\n</ul>\n\n\t<ul id=\"footer-icons\" class=\"noprint\">\n\t<li id=\"footer-copyrightico\"><a href=\"https://wikimediafoundation.org/\"><img src=\"/static/images/footer/wikimedia-button.png\" srcset=\"/static/images/footer/wikimedia-button-1.5x.png 1.5x, /static/images/footer/wikimedia-button-2x.png 2x\" width=\"88\" height=\"31\" alt=\"Wikimedia Foundation\" loading=\"lazy\" /></a></li>\n\t<li id=\"footer-poweredbyico\"><a href=\"https://www.mediawiki.org/\"><img src=\"/static/images/footer/poweredby_mediawiki_88x31.png\" alt=\"Powered by MediaWiki\" srcset=\"/static/images/footer/poweredby_mediawiki_132x47.png 1.5x, /static/images/footer/poweredby_mediawiki_176x62.png 2x\" width=\"88\" height=\"31\" loading=\"lazy\"></a></li>\n</ul>\n\n</footer>\n\n\t\t</div>\n\t</div> \n</div> \n<div class=\"vector-settings\" id=\"p-dock-bottom\">\n\t<ul>\n\t\t<li>\n\t\t</li>\n\t</ul>\n</div>\n<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({\"wgHostname\":\"mw-web.codfw.main-6d9bc5f875-28ppd\",\"wgBackendResponseTime\":133,\"wgPageParseReport\":{\"limitreport\":{\"cputime\":\"0.891\",\"walltime\":\"1.078\",\"ppvisitednodes\":{\"value\":5608,\"limit\":1000000},\"postexpandincludesize\":{\"value\":212394,\"limit\":2097152},\"templateargumentsize\":{\"value\":4147,\"limit\":2097152},\"expansiondepth\":{\"value\":18,\"limit\":100},\"expensivefunctioncount\":{\"value\":11,\"limit\":500},\"unstrip-depth\":{\"value\":1,\"limit\":20},\"unstrip-size\":{\"value\":335964,\"limit\":5000000},\"entityaccesscount\":{\"value\":2,\"limit\":400},\"timingprofile\":[\"100.00%  926.310      1 -total\",\" 51.44%  476.525      1 Template:Reflist\",\" 32.39%  300.056     63 Template:Cite_web\",\" 17.49%  162.010      2 Template:Infobox\",\" 14.79%  137.041      1 Template:Infobox_software\",\"  9.36%   86.727      1 Template:Differentiable_computing\",\"  9.29%   86.037      2 Template:Navbox\",\"  6.95%   64.340      1 Template:Short_description\",\"  5.62%   52.079      9 Template:Cite_arXiv\",\"  4.17%   38.642      2 Template:Pagetype\"]},\"scribunto\":{\"limitreport-timeusage\":{\"value\":\"0.556\",\"limit\":\"10.000\"},\"limitreport-memusage\":{\"value\":7918086,\"limit\":52428800}},\"cachereport\":{\"origin\":\"mw-web.codfw.main-6d9bc5f875-htggl\",\"timestamp\":\"20240616113910\",\"ttl\":2592000,\"transientcontent\":false}}});});</script>\n<script type=\"application/ld+json\">{\"@context\":\"https:\\/\\/schema.org\",\"@type\":\"Article\",\"name\":\"Stable Diffusion\",\"url\":\"https:\\/\\/en.wikipedia.org\\/wiki\\/Stable_Diffusion\",\"sameAs\":\"http:\\/\\/www.wikidata.org\\/entity\\/Q113660857\",\"mainEntity\":\"http:\\/\\/www.wikidata.org\\/entity\\/Q113660857\",\"author\":{\"@type\":\"Organization\",\"name\":\"Contributors to Wikimedia projects\"},\"publisher\":{\"@type\":\"Organization\",\"name\":\"Wikimedia Foundation, Inc.\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\/\\/www.wikimedia.org\\/static\\/images\\/wmf-hor-googpub.png\"}},\"datePublished\":\"2022-08-31T09:19:32Z\",\"dateModified\":\"2024-06-16T11:38:19Z\",\"image\":\"https:\\/\\/upload.wikimedia.org\\/wikipedia\\/commons\\/d\\/d3\\/Astronaut_Riding_a_Horse_%28SDXL%29.jpg\",\"headline\":\"image-generating machine learning model\"}</script>\n</body>\n</html>\n"
      ]
    },
    {
      "id": 10,
      "type": "ShowText|pysssss",
      "pos": [
        2800,
        620
      ],
      "size": {
        "0": 860,
        "1": 1700
      },
      "flags": {},
      "order": 5,
      "mode": 0,
      "inputs": [
        {
          "name": "text",
          "type": "STRING",
          "link": 10,
          "widget": {
            "name": "text"
          }
        }
      ],
      "outputs": [
        {
          "name": "STRING",
          "type": "STRING",
          "links": [
            37
          ],
          "shape": 6,
          "slot_index": 0
        }
      ],
      "properties": {
        "Node name for S&R": "ShowText|pysssss"
      },
      "widgets_values": [
        "",
        "- Main page\n- Contents\n- Current events\n- Random article\n- About Wikipedia\n- Contact us\n- Donate\n- Help\n- Learn to edit\n- Community portal\n- Recent changes\n- Upload file\n- Create account\n\n- Log in\n\n-  Create account\n-  Log in\n- Contributions\n- Talk\n## Contents\n- \n\n(Top)\n\n\n- \n\n\n1\nDevelopment\n\n\n\n\n\n- \n\n\n2\nTechnology\n\n\n\n\nToggle Technology subsection\n\n\n\n\n\n2.1\nArchitecture\n\n\n\n\n\n\n2.1.1\nSD XL\n\n\n\n\n\n\n\n\n2.1.2\nSD 3.0\n\n\n\n\n\n\n\n\n\n\n2.2\nTraining data\n\n\n\n\n\n\n\n\n2.3\nTraining procedures\n\n\n\n\n\n\n\n\n2.4\nLimitations\n\n\n\n\n\n\n\n\n2.5\nEnd-user fine-tuning\n\n\n\n\n\n\n\n- \n\n\n2.1\nArchitecture\n\n\n\n\n\n\n2.1.1\nSD XL\n\n\n\n\n\n\n\n\n2.1.2\nSD 3.0\n\n\n\n\n\n\n\n- \n\n\n2.1.1\nSD XL\n\n\n\n\n\n- \n\n\n2.1.2\nSD 3.0\n\n\n\n\n\n- \n\n\n2.2\nTraining data\n\n\n\n\n\n- \n\n\n2.3\nTraining procedures\n\n\n\n\n\n- \n\n\n2.4\nLimitations\n\n\n\n\n\n- \n\n\n2.5\nEnd-user fine-tuning\n\n\n\n\n\n- \n\n\n3\nCapabilities\n\n\n\n\nToggle Capabilities subsection\n\n\n\n\n\n3.1\nText to image generation\n\n\n\n\n\n\n\n\n3.2\nImage modification\n\n\n\n\n\n\n\n\n3.3\nControlNet\n\n\n\n\n\n\n\n- \n\n\n3.1\nText to image generation\n\n\n\n\n\n- \n\n\n3.2\nImage modification\n\n\n\n\n\n- \n\n\n3.3\nControlNet\n\n\n\n\n\n- \n\n\n4\nReleases\n\n\n\n\n\n- \n\n\n5\nUsage and controversy\n\n\n\n\nToggle Usage and controversy subsection\n\n\n\n\n\n5.1\nComfyUI extension compromise\n\n\n\n\n\n\n\n- \n\n\n5.1\nComfyUI extension compromise\n\n\n\n\n\n- \n\n\n6\nLitigation\n\n\n\n\n\n- \n\n\n7\nLicense\n\n\n\n\n\n- \n\n\n8\nSee also\n\n\n\n\n\n- \n\n\n9\nReferences\n\n\n\n\n\n- \n\n\n10\nExternal links\n\n\n\n\n\n# Stable Diffusion\n- Azərbaycanca\n- Български\n- Català\n- Čeština\n- Deutsch\n- Español\n- فارسی\n- Français\n- Galego\n- 한국어\n- Bahasa Indonesia\n- Italiano\n- עברית\n- Nederlands\n- 日本語\n- Português\n- Runa Simi\n- Русский\n- සිංහල\n- Suomi\n- Türkçe\n- Українська\n- Tiếng Việt\n- 粵語\n- 中文\n- Article\n- Talk\n- Read\n- Edit\n- View history\n- Read\n- Edit\n- View history\n- What links here\n- Related changes\n- Upload file\n- Special pages\n- Permanent link\n- Page information\n- Cite this page\n- Get shortened URL\n- Download QR code\n- Wikidata item\n- Download as PDF\n- Printable version\n- Wikimedia Commons\n\n\n- github.com/Stability-AI/generative-models \nStable Diffusion is a deep learning, text-to-image model released in 2022 based on diffusion techniques. The generative artificial intelligence technology is the premier product of Stability AI and is considered to be a part of the ongoing artificial intelligence boom.\n\nIt is primarily used to generate detailed images conditioned on text descriptions, though it can also be applied to other tasks such as inpainting, outpainting, and generating image-to-image translations guided by a text prompt.[3] Its development involved  researchers from the CompVis Group at Ludwig Maximilian University of Munich and Runway with a computational donation from Stability and training data from non-profit organizations.[4][5][6][7]\n\nStable Diffusion is a latent diffusion model, a kind of deep generative artificial neural network. Its code and model weights have been released publicly,[8] and it can run on most consumer hardware equipped with a modest GPU with at least 4 GB VRAM. This marked a departure from previous proprietary text-to-image models such as DALL-E and Midjourney which were accessible only via cloud services.[9][10]\n\n## Development[edit]\nStable Diffusion, originated from a project called Latent Diffusion,[11] developed in Germany by researchers at Ludwig Maximilian University in Munich and Heidelberg University. Four of the original 5 authors (Robin Rombach, Andreas Blattmann, Patrick Esser and Dominik Lorenz) later joined Stability AI and released subsequent versions of Stable Diffusion.[12]\n\nThe technical license for the model was released by the CompVis group at Ludwig Maximilian University of Munich.[10] Development was led by Patrick Esser of Runway and Robin Rombach of CompVis, who were among the researchers who had earlier invented the latent diffusion model architecture used by Stable Diffusion.[7] Stability AI also credited EleutherAI and LAION (a German nonprofit which assembled the dataset on which Stable Diffusion was trained) as supporters of the project.[7]\n\n## Technology[edit]\n### Architecture[edit]\nStable Diffusion uses a kind of diffusion model (DM), called a latent diffusion model (LDM), developed by the CompVis group at LMU Munich.[13][8] Introduced in 2015, diffusion models are trained with the objective of removing successive applications of Gaussian noise on training images, which can be thought of as a sequence of denoising autoencoders. Stable Diffusion consists of 3 parts: the variational autoencoder (VAE), U-Net, and an optional text encoder.[14] The VAE encoder compresses the image from pixel space to a smaller dimensional latent space, capturing a more fundamental semantic meaning of the image.[13] Gaussian noise is iteratively applied to the compressed latent representation during forward diffusion.[14] The U-Net block, composed of a ResNet backbone, denoises the output from forward diffusion backwards to obtain a latent representation. Finally, the VAE decoder generates the final image by converting the representation back into pixel space.[14]\n\nThe denoising step can be flexibly conditioned on a string of text, an image, or another modality. The encoded conditioning data is exposed to denoising U-Nets via a cross-attention mechanism.[14] For conditioning on text, the fixed, pretrained CLIP ViT-L/14 text encoder is used to transform text prompts to an embedding space.[8] Researchers point to increased computational efficiency for training and generation as an advantage of LDMs.[7][13]\n\nThe name diffusion takes inspiration from the thermodynamic diffusion and an important link was made between this purely physical field and deep learning in 2015.[15][16]\n\nWith 860 million parameters in the U-Net and 123 million in the text encoder, Stable Diffusion is considered relatively lightweight by 2022 standards, and unlike other diffusion models, it can run on consumer GPUs,[17] and even CPU-only if using the OpenVINO version of Stable Diffusion.[18]\n\nThe XL version uses the same architecture,[19] except larger: larger UNet backbone, larger cross-attention context, two text encoders instead of one, and trained on multiple aspect ratios (not just the square aspect ratio like previous versions).\n\nThe SD XL Refiner, released at the same time, has the same architecture as SD XL, but it was trained for adding fine details to preexisting images via text-conditional img2img.\n\nThe 3.0 version[20] completely changes the backbone. Not a UNet, but a Rectified Flow Transformer, which implements the rectified flow method[21][22] with a Transformer.\n\nThe Transformer architecture used for SD 3.0 has three \"tracks\", for original text encoding, transformed text encoding, and image encoding (in latent space). The transformed text encoding and image encoding are mixed during each transformer block.\n\nThe architecture is named \"multimodal diffusion transformer (MMDiT), where the \"multimodal\" means that it mixes text and image encodings inside its operations. This differs from previous versions of DiT, where the text encoding affects the image encoding, but not vice versa.\n\n### Training data[edit]\nStable Diffusion was trained on pairs of images and captions taken from LAION-5B, a publicly available dataset derived from Common Crawl data scraped from the web, where 5 billion image-text pairs were classified based on language and filtered into separate datasets by resolution, a predicted likelihood of containing a watermark, and predicted \"aesthetic\" score (e.g. subjective visual quality).[23] The dataset was created by LAION, a German non-profit which receives funding from Stability AI.[23][24] The Stable Diffusion model was trained on three subsets of LAION-5B: laion2B-en, laion-high-resolution, and laion-aesthetics v2 5+.[23] A third-party analysis of the model's training data identified that out of a smaller subset of 12 million images taken from the original wider dataset used, approximately 47% of the sample size of images came from 100 different domains, with Pinterest taking up 8.5% of the subset, followed by websites such as WordPress, Blogspot, Flickr, DeviantArt and Wikimedia Commons.[citation needed]  An investigation by Bayerischer Rundfunk showed that LAION's datasets, hosted on Hugging Face, contain large amounts of private and sensitive data.[25]\n\n### Training procedures[edit]\nThe model was initially trained on the laion2B-en and laion-high-resolution subsets, with the last few rounds of training done on LAION-Aesthetics v2 5+, a subset of 600 million captioned images which the LAION-Aesthetics Predictor V2 predicted that humans would, on average, give a score of at least 5 out of 10 when asked to rate how much they liked them.[26][23][27] The LAION-Aesthetics v2 5+ subset also excluded low-resolution images and images which LAION-5B-WatermarkDetection identified as carrying a watermark with greater than 80% probability.[23] Final rounds of training additionally dropped 10% of text conditioning to improve Classifier-Free Diffusion Guidance.[28]\n\nThe model was trained using 256 Nvidia A100 GPUs on Amazon Web Services for a total of 150,000 GPU-hours, at a cost of $600,000.[29][30][31]\n\nSD3 was trained at a cost of around $10 million.[32]\n\n### Limitations[edit]\nStable Diffusion has issues with degradation and inaccuracies in certain scenarios. Initial releases of the model were trained on a dataset that consists of 512×512 resolution images, meaning that the quality of generated images noticeably degrades when user specifications deviate from its \"expected\" 512×512 resolution;[33] the version 2.0 update of the Stable Diffusion model later introduced the ability to natively generate images at 768×768 resolution.[34] Another challenge is in generating human limbs due to poor data quality of limbs in the LAION database.[35] The model is insufficiently trained to understand human limbs and faces due to the lack of representative features in the database, and prompting the model to generate images of such type can confound the model.[36] Stable Diffusion XL (SDXL) version 1.0, released in July 2023, introduced native 1024x1024 resolution and improved generation for limbs and text.[37][38]\n\nAccessibility for individual developers can also be a problem. In order to customize the model for new use cases that are not included in the dataset, such as generating anime characters (\"waifu diffusion\"),[39] new data and further training are required. Fine-tuned adaptations of Stable Diffusion created through additional retraining have been used for a variety of different use-cases, from medical imaging[40] to algorithmically generated music.[41] However, this fine-tuning process is sensitive to the quality of new data; low resolution images or different resolutions from the original data can not only fail to learn the new task but degrade the overall performance of the model. Even when the model is additionally trained on high quality images, it is difficult for individuals to run models in consumer electronics. For example, the training process for waifu-diffusion requires a minimum 30 GB of VRAM,[42] which exceeds the usual resource provided in such consumer GPUs as Nvidia's GeForce 30 series, which has only about 12 GB.[43]\n\nThe creators of Stable Diffusion acknowledge the potential for algorithmic bias, as the model was primarily trained on images with English descriptions.[30] As a result, generated images reinforce social biases and are from a western perspective, as the creators note that the model lacks data from other communities and cultures. The model gives more accurate results for prompts that are written in English in comparison to those written in other languages, with western or white cultures often being the default representation.[30]\n\n### End-user fine-tuning[edit]\nTo address the limitations of the model's initial training, end-users may opt to implement additional training to fine-tune generation outputs to match more specific use-cases, a process also referred to as personalization. There are three methods in which user-accessible fine-tuning can be applied to a Stable Diffusion model checkpoint:\n\n- An \"embedding\" can be trained from a collection of user-provided images, and allows the model to generate visually similar images whenever the name of the embedding is used within a generation prompt.[44] Embeddings are based on the \"textual inversion\" concept developed by researchers from Tel Aviv University in 2022 with support from Nvidia, where vector representations for specific tokens used by the model's text encoder are linked to new pseudo-words. Embeddings can be used to reduce biases within the original model, or mimic visual styles.[45]\n- A \"hypernetwork\" is a small pretrained neural network that is applied to various points within a larger neural network, and refers to the technique created by NovelAI developer Kurumuz in 2021, originally intended for text-generation transformer models. Hypernetworks steer results towards a particular direction, allowing Stable Diffusion-based models to imitate the art style of specific artists, even if the artist is not recognised by the original model; they process the image by finding key areas of importance such as hair and eyes, and then patch these areas in secondary latent space.[46]\n- DreamBooth is a deep learning generation model developed by researchers from Google Research and Boston University in 2022 which can fine-tune the model to generate precise, personalised outputs that depict a specific subject, following training via a set of images which depict the subject.[47]\n## Capabilities[edit]\nThe Stable Diffusion model supports the ability to generate new images from scratch through the use of a text prompt describing elements to be included or omitted from the output.[8] Existing images can be re-drawn by the model to incorporate new elements described by a text prompt (a process known as \"guided image synthesis\"[48]) through its diffusion-denoising mechanism.[8] In addition, the model also allows the use of prompts to partially alter existing images via inpainting and outpainting, when used with an appropriate user interface that supports such features, of which numerous different open source implementations exist.[49]\n\nStable Diffusion is recommended to be run with 10 GB or more VRAM, however users with less VRAM may opt to load the weights in float16 precision instead of the default float32 to tradeoff model performance with lower VRAM usage.[33]\n\n### Text to image generation[edit]\n- Top: no negative prompt\n- Centre: \"green trees\"\n- Bottom: \"round stones, round rocks\"\nThe text to image sampling script within Stable Diffusion, known as \"txt2img\", consumes a text prompt in addition to assorted option parameters covering sampling types, output image dimensions, and seed values. The script outputs an image file based on the model's interpretation of the prompt.[8] Generated images are tagged with an invisible digital watermark to allow users to identify an image as generated by Stable Diffusion,[8] although this watermark loses its efficacy if the image is resized or rotated.[50]\n\nEach txt2img generation will involve a specific seed value which affects the output image. Users may opt to randomize the seed in order to explore different generated outputs, or use the same seed to obtain the same image output as a previously generated image.[33] Users are also able to adjust the number of inference steps for the sampler; a higher value takes a longer duration of time, however a smaller value may result in visual defects.[33] Another configurable option, the classifier-free guidance scale value, allows the user to adjust how closely the output image adheres to the prompt.[28] More experimentative use cases may opt for a lower scale value, while use cases aiming for more specific outputs may use a higher value.[33]\n\nAdditional text2img features are provided by front-end implementations of Stable Diffusion, which allow users to modify the weight given to specific parts of the text prompt. Emphasis markers allow users to add or reduce emphasis to keywords by enclosing them with brackets.[51] An alternative method of adjusting weight to parts of the prompt are \"negative prompts\". Negative prompts are a feature included in some front-end implementations, including Stability AI's own DreamStudio cloud service, and allow the user to specify prompts which the model should avoid during image generation. The specified prompts may be undesirable image features that would otherwise be present within image outputs due to the positive prompts provided by the user, or due to how the model was originally trained, with mangled human hands being a common example.[49][52]\n\n### Image modification[edit]\n- Left: Original image created with Stable Diffusion 1.5\n- Right: Modified image created with Stable Diffusion XL 1.0\nStable Diffusion also includes another sampling script, \"img2img\", which consumes a text prompt, path to an existing image, and strength value between 0.0 and 1.0. The script outputs a new image based on the original image that also features elements provided within the text prompt. The strength value denotes the amount of noise added to the output image. A higher strength value produces more variation within the image but may produce an image that is not semantically consistent with the prompt provided.[8]\n\nThe ability of img2img to add noise to the original image makes it potentially useful for data anonymization and data augmentation, in which the visual features of image data are changed and anonymized.[53] The same process may also be useful for image upscaling, in which the resolution of an image is increased, with more detail potentially being added to the image.[53] Additionally, Stable Diffusion has been experimented with as a tool for image compression. Compared to JPEG and WebP, the recent methods used for image compression in Stable Diffusion face limitations in preserving small text and faces.[54]\n\nAdditional use-cases for image modification via img2img are offered by numerous front-end implementations of the Stable Diffusion model. Inpainting involves selectively modifying a portion of an existing image delineated by a user-provided layer mask, which fills the masked space with newly generated content based on the provided prompt.[49] A dedicated model specifically fine-tuned for inpainting use-cases was created by Stability AI alongside the release of Stable Diffusion 2.0.[34] Conversely, outpainting extends an image beyond its original dimensions, filling the previously empty space with content generated based on the provided prompt.[49]\n\nA depth-guided model, named \"depth2img\", was introduced with the release of Stable Diffusion 2.0 on November 24, 2022; this model infers the depth of the provided input image, and generates a new output image based on both the text prompt and the depth information, which allows the coherence and depth of the original input image to be maintained in the generated output.[34]\n\n### ControlNet[edit]\nControlNet[55] is a neural network architecture designed to manage diffusion models by incorporating additional conditions. It duplicates the weights of neural network blocks into a \"locked\" copy and a \"trainable\" copy. The \"trainable\" copy learns the desired condition, while the \"locked\" copy preserves the original model. This approach ensures that training with small datasets of image pairs does not compromise the integrity of production-ready diffusion models. The \"zero convolution\" is a 1×1 convolution with both weight and bias initialized to zero. Before training, all zero convolutions produce zero output, preventing any distortion caused by ControlNet. No layer is trained from scratch; the process is still fine-tuning, keeping the original model secure. This method enables training on small-scale or even personal devices.\n\n## Releases[edit]\naround 3.5x larger than previous versions.[63]\n\nKey papers\n\n- Learning Transferable Visual Models From Natural Language Supervision (2021).[67] This paper describes the CLIP method for training text encoders, which convert text into floating point vectors. Such text encodings are used by the diffusion model to create images.\n- SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations (2021).[68] This paper describes SDEdit, aka \"img2img\".\n- High-Resolution Image Synthesis with Latent Diffusion Models (2021, updated in 2022).[69] This paper describes the latent diffusion model (LDM). This is the backbone of the Stable Diffusion architecture.\n- Classifier-Free Diffusion Guidance (2022).[28] This paper describes CFG, which allows the text encoding vector to steer the diffusion model towards creating the image described by the text.\n- SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis (2023).[19] Describes SDXL.\n- Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow (2022).[21][22] Describes rectified flow, which is used for the backbone architecture of SD 3.0.\n- Scaling Rectified Flow Transformers for High-resolution Image Synthesis (2024).[20] Describes SD 3.0.\nTraining cost\n\n- SD 2.0: 0.2 million hours on A100 (40GB).[59]\n## Usage and controversy[edit]\nStable Diffusion claims no rights on generated images and freely gives users the rights of usage to any generated images from the model provided that the image content is not illegal or harmful to individuals.[70]\n\nThe images Stable Diffusion was trained on have been filtered without human input, leading to some harmful images and large amounts of private and sensitive information appearing in the training data.[25]\n\nMore traditional visual artists have expressed concern that widespread usage of image synthesis software such as Stable Diffusion may eventually lead to human artists, along with photographers, models, cinematographers, and actors, gradually losing commercial viability against AI-based competitors.[71]\n\nStable Diffusion is notably more permissive in the types of content users may generate, such as violent or sexually explicit imagery, in comparison to other commercial products based on generative AI.[72] Addressing the concerns that the model may be used for abusive purposes, CEO of Stability AI, Emad Mostaque, argues that \"[it is] peoples' responsibility as to whether they are ethical, moral, and legal in how they operate this technology\",[10] and that putting the capabilities of Stable Diffusion into the hands of the public would result in the technology providing a net benefit, in spite of the potential negative consequences.[10] In addition, Mostaque argues that the intention behind the open availability of Stable Diffusion is to end corporate control and dominance over such technologies, who have previously only developed closed AI systems for image synthesis.[10][72] This is reflected by the fact that any restrictions Stability AI places on the content that users may generate can easily be bypassed due to the availability of the source code.[73]\n\nControversy around photorealistic sexualized depictions of underage characters have been brought up, due to such images generated by Stable Diffusion being shared on websites such as Pixiv.[74]\n\n### ComfyUI extension compromise[edit]\nIn June 2024, A hacker group called \"Nullbulge\" compromised an extension of a graphic user interface for Stable Diffusion called ComfyUI to add malicious code to it. The compromised extension, called ComfyUI_LLMVISION, was used for integrating the interface with AI language models GPT-4 and Claude 3, and was hosted on Github. Nullbulge hosted a list of hundreds of ComfyUI users' login details across multiple services on its website, while users of the extension reported receiving numerous login notifications. vpnMentor conducted security research on the extension and claimed it could \"steal crypto wallets, screenshot the user’s screen, expose device information and IP addresses, and steal files that contain certain keywords or extensions\". \n\nNullbulge's website claims they targeted users who committed \"one of our sins\", which included AI-art generation, art theft, promoting cryptocurrency, and any other kind of theft from artists such as from Patreon. They claimed that they were \"a collective of individuals who believe in the importance of protecting artists' rights and ensuring fair compensation for their work\" and that they believed that \"AI-generated artwork is detrimental to the creative industry and should be discouraged\".[75]\n\n## Litigation[edit]\nIn January 2023, three artists, Sarah Andersen, Kelly McKernan, and Karla Ortiz, filed a copyright infringement lawsuit against Stability AI, Midjourney, and DeviantArt, claiming that these companies have infringed the rights of millions of artists by training AI tools on five billion images scraped from the web without the consent of the original artists.[76] The same month, Stability AI was also sued by Getty Images for using its images in the training data.[77]\n\nIn July 2023, U.S. District Judge William Orrick inclined to dismiss most of the lawsuit filed by Andersen, McKernan, and Ortiz but allowed them to file a new complaint.[78]\n\n## License[edit]\nUnlike models like DALL-E, Stable Diffusion makes its source code available,[79][8] along with the model (pretrained weights). It applies the Creative ML OpenRAIL-M license, a form of Responsible AI License (RAIL), to the model (M).[80] The license prohibits certain use cases, including crime, libel, harassment, doxing, \"exploiting ... minors\", giving medical advice, automatically creating legal obligations, producing legal evidence, and \"discriminating against or harming individuals or groups based on ... social behavior or ... personal or personality characteristics ... [or] legally protected characteristics or categories\".[81][82] The user owns the rights to their generated output images, and is free to use them commercially.[83]\n\n## See also[edit]\n- Artificial intelligence art\n- Midjourney\n- Craiyon\n- Hugging Face\n- Imagen (Google Brain)\n## References[edit]\n- ^ \"Announcing SDXL 1.0\". stability.ai. Archived from the original on July 26, 2023.\n\n- ^ Ryan O'Connor (August 23, 2022). \"How to Run Stable Diffusion Locally to Generate Images\". Archived from the original on October 13, 2023. Retrieved May 4, 2023.\n\n- ^ \"Diffuse The Rest - a Hugging Face Space by huggingface\". huggingface.co. Archived from the original on September 5, 2022. Retrieved September 5, 2022.\n\n- ^ \"Leaked deck raises questions over Stability AI's Series A pitch to investors\". sifted.eu. Archived from the original on June 29, 2023. Retrieved June 20, 2023.\n\n- ^ \"Revolutionizing image generation by AI: Turning text into images\". www.lmu.de. Archived from the original on September 17, 2022. Retrieved June 21, 2023.\n\n- ^ Mostaque, Emad (November 2, 2022). \"Stable Diffusion came from the Machine Vision & Learning research group (CompVis) @LMU_Muenchen\". Twitter. Archived from the original on July 20, 2023. Retrieved June 22, 2023.\n\n- ^ a b c d \"Stable Diffusion Launch Announcement\". Stability.Ai. Archived from the original on September 5, 2022. Retrieved September 6, 2022.\n\n- ^ a b c d e f g h i \"Stable Diffusion Repository on GitHub\". CompVis - Machine Vision and Learning Research Group, LMU Munich. September 17, 2022. Archived from the original on January 18, 2023. Retrieved September 17, 2022.\n\n- ^ \"The new killer app: Creating AI art will absolutely crush your PC\". PCWorld. Archived from the original on August 31, 2022. Retrieved August 31, 2022.\n\n- ^ a b c d e Vincent, James (September 15, 2022). \"Anyone can use this AI art generator — that's the risk\". The Verge. Archived from the original on January 21, 2023. Retrieved September 30, 2022.\n\n- ^ \"CompVis/Latent-diffusion\". GitHub.\n\n- ^ \"Stable Diffusion 3: Research Paper\".\n\n- ^ a b c Rombach; Blattmann; Lorenz; Esser; Ommer (June 2022). High-Resolution Image Synthesis with Latent Diffusion Models (PDF). International Conference on Computer Vision and Pattern Recognition (CVPR). New Orleans, LA. pp. 10684–10695. arXiv:2112.10752. Archived (PDF) from the original on January 20, 2023. Retrieved September 17, 2022.\n\n- ^ a b c d Alammar, Jay. \"The Illustrated Stable Diffusion\". jalammar.github.io. Archived from the original on November 1, 2022. Retrieved October 31, 2022.\n\n- ^ David, Foster. \"8. Diffusion Models\". Generative Deep Learning (2 ed.). O'Reilly.\n\n- ^ Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, Surya Ganguli (March 12, 2015). \"Deep Unsupervised Learning using Nonequilibrium Thermodynamics\". Arxiv. arXiv:1503.03585.{{cite journal}}:  CS1 maint: multiple names: authors list (link)\n\n- ^ \"Stable diffusion pipelines\". huggingface.co. Archived from the original on June 25, 2023. Retrieved June 22, 2023.\n\n- ^ \"Text-to-Image Generation with Stable Diffusion and OpenVINO™\". openvino.ai. Intel. Retrieved February 10, 2024.\n\n- ^ a b c Podell, Dustin; English, Zion; Lacey, Kyle; Blattmann, Andreas; Dockhorn, Tim; Müller, Jonas; Penna, Joe; Rombach, Robin (July 4, 2023). \"SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis\". arXiv:2307.01952 [cs.CV].\n\n- ^ a b c Esser, Patrick; Kulal, Sumith; Blattmann, Andreas; Entezari, Rahim; Müller, Jonas; Saini, Harry; Levi, Yam; Lorenz, Dominik; Sauer, Axel (March 5, 2024), Scaling Rectified Flow Transformers for High-Resolution Image Synthesis, arXiv:2403.03206\n\n- ^ a b Liu, Xingchao; Gong, Chengyue; Liu, Qiang (September 7, 2022), Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow, arXiv:2209.03003\n\n- ^ a b \"Rectified Flow — Rectified Flow\". www.cs.utexas.edu. Retrieved March 6, 2024.\n\n- ^ a b c d e Baio, Andy (August 30, 2022). \"Exploring 12 Million of the 2.3 Billion Images Used to Train Stable Diffusion's Image Generator\". Waxy.org. Archived from the original on January 20, 2023. Retrieved November 2, 2022.\n\n- ^ \"This artist is dominating AI-generated art. And he's not happy about it\". MIT Technology Review. Archived from the original on January 14, 2023. Retrieved November 2, 2022.\n\n- ^ a b Brunner, Katharina; Harlan, Elisa (July 7, 2023). \"We Are All Raw Material for AI\". Bayerischer Rundfunk (BR). Archived from the original on September 12, 2023. Retrieved September 12, 2023.\n\n- ^ Schuhmann, Christoph (November 2, 2022), CLIP+MLP Aesthetic Score Predictor, archived from the original on June 8, 2023, retrieved November 2, 2022\n\n- ^ \"LAION-Aesthetics | LAION\". laion.ai. Archived from the original on August 26, 2022. Retrieved September 2, 2022.\n\n- ^ a b c Ho, Jonathan; Salimans, Tim (July 25, 2022). \"Classifier-Free Diffusion Guidance\". arXiv:2207.12598 [cs.LG].\n\n- ^ Mostaque, Emad (August 28, 2022). \"Cost of construction\". Twitter. Archived from the original on September 6, 2022. Retrieved September 6, 2022.\n\n- ^ a b c \"CompVis/stable-diffusion-v1-4 · Hugging Face\". huggingface.co. Archived from the original on January 11, 2023. Retrieved November 2, 2022.\n\n- ^ Wiggers, Kyle (August 12, 2022). \"A startup wants to democratize the tech behind DALL-E 2, consequences be damned\". TechCrunch. Archived from the original on January 19, 2023. Retrieved November 2, 2022.\n\n- ^ emad_9608 (April 19, 2024). \"10m is about right\". r/StableDiffusion. Retrieved April 25, 2024.{{cite web}}:  CS1 maint: numeric names: authors list (link)\n\n- ^ a b c d e \"Stable Diffusion with 🧨 Diffusers\". huggingface.co. Archived from the original on January 17, 2023. Retrieved October 31, 2022.\n\n- ^ a b c \"Stable Diffusion 2.0 Release\". stability.ai. Archived from the original on December 10, 2022.\n\n- ^ \"LAION\". laion.ai. Archived from the original on October 16, 2023. Retrieved October 31, 2022.\n\n- ^ \"Generating images with Stable Diffusion\". Paperspace Blog. August 24, 2022. Archived from the original on October 31, 2022. Retrieved October 31, 2022.\n\n- ^ \"Announcing SDXL 1.0\". Stability AI. Archived from the original on July 26, 2023. Retrieved August 21, 2023.\n\n- ^ Edwards, Benj (July 27, 2023). \"Stability AI releases Stable Diffusion XL, its next-gen image synthesis model\". Ars Technica. Archived from the original on August 21, 2023. Retrieved August 21, 2023.\n\n- ^ \"hakurei/waifu-diffusion · Hugging Face\". huggingface.co. Archived from the original on October 8, 2023. Retrieved October 31, 2022.\n\n- ^ Chambon, Pierre; Bluethgen, Christian; Langlotz, Curtis P.; Chaudhari, Akshay (October 9, 2022). \"Adapting Pretrained Vision-Language Foundational Models to Medical Imaging Domains\". arXiv:2210.04133 [cs.CV].\n\n- ^ Seth Forsgren; Hayk Martiros. \"Riffusion - Stable diffusion for real-time music generation\". Riffusion. Archived from the original on December 16, 2022.\n\n- ^ Mercurio, Anthony (October 31, 2022), Waifu Diffusion, archived from the original on October 31, 2022, retrieved October 31, 2022\n\n- ^ Smith, Ryan. \"NVIDIA Quietly Launches GeForce RTX 3080 12GB: More VRAM, More Power, More Money\". www.anandtech.com. Archived from the original on August 27, 2023. Retrieved October 31, 2022.\n\n- ^ Dave James (October 28, 2022). \"I thrashed the RTX 4090 for 8 hours straight training Stable Diffusion to paint like my uncle Hermann\". PC Gamer. Archived from the original on November 9, 2022.\n\n- ^ Gal, Rinon; Alaluf, Yuval; Atzmon, Yuval; Patashnik, Or; Bermano, Amit H.; Chechik, Gal; Cohen-Or, Daniel (August 2, 2022). \"An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion\". arXiv:2208.01618 [cs.CV].\n\n- ^ \"NovelAI Improvements on Stable Diffusion\". NovelAI. October 11, 2022. Archived from the original on October 27, 2022.\n\n- ^ Yuki Yamashita (September 1, 2022). \"愛犬の合成画像を生成できるAI 文章で指示するだけでコスプレ 米Googleが開発\". ITmedia Inc. (in Japanese). Archived from the original on August 31, 2022.\n\n- ^ Meng, Chenlin; He, Yutong; Song, Yang; Song, Jiaming; Wu, Jiajun; Zhu, Jun-Yan; Ermon, Stefano (August 2, 2021). \"SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations\". arXiv:2108.01073 [cs.CV].\n\n- ^ a b c d \"Stable Diffusion web UI\". GitHub. November 10, 2022. Archived from the original on January 20, 2023. Retrieved September 27, 2022.\n\n- ^ invisible-watermark, Shield Mountain, November 2, 2022, archived from the original on October 18, 2022, retrieved November 2, 2022\n\n- ^ \"stable-diffusion-tools/emphasis at master · JohannesGaessler/stable-diffusion-tools\". GitHub. Archived from the original on October 2, 2022. Retrieved November 2, 2022.\n\n- ^ \"Stable Diffusion v2.1 and DreamStudio Updates 7-Dec 22\". stability.ai. Archived from the original on December 10, 2022.\n\n- ^ a b Luzi, Lorenzo; Siahkoohi, Ali; Mayer, Paul M.; Casco-Rodriguez, Josue; Baraniuk, Richard (October 21, 2022). \"Boomerang: Local sampling on image manifolds using diffusion models\". arXiv:2210.12100 [cs.CV].\n\n- ^ Bühlmann, Matthias (September 28, 2022). \"Stable Diffusion Based Image Compression\". Medium. Archived from the original on November 2, 2022. Retrieved November 2, 2022.\n\n- ^ Zhang, Lvmin (February 10, 2023). \"Adding Conditional Control to Text-to-Image Diffusion Models\". arXiv:2302.05543 [cs.CV].\n\n- ^ \"CompVis/stable-diffusion-v1-4 · Hugging Face\". huggingface.co. Archived from the original on January 11, 2023. Retrieved August 17, 2023.\n\n- ^ \"CompVis (CompVis)\". huggingface.co. August 23, 2023. Retrieved March 6, 2024.\n\n- ^ \"runwayml/stable-diffusion-v1-5 · Hugging Face\". huggingface.co. Archived from the original on September 21, 2023. Retrieved August 17, 2023.\n\n- ^ a b \"stabilityai/stable-diffusion-2 · Hugging Face\". huggingface.co. Archived from the original on September 21, 2023. Retrieved August 17, 2023.\n\n- ^ \"stabilityai/stable-diffusion-2-base · Hugging Face\". huggingface.co. Retrieved January 1, 2024.\n\n- ^ \"stabilityai/stable-diffusion-2-1 · Hugging Face\". huggingface.co. Archived from the original on September 21, 2023. Retrieved August 17, 2023.\n\n- ^ \"stabilityai/stable-diffusion-xl-base-1.0 · Hugging Face\". huggingface.co. Archived from the original on October 8, 2023. Retrieved August 17, 2023.\n\n- ^ \"Announcing SDXL 1.0\". Stability AI. Retrieved January 1, 2024.\n\n- ^ \"stabilityai/sdxl-turbo · Hugging Face\". huggingface.co. Retrieved January 1, 2024.\n\n- ^ \"Adversarial Diffusion Distillation\". Stability AI. Retrieved January 1, 2024.\n\n- ^ \"Stable Diffusion 3\". Stability AI. Retrieved March 5, 2024.\n\n- ^ Radford, Alec; Kim, Jong Wook; Hallacy, Chris; Ramesh, Aditya; Goh, Gabriel; Agarwal, Sandhini; Sastry, Girish; Askell, Amanda; Mishkin, Pamela (February 26, 2021). \"Learning Transferable Visual Models From Natural Language Supervision\". arXiv:2103.00020 [cs.CV].\n\n- ^ Meng, Chenlin; He, Yutong; Song, Yang; Song, Jiaming; Wu, Jiajun; Zhu, Jun-Yan; Ermon, Stefano (January 4, 2022). \"SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations\". arXiv:2108.01073 [cs.CV].\n\n- ^ Rombach, Robin; Blattmann, Andreas; Lorenz, Dominik; Esser, Patrick; Ommer, Björn (2022). \"High-Resolution Image Synthesis With Latent Diffusion Models\". Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 10684–10695. arXiv:2112.10752.\n\n- ^ \"LICENSE.md · stabilityai/stable-diffusion-xl-base-1.0 at main\". huggingface.co. July 26, 2023. Retrieved January 1, 2024.\n\n- ^ Heikkilä, Melissa (September 16, 2022). \"This artist is dominating AI-generated art. And he's not happy about it\". MIT Technology Review. Archived from the original on January 14, 2023. Retrieved September 26, 2022.\n\n- ^ a b Ryo Shimizu (August 26, 2022). \"Midjourneyを超えた？ 無料の作画AI｢ #StableDiffusion ｣が｢AIを民主化した｣と断言できる理由\". Business Insider Japan (in Japanese). Archived from the original on December 10, 2022. Retrieved October 4, 2022.\n\n- ^ Cai, Kenrick. \"Startup Behind AI Image Generator Stable Diffusion Is In Talks To Raise At A Valuation Up To $1 Billion\". Forbes. Archived from the original on September 30, 2023. Retrieved October 31, 2022.\n\n- ^ \"Illegal trade in AI child sex abuse images exposed\". BBC News. June 27, 2023. Archived from the original on September 21, 2023. Retrieved September 26, 2023.\n\n- ^ Maiberg, Emanuel (June 11, 2024). \"Hackers Target AI Users With Malicious Stable Diffusion Tool on GitHub to Protest 'Art Theft'\". 404 Media. Retrieved June 14, 2024.\n\n- ^ Vincent, James (January 16, 2023). \"AI art tools Stable Diffusion and Midjourney targeted with copyright lawsuit\". The Verge. Archived from the original on March 9, 2023. Retrieved January 16, 2023.\n\n- ^ Korn, Jennifer (January 17, 2023). \"Getty Images suing the makers of popular AI art tool for allegedly stealing photos\". CNN. Archived from the original on March 1, 2023. Retrieved January 22, 2023.\n\n- ^ Brittain, Blake (July 19, 2023). \"US judge finds flaws in artists' lawsuit against AI companies\". Reuters. Archived from the original on September 6, 2023. Retrieved August 6, 2023.\n\n- ^ \"Stable Diffusion Public Release\". Stability.Ai. Archived from the original on August 30, 2022. Retrieved August 31, 2022.\n\n- ^ \"From RAIL to Open RAIL: Topologies of RAIL Licenses\". Responsible AI Licenses (RAIL). August 18, 2022. Archived from the original on July 27, 2023. Retrieved February 20, 2023.\n\n- ^ \"Ready or not, mass video deepfakes are coming\". The Washington Post. August 30, 2022. Archived from the original on August 31, 2022. Retrieved August 31, 2022.\n\n- ^ \"License - a Hugging Face Space by CompVis\". huggingface.co. Archived from the original on September 4, 2022. Retrieved September 5, 2022.\n\n- ^ Katsuo Ishida (August 26, 2022). \"言葉で指示した画像を凄いAIが描き出す「Stable Diffusion」 ～画像は商用利用も可能\". Impress Corporation (in Japanese). Archived from the original on November 14, 2022. Retrieved October 4, 2022.\n\n## External links[edit]\n- Stable Diffusion Demo\n- Interactive Explanation of Stable Diffusion\n- \"We Are All Raw Material for AI\": Investigation on sensitive and private data in Stable Diffusions training data\n- \"Negative Prompts in Stable Diffusion\"\n- v\n- t\n- e\n- Differentiable programming\n- Information geometry\n- Statistical manifold\n- Automatic differentiation\n- Neuromorphic engineering\n- Pattern recognition\n- Tensor calculus\n- Computational learning theory\n- Inductive bias\n- Gradient descent\nSGD\n- SGD\n- Clustering\n- Regression\nOverfitting\n- Overfitting\n- Hallucination\n- Adversary\n- Attention\n- Convolution\n- Loss functions\n- Backpropagation\n- Batchnorm\n- Activation\nSoftmax\nSigmoid\nRectifier\n- Softmax\n- Sigmoid\n- Rectifier\n- Regularization\n- Datasets\nAugmentation\n- Augmentation\n- Diffusion\n- Autoregression\n- Machine learning\nIn-context learning\n- In-context learning\n- Artificial neural network\nDeep learning\n- Deep learning\n- Scientific computing\n- Artificial Intelligence\n- Language model\nLarge language model\n- Large language model\n- IPU\n- TPU\n- VPU\n- Memristor\n- SpiNNaker\n- TensorFlow\n- PyTorch\n- Keras\n- Theano\n- JAX\n- Flux.jl\n- MindSpore\n- AlexNet\n- WaveNet\n- Human image synthesis\n- HWR\n- OCR\n- Speech synthesis\n- Speech recognition\n- Facial recognition\n- AlphaFold\n- Text-to-image models\nDALL-E\nMidjourney\nStable Diffusion\n- DALL-E\n- Midjourney\n- Stable Diffusion\n- Text-to-video models\nSora\nVideoPoet\n- Sora\n- VideoPoet\n- Whisper\n- Word2vec\n- Seq2seq\n- BERT\n- Gemini\n- LaMDA\nBard\n- Bard\n- NMT\n- Project Debater\n- IBM Watson\n- IBM Watsonx\n- Granite\n- GPT-1\n- GPT-2\n- GPT-3\n- GPT-4\n- ChatGPT\n- GPT-J\n- Chinchilla AI\n- PaLM\n- BLOOM\n- LLaMA\n- PanGu-Σ\n- AlphaGo\n- AlphaZero\n- Q-learning\n- SARSA\n- OpenAI Five\n- Self-driving car\n- MuZero\n- Action selection\nAuto-GPT\n- Auto-GPT\n- Robot control\n- Yoshua Bengio\n- Alex Graves\n- Ian Goodfellow\n- Stephen Grossberg\n- Demis Hassabis\n- Geoffrey Hinton\n- Yann LeCun\n- Fei-Fei Li\n- Andrew Ng\n- Jürgen Schmidhuber\n- David Silver\n- Ilya Sutskever\n- Anthropic\n- EleutherAI\n- Google DeepMind\n- Hugging Face\n- OpenAI\n- Meta AI\n- Mila\n- MIT CSAIL\n- Huawei\n- Neural Turing machine\n- Differentiable neural computer\n- Transformer\n- Recurrent neural network (RNN)\n- Long short-term memory (LSTM)\n- Gated recurrent unit (GRU)\n- Echo state network\n- Multilayer perceptron (MLP)\n- Convolutional neural network\n- Residual neural network\n- Mamba\n- Autoencoder\n- Variational autoencoder (VAE)\n- Generative adversarial network (GAN)\n- Graph neural network\n-  Portals\nComputer programming\nTechnology\n- Computer programming\n- Technology\n-  Categories\nArtificial neural networks\nMachine learning\n- Artificial neural networks\n- Machine learning\n- Artificial intelligence art\n- Deep learning software applications\n- Text-to-image generation\n- Unsupervised learning\n- Art controversies\n- Works involved in plagiarism controversies\n- 2022 software\n- CS1 maint: multiple names: authors list\n- CS1 maint: numeric names: authors list\n- CS1 Japanese-language sources (ja)\n- Articles with short description\n- Short description matches Wikidata\n- Use mdy dates from October 2023\n- All articles with unsourced statements\n- Articles with unsourced statements from October 2023\n- Pages using multiple image with auto scaled images\n- Commons category link from Wikidata\n-  This page was last edited on 16 June 2024, at 11:38 (UTC).\n- Text is available under the Creative Commons Attribution-ShareAlike License 4.0;\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.\n- Privacy policy\n- About Wikipedia\n- Disclaimers\n- Contact Wikipedia\n- Code of Conduct\n- Developers\n- Statistics\n- Cookie statement\n- Mobile view\n- \n- \n-\n"
      ]
    },
    {
      "id": 28,
      "type": "ShowText|pysssss",
      "pos": [
        4100,
        1280
      ],
      "size": {
        "0": 900,
        "1": 1000
      },
      "flags": {},
      "order": 8,
      "mode": 0,
      "inputs": [
        {
          "name": "text",
          "type": "STRING",
          "link": 30,
          "widget": {
            "name": "text"
          }
        }
      ],
      "outputs": [
        {
          "name": "STRING",
          "type": "STRING",
          "links": null,
          "shape": 6
        }
      ],
      "properties": {
        "Node name for S&R": "ShowText|pysssss"
      },
      "widgets_values": [
        "",
        "Summarize the content below\n\n\n - Main page\n- Contents\n- Current events\n- Random article\n- About Wikipedia\n- Contact us\n- Donate\n- Help\n- Learn to edit\n- Community portal\n- Recent changes\n- Upload file\n- Create account\n\n- Log in\n\n-  Create account\n-  Log in\n- Contributions\n- Talk\n## Contents\n- \n\n(Top)\n\n\n- \n\n\n1\nDevelopment\n\n\n\n\n\n- \n\n\n2\nTechnology\n\n\n\n\nToggle Technology subsection\n\n\n\n\n\n2.1\nArchitecture\n\n\n\n\n\n\n2.1.1\nSD XL\n\n\n\n\n\n\n\n\n2.1.2\nSD 3.0\n\n\n\n\n\n\n\n\n\n\n2.2\nTraining data\n\n\n\n\n\n\n\n\n2.3\nTraining procedures\n\n\n\n\n\n\n\n\n2.4\nLimitations\n\n\n\n\n\n\n\n\n2.5\nEnd-user fine-tuning\n\n\n\n\n\n\n\n- \n\n\n2.1\nArchitecture\n\n\n\n\n\n\n2.1.1\nSD XL\n\n\n\n\n\n\n\n\n2.1.2\nSD 3.0\n\n\n\n\n\n\n\n- \n\n\n2.1.1\nSD XL\n\n\n\n\n\n- \n\n\n2.1.2\nSD 3.0\n\n\n\n\n\n- \n\n\n2.2\nTraining data\n\n\n\n\n\n- \n\n\n2.3\nTraining procedures\n\n\n\n\n\n- \n\n\n2.4\nLimitations\n\n\n\n\n\n- \n\n\n2.5\nEnd-user fine-tuning\n\n\n\n\n\n- \n\n\n3\nCapabilities\n\n\n\n\nToggle Capabilities subsection\n\n\n\n\n\n3.1\nText to image generation\n\n\n\n\n\n\n\n\n3.2\nImage modification\n\n\n\n\n\n\n\n\n3.3\nControlNet\n\n\n\n\n\n\n\n- \n\n\n3.1\nText to image generation\n\n\n\n\n\n- \n\n\n3.2\nImage modification\n\n\n\n\n\n- \n\n\n3.3\nControlNet\n\n\n\n\n\n- \n\n\n4\nReleases\n\n\n\n\n\n- \n\n\n5\nUsage and controversy\n\n\n\n\nToggle Usage and controversy subsection\n\n\n\n\n\n5.1\nComfyUI extension compromise\n\n\n\n\n\n\n\n- \n\n\n5.1\nComfyUI extension compromise\n\n\n\n\n\n- \n\n\n6\nLitigation\n\n\n\n\n\n- \n\n\n7\nLicense\n\n\n\n\n\n- \n\n\n8\nSee also\n\n\n\n\n\n- \n\n\n9\nReferences\n\n\n\n\n\n- \n\n\n10\nExternal links\n\n\n\n\n\n# Stable Diffusion\n- Azərbaycanca\n- Български\n- Català\n- Čeština\n- Deutsch\n- Español\n- فارسی\n- Français\n- Galego\n- 한국어\n- Bahasa Indonesia\n- Italiano\n- עברית\n- Nederlands\n- 日本語\n- Português\n- Runa Simi\n- Русский\n- සිංහල\n- Suomi\n- Türkçe\n- Українська\n- Tiếng Việt\n- 粵語\n- 中文\n- Article\n- Talk\n- Read\n- Edit\n- View history\n- Read\n- Edit\n- View history\n- What links here\n- Related changes\n- Upload file\n- Special pages\n- Permanent link\n- Page information\n- Cite this page\n- Get shortened URL\n- Download QR code\n- Wikidata item\n- Download as PDF\n- Printable version\n- Wikimedia Commons\n\n\n- github.com/Stability-AI/generative-models \nStable Diffusion is a deep learning, text-to-image model released in 2022 based on diffusion techniques. The generative artificial intelligence technology is the premier product of Stability AI and is considered to be a part of the ongoing artificial intelligence boom.\n\nIt is primarily used to generate detailed images conditioned on text descriptions, though it can also be applied to other tasks such as inpainting, outpainting, and generating image-to-image translations guided by a text prompt.[3] Its development involved  researchers from the CompVis Group at Ludwig Maximilian University of Munich and Runway with a computational donation from Stability and training data from non-profit organizations.[4][5][6][7]\n\nStable Diffusion is a latent diffusion model, a kind of deep generative artificial neural network. Its code and model weights have been released publicly,[8] and it can run on most consumer hardware equipped with a modest GPU with at least 4 GB VRAM. This marked a departure from previous proprietary text-to-image models such as DALL-E and Midjourney which were accessible only via cloud services.[9][10]\n\n## Development[edit]\nStable Diffusion, originated from a project called Latent Diffusion,[11] developed in Germany by researchers at Ludwig Maximilian University in Munich and Heidelberg University. Four of the original 5 authors (Robin Rombach, Andreas Blattmann, Patrick Esser and Dominik Lorenz) later joined Stability AI and released subsequent versions of Stable Diffusion.[12]\n\nThe technical license for the model was released by the CompVis group at Ludwig Maximilian University of Munich.[10] Development was led by Patrick Esser of Runway and Robin Rombach of CompVis, who were among the researchers who had earlier invented the latent diffusion model architecture used by Stable Diffusion.[7] Stability AI also credited EleutherAI and LAION (a German nonprofit which assembled the dataset on which Stable Diffusion was trained) as supporters of the project.[7]\n\n## Technology[edit]\n### Architecture[edit]\nStable Diffusion uses a kind of diffusion model (DM), called a latent diffusion model (LDM), developed by the CompVis group at LMU Munich.[13][8] Introduced in 2015, diffusion models are trained with the objective of removing successive applications of Gaussian noise on training images, which can be thought of as a sequence of denoising autoencoders. Stable Diffusion consists of 3 parts: the variational autoencoder (VAE), U-Net, and an optional text encoder.[14] The VAE encoder compresses the image from pixel space to a smaller dimensional latent space, capturing a more fundamental semantic meaning of the image.[13] Gaussian noise is iteratively applied to the compressed latent representation during forward diffusion.[14] The U-Net block, composed of a ResNet backbone, denoises the output from forward diffusion backwards to obtain a latent representation. Finally, the VAE decoder generates the final image by converting the representation back into pixel space.[14]\n\nThe denoising step can be flexibly conditioned on a string of text, an image, or another modality. The encoded conditioning data is exposed to denoising U-Nets via a cross-attention mechanism.[14] For conditioning on text, the fixed, pretrained CLIP ViT-L/14 text encoder is used to transform text prompts to an embedding space.[8] Researchers point to increased computational efficiency for training and generation as an advantage of LDMs.[7][13]\n\nThe name diffusion takes inspiration from the thermodynamic diffusion and an important link was made between this purely physical field and deep learning in 2015.[15][16]\n\nWith 860 million parameters in the U-Net and 123 million in the text encoder, Stable Diffusion is considered relatively lightweight by 2022 standards, and unlike other diffusion models, it can run on consumer GPUs,[17] and even CPU-only if using the OpenVINO version of Stable Diffusion.[18]\n\nThe XL version uses the same architecture,[19] except larger: larger UNet backbone, larger cross-attention context, two text encoders instead of one, and trained on multiple aspect ratios (not just the square aspect ratio like previous versions).\n\nThe SD XL Refiner, released at the same time, has the same architecture as SD XL, but it was trained for adding fine details to preexisting images via text-conditional img2img.\n\nThe 3.0 version[20] completely changes the backbone. Not a UNet, but a Rectified Flow Transformer, which implements the rectified flow method[21][22] with a Transformer.\n\nThe Transformer architecture used for SD 3.0 has three \"tracks\", for original text encoding, transformed text encoding, and image encoding (in latent space). The transformed text encoding and image encoding are mixed during each transformer block.\n\nThe architecture is named \"multimodal diffusion transformer (MMDiT), where the \"multimodal\" means that it mixes text and image encodings inside its operations. This differs from previous versions of DiT, where the text encoding affects the image encoding, but not vice versa.\n\n### Training data[edit]\nStable Diffusion was trained on pairs of images and captions taken from LAION-5B, a publicly available dataset derived from Common Crawl data scraped from the web, where 5 billion image-text pairs were classified based on language and filtered into separate datasets by resolution, a predicted likelihood of containing a watermark, and predicted \"aesthetic\" score (e.g. subjective visual quality).[23] The dataset was created by LAION, a German non-profit which receives funding from Stability AI.[23][24] The Stable Diffusion model was trained on three subsets of LAION-5B: laion2B-en, laion-high-resolution, and laion-aesthetics v2 5+.[23] A third-party analysis of the model's training data identified that out of a smaller subset of 12 million images taken from the original wider dataset used, approximately 47% of the sample size of images came from 100 different domains, with Pinterest taking up 8.5% of the subset, followed by websites such as WordPress, Blogspot, Flickr, DeviantArt and Wikimedia Commons.[citation needed]  An investigation by Bayerischer Rundfunk showed that LAION's datasets, hosted on Hugging Face, contain large amounts of private and sensitive data.[25]\n\n### Training procedures[edit]\nThe model was initially trained on the laion2B-en and laion-high-resolution subsets, with the last few rounds of training done on LAION-Aesthetics v2 5+, a subset of 600 million captioned images which the LAION-Aesthetics Predictor V2 predicted that humans would, on average, give a score of at least 5 out of 10 when asked to rate how much they liked them.[26][23][27] The LAION-Aesthetics v2 5+ subset also excluded low-resolution images and images which LAION-5B-WatermarkDetection identified as carrying a watermark with greater than 80% probability.[23] Final rounds of training additionally dropped 10% of text conditioning to improve Classifier-Free Diffusion Guidance.[28]\n\nThe model was trained using 256 Nvidia A100 GPUs on Amazon Web Services for a total of 150,000 GPU-hours, at a cost of $600,000.[29][30][31]\n\nSD3 was trained at a cost of around $10 million.[32]\n\n### Limitations[edit]\nStable Diffusion has issues with degradation and inaccuracies in certain scenarios. Initial releases of the model were trained on a dataset that consists of 512×512 resolution images, meaning that the quality of generated images noticeably degrades when user specifications deviate from its \"expected\" 512×512 resolution;[33] the version 2.0 update of the Stable Diffusion model later introduced the ability to natively generate images at 768×768 resolution.[34] Another challenge is in generating human limbs due to poor data quality of limbs in the LAION database.[35] The model is insufficiently trained to understand human limbs and faces due to the lack of representative features in the database, and prompting the model to generate images of such type can confound the model.[36] Stable Diffusion XL (SDXL) version 1.0, released in July 2023, introduced native 1024x1024 resolution and improved generation for limbs and text.[37][38]\n\nAccessibility for individual developers can also be a problem. In order to customize the model for new use cases that are not included in the dataset, such as generating anime characters (\"waifu diffusion\"),[39] new data and further training are required. Fine-tuned adaptations of Stable Diffusion created through additional retraining have been used for a variety of different use-cases, from medical imaging[40] to algorithmically generated music.[41] However, this fine-tuning process is sensitive to the quality of new data; low resolution images or different resolutions from the original data can not only fail to learn the new task but degrade the overall performance of the model. Even when the model is additionally trained on high quality images, it is difficult for individuals to run models in consumer electronics. For example, the training process for waifu-diffusion requires a minimum 30 GB of VRAM,[42] which exceeds the usual resource provided in such consumer GPUs as Nvidia's GeForce 30 series, which has only about 12 GB.[43]\n\nThe creators of Stable Diffusion acknowledge the potential for algorithmic bias, as the model was primarily trained on images with English descriptions.[30] As a result, generated images reinforce social biases and are from a western perspective, as the creators note that the model lacks data from other communities and cultures. The model gives more accurate results for prompts that are written in English in comparison to those written in other languages, with western or white cultures often being the default representation.[30]\n\n### End-user fine-tuning[edit]\nTo address the limitations of the model's initial training, end-users may opt to implement additional training to fine-tune generation outputs to match more specific use-cases, a process also referred to as personalization. There are three methods in which user-accessible fine-tuning can be applied to a Stable Diffusion model checkpoint:\n\n- An \"embedding\" can be trained from a collection of user-provided images, and allows the model to generate visually similar images whenever the name of the embedding is used within a generation prompt.[44] Embeddings are based on the \"textual inversion\" concept developed by researchers from Tel Aviv University in 2022 with support from Nvidia, where vector representations for specific tokens used by the model's text encoder are linked to new pseudo-words. Embeddings can be used to reduce biases within the original model, or mimic visual styles.[45]\n- A \"hypernetwork\" is a small pretrained neural network that is applied to various points within a larger neural network, and refers to the technique created by NovelAI developer Kurumuz in 2021, originally intended for text-generation transformer models. Hypernetworks steer results towards a particular direction, allowing Stable Diffusion-based models to imitate the art style of specific artists, even if the artist is not recognised by the original model; they process the image by finding key areas of importance such as hair and eyes, and then patch these areas in secondary latent space.[46]\n- DreamBooth is a deep learning generation model developed by researchers from Google Research and Boston University in 2022 which can fine-tune the model to generate precise, personalised outputs that depict a specific subject, following training via a set of images which depict the subject.[47]\n## Capabilities[edit]\nThe Stable Diffusion model supports the ability to generate new images from scratch through the use of a text prompt describing elements to be included or omitted from the output.[8] Existing images can be re-drawn by the model to incorporate new elements described by a text prompt (a process known as \"guided image synthesis\"[48]) through its diffusion-denoising mechanism.[8] In addition, the model also allows the use of prompts to partially alter existing images via inpainting and outpainting, when used with an appropriate user interface that supports such features, of which numerous different open source implementations exist.[49]\n\nStable Diffusion is recommended to be run with 10 GB or more VRAM, however users with less VRAM may opt to load the weights in float16 precision instead of the default float32 to tradeoff model performance with lower VRAM usage.[33]\n\n### Text to image generation[edit]\n- Top: no negative prompt\n- Centre: \"green trees\"\n- Bottom: \"round stones, round rocks\"\nThe text to image sampling script within Stable Diffusion, known as \"txt2img\", consumes a text prompt in addition to assorted option parameters covering sampling types, output image dimensions, and seed values. The script outputs an image file based on the model's interpretation of the prompt.[8] Generated images are tagged with an invisible digital watermark to allow users to identify an image as generated by Stable Diffusion,[8] although this watermark loses its efficacy if the image is resized or rotated.[50]\n\nEach txt2img generation will involve a specific seed value which affects the output image. Users may opt to randomize the seed in order to explore different generated outputs, or use the same seed to obtain the same image output as a previously generated image.[33] Users are also able to adjust the number of inference steps for the sampler; a higher value takes a longer duration of time, however a smaller value may result in visual defects.[33] Another configurable option, the classifier-free guidance scale value, allows the user to adjust how closely the output image adheres to the prompt.[28] More experimentative use cases may opt for a lower scale value, while use cases aiming for more specific outputs may use a higher value.[33]\n\nAdditional text2img features are provided by front-end implementations of Stable Diffusion, which allow users to modify the weight given to specific parts of the text prompt. Emphasis markers allow users to add or reduce emphasis to keywords by enclosing them with brackets.[51] An alternative method of adjusting weight to parts of the prompt are \"negative prompts\". Negative prompts are a feature included in some front-end implementations, including Stability AI's own DreamStudio cloud service, and allow the user to specify prompts which the model should avoid during image generation. The specified prompts may be undesirable image features that would otherwise be present within image outputs due to the positive prompts provided by the user, or due to how the model was originally trained, with mangled human hands being a common example.[49][52]\n\n### Image modification[edit]\n- Left: Original image created with Stable Diffusion 1.5\n- Right: Modified image created with Stable Diffusion XL 1.0\nStable Diffusion also includes another sampling script, \"img2img\", which consumes a text prompt, path to an existing image, and strength value between 0.0 and 1.0. The script outputs a new image based on the original image that also features elements provided within the text prompt. The strength value denotes the amount of noise added to the output image. A higher strength value produces more variation within the image but may produce an image that is not semantically consistent with the prompt provided.[8]\n\nThe ability of img2img to add noise to the original image makes it potentially useful for data anonymization and data augmentation, in which the visual features of image data are changed and anonymized.[53] The same process may also be useful for image upscaling, in which the resolution of an image is increased, with more detail potentially being added to the image.[53] Additionally, Stable Diffusion has been experimented with as a tool for image compression. Compared to JPEG and WebP, the recent methods used for image compression in Stable Diffusion face limitations in preserving small text and faces.[54]\n\nAdditional use-cases for image modification via img2img are offered by numerous front-end implementations of the Stable Diffusion model. Inpainting involves selectively modifying a portion of an existing image delineated by a user-provided layer mask, which fills the masked space with newly generated content based on the provided prompt.[49] A dedicated model specifically fine-tuned for inpainting use-cases was created by Stability AI alongside the release of Stable Diffusion 2.0.[34] Conversely, outpainting extends an image beyond its original dimensions, filling the previously empty space with content generated based on the provided prompt.[49]\n\nA depth-guided model, named \"depth2img\", was introduced with the release of Stable Diffusion 2.0 on November 24, 2022; this model infers the depth of the provided input image, and generates a new output image based on both the text prompt and the depth information, which allows the coherence and depth of the original input image to be maintained in the generated output.[34]\n\n### ControlNet[edit]\nControlNet[55] is a neural network architecture designed to manage diffusion models by incorporating additional conditions. It duplicates the weights of neural network blocks into a \"locked\" copy and a \"trainable\" copy. The \"trainable\" copy learns the desired condition, while the \"locked\" copy preserves the original model. This approach ensures that training with small datasets of image pairs does not compromise the integrity of production-ready diffusion models. The \"zero convolution\" is a 1×1 convolution with both weight and bias initialized to zero. Before training, all zero convolutions produce zero output, preventing any distortion caused by ControlNet. No layer is trained from scratch; the process is still fine-tuning, keeping the original model secure. This method enables training on small-scale or even personal devices.\n\n## Releases[edit]\naround 3.5x larger than previous versions.[63]\n\nKey papers\n\n- Learning Transferable Visual Models From Natural Language Supervision (2021).[67] This paper describes the CLIP method for training text encoders, which convert text into floating point vectors. Such text encodings are used by the diffusion model to create images.\n- SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations (2021).[68] This paper describes SDEdit, aka \"img2img\".\n- High-Resolution Image Synthesis with Latent Diffusion Models (2021, updated in 2022).[69] This paper describes the latent diffusion model (LDM). This is the backbone of the Stable Diffusion architecture.\n- Classifier-Free Diffusion Guidance (2022).[28] This paper describes CFG, which allows the text encoding vector to steer the diffusion model towards creating the image described by the text.\n- SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis (2023).[19] Describes SDXL.\n- Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow (2022).[21][22] Describes rectified flow, which is used for the backbone architecture of SD 3.0.\n- Scaling Rectified Flow Transformers for High-resolution Image Synthesis (2024).[20] Describes SD 3.0.\nTraining cost\n\n- SD 2.0: 0.2 million hours on A100 (40GB).[59]\n## Usage and controversy[edit]\nStable Diffusion claims no rights on generated images and freely gives users the rights of usage to any generated images from the model provided that the image content is not illegal or harmful to individuals.[70]\n\nThe images Stable Diffusion was trained on have been filtered without human input, leading to some harmful images and large amounts of private and sensitive information appearing in the training data.[25]\n\nMore traditional visual artists have expressed concern that widespread usage of image synthesis software such as Stable Diffusion may eventually lead to human artists, along with photographers, models, cinematographers, and actors, gradually losing commercial viability against AI-based competitors.[71]\n\nStable Diffusion is notably more permissive in the types of content users may generate, such as violent or sexually explicit imagery, in comparison to other commercial products based on generative AI.[72] Addressing the concerns that the model may be used for abusive purposes, CEO of Stability AI, Emad Mostaque, argues that \"[it is] peoples' responsibility as to whether they are ethical, moral, and legal in how they operate this technology\",[10] and that putting the capabilities of Stable Diffusion into the hands of the public would result in the technology providing a net benefit, in spite of the potential negative consequences.[10] In addition, Mostaque argues that the intention behind the open availability of Stable Diffusion is to end corporate control and dominance over such technologies, who have previously only developed closed AI systems for image synthesis.[10][72] This is reflected by the fact that any restrictions Stability AI places on the content that users may generate can easily be bypassed due to the availability of the source code.[73]\n\nControversy around photorealistic sexualized depictions of underage characters have been brought up, due to such images generated by Stable Diffusion being shared on websites such as Pixiv.[74]\n\n### ComfyUI extension compromise[edit]\nIn June 2024, A hacker group called \"Nullbulge\" compromised an extension of a graphic user interface for Stable Diffusion called ComfyUI to add malicious code to it. The compromised extension, called ComfyUI_LLMVISION, was used for integrating the interface with AI language models GPT-4 and Claude 3, and was hosted on Github. Nullbulge hosted a list of hundreds of ComfyUI users' login details across multiple services on its website, while users of the extension reported receiving numerous login notifications. vpnMentor conducted security research on the extension and claimed it could \"steal crypto wallets, screenshot the user’s screen, expose device information and IP addresses, and steal files that contain certain keywords or extensions\". \n\nNullbulge's website claims they targeted users who committed \"one of our sins\", which included AI-art generation, art theft, promoting cryptocurrency, and any other kind of theft from artists such as from Patreon. They claimed that they were \"a collective of individuals who believe in the importance of protecting artists' rights and ensuring fair compensation for their work\" and that they believed that \"AI-generated artwork is detrimental to the creative industry and should be discouraged\".[75]\n\n## Litigation[edit]\nIn January 2023, three artists, Sarah Andersen, Kelly McKernan, and Karla Ortiz, filed a copyright infringement lawsuit against Stability AI, Midjourney, and DeviantArt, claiming that these companies have infringed the rights of millions of artists by training AI tools on five billion images scraped from the web without the consent of the original artists.[76] The same month, Stability AI was also sued by Getty Images for using its images in the training data.[77]\n\nIn July 2023, U.S. District Judge William Orrick inclined to dismiss most of the lawsuit filed by Andersen, McKernan, and Ortiz but allowed them to file a new complaint.[78]\n\n## License[edit]\nUnlike models like DALL-E, Stable Diffusion makes its source code available,[79][8] along with the model (pretrained weights). It applies the Creative ML OpenRAIL-M license, a form of Responsible AI License (RAIL), to the model (M).[80] The license prohibits certain use cases, including crime, libel, harassment, doxing, \"exploiting ... minors\", giving medical advice, automatically creating legal obligations, producing legal evidence, and \"discriminating against or harming individuals or groups based on ... social behavior or ... personal or personality characteristics ... [or] legally protected characteristics or categories\".[81][82] The user owns the rights to their generated output images, and is free to use them commercially.[83]\n\n## See also[edit]\n- Artificial intelligence art\n- Midjourney\n- Craiyon\n- Hugging Face\n- Imagen (Google Brain)\n## References[edit]\n- ^ \"Announcing SDXL 1.0\". stability.ai. Archived from the original on July 26, 2023.\n\n- ^ Ryan O'Connor (August 23, 2022). \"How to Run Stable Diffusion Locally to Generate Images\". Archived from the original on October 13, 2023. Retrieved May 4, 2023.\n\n- ^ \"Diffuse The Rest - a Hugging Face Space by huggingface\". huggingface.co. Archived from the original on September 5, 2022. Retrieved September 5, 2022.\n\n- ^ \"Leaked deck raises questions over Stability AI's Series A pitch to investors\". sifted.eu. Archived from the original on June 29, 2023. Retrieved June 20, 2023.\n\n- ^ \"Revolutionizing image generation by AI: Turning text into images\". www.lmu.de. Archived from the original on September 17, 2022. Retrieved June 21, 2023.\n\n- ^ Mostaque, Emad (November 2, 2022). \"Stable Diffusion came from the Machine Vision & Learning research group (CompVis) @LMU_Muenchen\". Twitter. Archived from the original on July 20, 2023. Retrieved June 22, 2023.\n\n- ^ a b c d \"Stable Diffusion Launch Announcement\". Stability.Ai. Archived from the original on September 5, 2022. Retrieved September 6, 2022.\n\n- ^ a b c d e f g h i \"Stable Diffusion Repository on GitHub\". CompVis - Machine Vision and Learning Research Group, LMU Munich. September 17, 2022. Archived from the original on January 18, 2023. Retrieved September 17, 2022.\n\n- ^ \"The new killer app: Creating AI art will absolutely crush your PC\". PCWorld. Archived from the original on August 31, 2022. Retrieved August 31, 2022.\n\n- ^ a b c d e Vincent, James (September 15, 2022). \"Anyone can use this AI art generator — that's the risk\". The Verge. Archived from the original on January 21, 2023. Retrieved September 30, 2022.\n\n- ^ \"CompVis/Latent-diffusion\". GitHub.\n\n- ^ \"Stable Diffusion 3: Research Paper\".\n\n- ^ a b c Rombach; Blattmann; Lorenz; Esser; Ommer (June 2022). High-Resolution Image Synthesis with Latent Diffusion Models (PDF). International Conference on Computer Vision and Pattern Recognition (CVPR). New Orleans, LA. pp. 10684–10695. arXiv:2112.10752. Archived (PDF) from the original on January 20, 2023. Retrieved September 17, 2022.\n\n- ^ a b c d Alammar, Jay. \"The Illustrated Stable Diffusion\". jalammar.github.io. Archived from the original on November 1, 2022. Retrieved October 31, 2022.\n\n- ^ David, Foster. \"8. Diffusion Models\". Generative Deep Learning (2 ed.). O'Reilly.\n\n- ^ Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, Surya Ganguli (March 12, 2015). \"Deep Unsupervised Learning using Nonequilibrium Thermodynamics\". Arxiv. arXiv:1503.03585.{{cite journal}}:  CS1 maint: multiple names: authors list (link)\n\n- ^ \"Stable diffusion pipelines\". huggingface.co. Archived from the original on June 25, 2023. Retrieved June 22, 2023.\n\n- ^ \"Text-to-Image Generation with Stable Diffusion and OpenVINO™\". openvino.ai. Intel. Retrieved February 10, 2024.\n\n- ^ a b c Podell, Dustin; English, Zion; Lacey, Kyle; Blattmann, Andreas; Dockhorn, Tim; Müller, Jonas; Penna, Joe; Rombach, Robin (July 4, 2023). \"SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis\". arXiv:2307.01952 [cs.CV].\n\n- ^ a b c Esser, Patrick; Kulal, Sumith; Blattmann, Andreas; Entezari, Rahim; Müller, Jonas; Saini, Harry; Levi, Yam; Lorenz, Dominik; Sauer, Axel (March 5, 2024), Scaling Rectified Flow Transformers for High-Resolution Image Synthesis, arXiv:2403.03206\n\n- ^ a b Liu, Xingchao; Gong, Chengyue; Liu, Qiang (September 7, 2022), Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow, arXiv:2209.03003\n\n- ^ a b \"Rectified Flow — Rectified Flow\". www.cs.utexas.edu. Retrieved March 6, 2024.\n\n- ^ a b c d e Baio, Andy (August 30, 2022). \"Exploring 12 Million of the 2.3 Billion Images Used to Train Stable Diffusion's Image Generator\". Waxy.org. Archived from the original on January 20, 2023. Retrieved November 2, 2022.\n\n- ^ \"This artist is dominating AI-generated art. And he's not happy about it\". MIT Technology Review. Archived from the original on January 14, 2023. Retrieved November 2, 2022.\n\n- ^ a b Brunner, Katharina; Harlan, Elisa (July 7, 2023). \"We Are All Raw Material for AI\". Bayerischer Rundfunk (BR). Archived from the original on September 12, 2023. Retrieved September 12, 2023.\n\n- ^ Schuhmann, Christoph (November 2, 2022), CLIP+MLP Aesthetic Score Predictor, archived from the original on June 8, 2023, retrieved November 2, 2022\n\n- ^ \"LAION-Aesthetics | LAION\". laion.ai. Archived from the original on August 26, 2022. Retrieved September 2, 2022.\n\n- ^ a b c Ho, Jonathan; Salimans, Tim (July 25, 2022). \"Classifier-Free Diffusion Guidance\". arXiv:2207.12598 [cs.LG].\n\n- ^ Mostaque, Emad (August 28, 2022). \"Cost of construction\". Twitter. Archived from the original on September 6, 2022. Retrieved September 6, 2022.\n\n- ^ a b c \"CompVis/stable-diffusion-v1-4 · Hugging Face\". huggingface.co. Archived from the original on January 11, 2023. Retrieved November 2, 2022.\n\n- ^ Wiggers, Kyle (August 12, 2022). \"A startup wants to democratize the tech behind DALL-E 2, consequences be damned\". TechCrunch. Archived from the original on January 19, 2023. Retrieved November 2, 2022.\n\n- ^ emad_9608 (April 19, 2024). \"10m is about right\". r/StableDiffusion. Retrieved April 25, 2024.{{cite web}}:  CS1 maint: numeric names: authors list (link)\n\n- ^ a b c d e \"Stable Diffusion with 🧨 Diffusers\". huggingface.co. Archived from the original on January 17, 2023. Retrieved October 31, 2022.\n\n- ^ a b c \"Stable Diffusion 2.0 Release\". stability.ai. Archived from the original on December 10, 2022.\n\n- ^ \"LAION\". laion.ai. Archived from the original on October 16, 2023. Retrieved October 31, 2022.\n\n- ^ \"Generating images with Stable Diffusion\". Paperspace Blog. August 24, 2022. Archived from the original on October 31, 2022. Retrieved October 31, 2022.\n\n- ^ \"Announcing SDXL 1.0\". Stability AI. Archived from the original on July 26, 2023. Retrieved August 21, 2023.\n\n- ^ Edwards, Benj (July 27, 2023). \"Stability AI releases Stable Diffusion XL, its next-gen image synthesis model\". Ars Technica. Archived from the original on August 21, 2023. Retrieved August 21, 2023.\n\n- ^ \"hakurei/waifu-diffusion · Hugging Face\". huggingface.co. Archived from the original on October 8, 2023. Retrieved October 31, 2022.\n\n- ^ Chambon, Pierre; Bluethgen, Christian; Langlotz, Curtis P.; Chaudhari, Akshay (October 9, 2022). \"Adapting Pretrained Vision-Language Foundational Models to Medical Imaging Domains\". arXiv:2210.04133 [cs.CV].\n\n- ^ Seth Forsgren; Hayk Martiros. \"Riffusion - Stable diffusion for real-time music generation\". Riffusion. Archived from the original on December 16, 2022.\n\n- ^ Mercurio, Anthony (October 31, 2022), Waifu Diffusion, archived from the original on October 31, 2022, retrieved October 31, 2022\n\n- ^ Smith, Ryan. \"NVIDIA Quietly Launches GeForce RTX 3080 12GB: More VRAM, More Power, More Money\". www.anandtech.com. Archived from the original on August 27, 2023. Retrieved October 31, 2022.\n\n- ^ Dave James (October 28, 2022). \"I thrashed the RTX 4090 for 8 hours straight training Stable Diffusion to paint like my uncle Hermann\". PC Gamer. Archived from the original on November 9, 2022.\n\n- ^ Gal, Rinon; Alaluf, Yuval; Atzmon, Yuval; Patashnik, Or; Bermano, Amit H.; Chechik, Gal; Cohen-Or, Daniel (August 2, 2022). \"An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion\". arXiv:2208.01618 [cs.CV].\n\n- ^ \"NovelAI Improvements on Stable Diffusion\". NovelAI. October 11, 2022. Archived from the original on October 27, 2022.\n\n- ^ Yuki Yamashita (September 1, 2022). \"愛犬の合成画像を生成できるAI 文章で指示するだけでコスプレ 米Googleが開発\". ITmedia Inc. (in Japanese). Archived from the original on August 31, 2022.\n\n- ^ Meng, Chenlin; He, Yutong; Song, Yang; Song, Jiaming; Wu, Jiajun; Zhu, Jun-Yan; Ermon, Stefano (August 2, 2021). \"SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations\". arXiv:2108.01073 [cs.CV].\n\n- ^ a b c d \"Stable Diffusion web UI\". GitHub. November 10, 2022. Archived from the original on January 20, 2023. Retrieved September 27, 2022.\n\n- ^ invisible-watermark, Shield Mountain, November 2, 2022, archived from the original on October 18, 2022, retrieved November 2, 2022\n\n- ^ \"stable-diffusion-tools/emphasis at master · JohannesGaessler/stable-diffusion-tools\". GitHub. Archived from the original on October 2, 2022. Retrieved November 2, 2022.\n\n- ^ \"Stable Diffusion v2.1 and DreamStudio Updates 7-Dec 22\". stability.ai. Archived from the original on December 10, 2022.\n\n- ^ a b Luzi, Lorenzo; Siahkoohi, Ali; Mayer, Paul M.; Casco-Rodriguez, Josue; Baraniuk, Richard (October 21, 2022). \"Boomerang: Local sampling on image manifolds using diffusion models\". arXiv:2210.12100 [cs.CV].\n\n- ^ Bühlmann, Matthias (September 28, 2022). \"Stable Diffusion Based Image Compression\". Medium. Archived from the original on November 2, 2022. Retrieved November 2, 2022.\n\n- ^ Zhang, Lvmin (February 10, 2023). \"Adding Conditional Control to Text-to-Image Diffusion Models\". arXiv:2302.05543 [cs.CV].\n\n- ^ \"CompVis/stable-diffusion-v1-4 · Hugging Face\". huggingface.co. Archived from the original on January 11, 2023. Retrieved August 17, 2023.\n\n- ^ \"CompVis (CompVis)\". huggingface.co. August 23, 2023. Retrieved March 6, 2024.\n\n- ^ \"runwayml/stable-diffusion-v1-5 · Hugging Face\". huggingface.co. Archived from the original on September 21, 2023. Retrieved August 17, 2023.\n\n- ^ a b \"stabilityai/stable-diffusion-2 · Hugging Face\". huggingface.co. Archived from the original on September 21, 2023. Retrieved August 17, 2023.\n\n- ^ \"stabilityai/stable-diffusion-2-base · Hugging Face\". huggingface.co. Retrieved January 1, 2024.\n\n- ^ \"stabilityai/stable-diffusion-2-1 · Hugging Face\". huggingface.co. Archived from the original on September 21, 2023. Retrieved August 17, 2023.\n\n- ^ \"stabilityai/stable-diffusion-xl-base-1.0 · Hugging Face\". huggingface.co. Archived from the original on October 8, 2023. Retrieved August 17, 2023.\n\n- ^ \"Announcing SDXL 1.0\". Stability AI. Retrieved January 1, 2024.\n\n- ^ \"stabilityai/sdxl-turbo · Hugging Face\". huggingface.co. Retrieved January 1, 2024.\n\n- ^ \"Adversarial Diffusion Distillation\". Stability AI. Retrieved January 1, 2024.\n\n- ^ \"Stable Diffusion 3\". Stability AI. Retrieved March 5, 2024.\n\n- ^ Radford, Alec; Kim, Jong Wook; Hallacy, Chris; Ramesh, Aditya; Goh, Gabriel; Agarwal, Sandhini; Sastry, Girish; Askell, Amanda; Mishkin, Pamela (February 26, 2021). \"Learning Transferable Visual Models From Natural Language Supervision\". arXiv:2103.00020 [cs.CV].\n\n- ^ Meng, Chenlin; He, Yutong; Song, Yang; Song, Jiaming; Wu, Jiajun; Zhu, Jun-Yan; Ermon, Stefano (January 4, 2022). \"SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations\". arXiv:2108.01073 [cs.CV].\n\n- ^ Rombach, Robin; Blattmann, Andreas; Lorenz, Dominik; Esser, Patrick; Ommer, Björn (2022). \"High-Resolution Image Synthesis With Latent Diffusion Models\". Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 10684–10695. arXiv:2112.10752.\n\n- ^ \"LICENSE.md · stabilityai/stable-diffusion-xl-base-1.0 at main\". huggingface.co. July 26, 2023. Retrieved January 1, 2024.\n\n- ^ Heikkilä, Melissa (September 16, 2022). \"This artist is dominating AI-generated art. And he's not happy about it\". MIT Technology Review. Archived from the original on January 14, 2023. Retrieved September 26, 2022.\n\n- ^ a b Ryo Shimizu (August 26, 2022). \"Midjourneyを超えた？ 無料の作画AI｢ #StableDiffusion ｣が｢AIを民主化した｣と断言できる理由\". Business Insider Japan (in Japanese). Archived from the original on December 10, 2022. Retrieved October 4, 2022.\n\n- ^ Cai, Kenrick. \"Startup Behind AI Image Generator Stable Diffusion Is In Talks To Raise At A Valuation Up To $1 Billion\". Forbes. Archived from the original on September 30, 2023. Retrieved October 31, 2022.\n\n- ^ \"Illegal trade in AI child sex abuse images exposed\". BBC News. June 27, 2023. Archived from the original on September 21, 2023. Retrieved September 26, 2023.\n\n- ^ Maiberg, Emanuel (June 11, 2024). \"Hackers Target AI Users With Malicious Stable Diffusion Tool on GitHub to Protest 'Art Theft'\". 404 Media. Retrieved June 14, 2024.\n\n- ^ Vincent, James (January 16, 2023). \"AI art tools Stable Diffusion and Midjourney targeted with copyright lawsuit\". The Verge. Archived from the original on March 9, 2023. Retrieved January 16, 2023.\n\n- ^ Korn, Jennifer (January 17, 2023). \"Getty Images suing the makers of popular AI art tool for allegedly stealing photos\". CNN. Archived from the original on March 1, 2023. Retrieved January 22, 2023.\n\n- ^ Brittain, Blake (July 19, 2023). \"US judge finds flaws in artists' lawsuit against AI companies\". Reuters. Archived from the original on September 6, 2023. Retrieved August 6, 2023.\n\n- ^ \"Stable Diffusion Public Release\". Stability.Ai. Archived from the original on August 30, 2022. Retrieved August 31, 2022.\n\n- ^ \"From RAIL to Open RAIL: Topologies of RAIL Licenses\". Responsible AI Licenses (RAIL). August 18, 2022. Archived from the original on July 27, 2023. Retrieved February 20, 2023.\n\n- ^ \"Ready or not, mass video deepfakes are coming\". The Washington Post. August 30, 2022. Archived from the original on August 31, 2022. Retrieved August 31, 2022.\n\n- ^ \"License - a Hugging Face Space by CompVis\". huggingface.co. Archived from the original on September 4, 2022. Retrieved September 5, 2022.\n\n- ^ Katsuo Ishida (August 26, 2022). \"言葉で指示した画像を凄いAIが描き出す「Stable Diffusion」 ～画像は商用利用も可能\". Impress Corporation (in Japanese). Archived from the original on November 14, 2022. Retrieved October 4, 2022.\n\n## External links[edit]\n- Stable Diffusion Demo\n- Interactive Explanation of Stable Diffusion\n- \"We Are All Raw Material for AI\": Investigation on sensitive and private data in Stable Diffusions training data\n- \"Negative Prompts in Stable Diffusion\"\n- v\n- t\n- e\n- Differentiable programming\n- Information geometry\n- Statistical manifold\n- Automatic differentiation\n- Neuromorphic engineering\n- Pattern recognition\n- Tensor calculus\n- Computational learning theory\n- Inductive bias\n- Gradient descent\nSGD\n- SGD\n- Clustering\n- Regression\nOverfitting\n- Overfitting\n- Hallucination\n- Adversary\n- Attention\n- Convolution\n- Loss functions\n- Backpropagation\n- Batchnorm\n- Activation\nSoftmax\nSigmoid\nRectifier\n- Softmax\n- Sigmoid\n- Rectifier\n- Regularization\n- Datasets\nAugmentation\n- Augmentation\n- Diffusion\n- Autoregression\n- Machine learning\nIn-context learning\n- In-context learning\n- Artificial neural network\nDeep learning\n- Deep learning\n- Scientific computing\n- Artificial Intelligence\n- Language model\nLarge language model\n- Large language model\n- IPU\n- TPU\n- VPU\n- Memristor\n- SpiNNaker\n- TensorFlow\n- PyTorch\n- Keras\n- Theano\n- JAX\n- Flux.jl\n- MindSpore\n- AlexNet\n- WaveNet\n- Human image synthesis\n- HWR\n- OCR\n- Speech synthesis\n- Speech recognition\n- Facial recognition\n- AlphaFold\n- Text-to-image models\nDALL-E\nMidjourney\nStable Diffusion\n- DALL-E\n- Midjourney\n- Stable Diffusion\n- Text-to-video models\nSora\nVideoPoet\n- Sora\n- VideoPoet\n- Whisper\n- Word2vec\n- Seq2seq\n- BERT\n- Gemini\n- LaMDA\nBard\n- Bard\n- NMT\n- Project Debater\n- IBM Watson\n- IBM Watsonx\n- Granite\n- GPT-1\n- GPT-2\n- GPT-3\n- GPT-4\n- ChatGPT\n- GPT-J\n- Chinchilla AI\n- PaLM\n- BLOOM\n- LLaMA\n- PanGu-Σ\n- AlphaGo\n- AlphaZero\n- Q-learning\n- SARSA\n- OpenAI Five\n- Self-driving car\n- MuZero\n- Action selection\nAuto-GPT\n- Auto-GPT\n- Robot control\n- Yoshua Bengio\n- Alex Graves\n- Ian Goodfellow\n- Stephen Grossberg\n- Demis Hassabis\n- Geoffrey Hinton\n- Yann LeCun\n- Fei-Fei Li\n- Andrew Ng\n- Jürgen Schmidhuber\n- David Silver\n- Ilya Sutskever\n- Anthropic\n- EleutherAI\n- Google DeepMind\n- Hugging Face\n- OpenAI\n- Meta AI\n- Mila\n- MIT CSAIL\n- Huawei\n- Neural Turing machine\n- Differentiable neural computer\n- Transformer\n- Recurrent neural network (RNN)\n- Long short-term memory (LSTM)\n- Gated recurrent unit (GRU)\n- Echo state network\n- Multilayer perceptron (MLP)\n- Convolutional neural network\n- Residual neural network\n- Mamba\n- Autoencoder\n- Variational autoencoder (VAE)\n- Generative adversarial network (GAN)\n- Graph neural network\n-  Portals\nComputer programming\nTechnology\n- Computer programming\n- Technology\n-  Categories\nArtificial neural networks\nMachine learning\n- Artificial neural networks\n- Machine learning\n- Artificial intelligence art\n- Deep learning software applications\n- Text-to-image generation\n- Unsupervised learning\n- Art controversies\n- Works involved in plagiarism controversies\n- 2022 software\n- CS1 maint: multiple names: authors list\n- CS1 maint: numeric names: authors list\n- CS1 Japanese-language sources (ja)\n- Articles with short description\n- Short description matches Wikidata\n- Use mdy dates from October 2023\n- All articles with unsourced statements\n- Articles with unsourced statements from October 2023\n- Pages using multiple image with auto scaled images\n- Commons category link from Wikidata\n-  This page was last edited on 16 June 2024, at 11:38 (UTC).\n- Text is available under the Creative Commons Attribution-ShareAlike License 4.0;\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.\n- Privacy policy\n- About Wikipedia\n- Disclaimers\n- Contact Wikipedia\n- Code of Conduct\n- Developers\n- Statistics\n- Cookie statement\n- Mobile view\n- \n- \n-\n"
      ]
    },
    {
      "id": 6,
      "type": "TextInput_",
      "pos": [
        940,
        560
      ],
      "size": {
        "0": 400,
        "1": 200
      },
      "flags": {},
      "order": 0,
      "mode": 0,
      "outputs": [
        {
          "name": "STRING",
          "type": "STRING",
          "links": [
            7
          ],
          "shape": 3,
          "slot_index": 0
        }
      ],
      "properties": {
        "Node name for S&R": "TextInput_"
      },
      "widgets_values": [
        "https://en.wikipedia.org/wiki/Stable_Diffusion"
      ]
    },
    {
      "id": 24,
      "type": "TextInput_",
      "pos": [
        3400,
        180
      ],
      "size": {
        "0": 400,
        "1": 200
      },
      "flags": {},
      "order": 1,
      "mode": 0,
      "outputs": [
        {
          "name": "STRING",
          "type": "STRING",
          "links": [
            28
          ],
          "shape": 3,
          "slot_index": 0
        }
      ],
      "properties": {
        "Node name for S&R": "TextInput_"
      },
      "widgets_values": [
        "Summarize the content below\n\n\n"
      ]
    },
    {
      "id": 12,
      "type": "ShowText|pysssss",
      "pos": [
        4940,
        460
      ],
      "size": {
        "0": 640,
        "1": 620
      },
      "flags": {},
      "order": 9,
      "mode": 0,
      "inputs": [
        {
          "name": "text",
          "type": "STRING",
          "link": 14,
          "widget": {
            "name": "text"
          }
        }
      ],
      "outputs": [
        {
          "name": "STRING",
          "type": "STRING",
          "links": null,
          "shape": 6
        }
      ],
      "properties": {
        "Node name for S&R": "ShowText|pysssss"
      },
      "widgets_values": [
        "",
        "The content appears to be a collection of news articles and research papers related to AI, specifically the Stable Diffusion model. The topics covered include:\n\n1. Stable Diffusion's potential to democratize AI (Business Insider Japan)\n2. Startup behind Stable Diffusion in talks to raise $1 billion valuation (Forbes)\n3. Illegal trade in AI child sex abuse images exposed (BBC News)\n4. Hackers target AI users with malicious Stable Diffusion tool on GitHub (404 Media)\n5. AI art tools Stable Diffusion and Midjourney targeted with copyright lawsuit (The Verge, CNN)\n6. US judge finds flaws in artists' lawsuit against AI companies (Reuters)\n7. Stable Diffusion public release announcement (Stability.Ai)\n\nThe articles also discuss various related topics, such as:\n\n* Artificial intelligence\n* Machine learning\n* Deep learning\n* Text-to-image generation\n* Unsupervised learning\n* Art controversies\n* Works involved in plagiarism controversies\n\nOverall, the content appears to be a collection of news and research papers related to AI, Stable Diffusion, and its applications."
      ]
    },
    {
      "id": 11,
      "type": "OllamaGenerateAdvance",
      "pos": [
        4460,
        460
      ],
      "size": {
        "0": 380,
        "1": 700
      },
      "flags": {},
      "order": 7,
      "mode": 0,
      "inputs": [
        {
          "name": "context",
          "type": "STRING",
          "link": null,
          "widget": {
            "name": "context"
          }
        },
        {
          "name": "prompt",
          "type": "STRING",
          "link": 29,
          "widget": {
            "name": "prompt"
          }
        }
      ],
      "outputs": [
        {
          "name": "response",
          "type": "STRING",
          "links": [
            14
          ],
          "shape": 3,
          "slot_index": 0
        },
        {
          "name": "context",
          "type": "STRING",
          "links": null,
          "shape": 3
        }
      ],
      "properties": {
        "Node name for S&R": "OllamaGenerateAdvance"
      },
      "widgets_values": [
        "",
        "enable",
        "http://127.0.0.1:11434",
        "llama3:latest",
        "You are an AI assistant",
        194615875,
        "randomize",
        40,
        0.9,
        0.8,
        -1,
        1,
        5,
        ""
      ]
    },
    {
      "id": 27,
      "type": "JoinStrings",
      "pos": [
        3880,
        600
      ],
      "size": {
        "0": 315,
        "1": 106
      },
      "flags": {},
      "order": 6,
      "mode": 0,
      "inputs": [
        {
          "name": "string1",
          "type": "STRING",
          "link": 28,
          "widget": {
            "name": "string1"
          }
        },
        {
          "name": "string2",
          "type": "STRING",
          "link": 37,
          "widget": {
            "name": "string2"
          }
        }
      ],
      "outputs": [
        {
          "name": "STRING",
          "type": "STRING",
          "links": [
            29,
            30
          ],
          "shape": 3,
          "slot_index": 0
        }
      ],
      "properties": {
        "Node name for S&R": "JoinStrings"
      },
      "widgets_values": [
        "",
        "",
        " "
      ]
    }
  ],
  "links": [
    [
      7,
      6,
      0,
      4,
      0,
      "STRING"
    ],
    [
      8,
      4,
      0,
      5,
      0,
      "STRING"
    ],
    [
      10,
      9,
      0,
      10,
      0,
      "STRING"
    ],
    [
      14,
      11,
      0,
      12,
      0,
      "STRING"
    ],
    [
      15,
      4,
      0,
      9,
      0,
      "STRING"
    ],
    [
      28,
      24,
      0,
      27,
      0,
      "STRING"
    ],
    [
      29,
      27,
      0,
      11,
      1,
      "STRING"
    ],
    [
      30,
      27,
      0,
      28,
      0,
      "STRING"
    ],
    [
      37,
      10,
      0,
      27,
      1,
      "STRING"
    ]
  ],
  "groups": [],
  "config": {},
  "extra": {
    "ds": {
      "scale": 0.4950000000000011,
      "offset": [
        113.15559749659633,
        504.30034870347606
      ]
    }
  },
  "version": 0.4
}